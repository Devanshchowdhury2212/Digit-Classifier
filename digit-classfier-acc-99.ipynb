{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-12T18:59:55.774350Z",
     "iopub.status.busy": "2021-06-12T18:59:55.773434Z",
     "iopub.status.idle": "2021-06-12T18:59:55.779006Z",
     "shell.execute_reply": "2021-06-12T18:59:55.778565Z",
     "shell.execute_reply.started": "2021-06-12T18:26:44.450775Z"
    },
    "papermill": {
     "duration": 0.05518,
     "end_time": "2021-06-12T18:59:55.779103",
     "exception": false,
     "start_time": "2021-06-12T18:59:55.723923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/sample_submission.csv\n",
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory+\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042645,
     "end_time": "2021-06-12T18:59:55.864090",
     "exception": false,
     "start_time": "2021-06-12T18:59:55.821445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This is a very beginner friendly kernel and my first kernel . I hope it helps you as much as i learned while implementing it , So lets Begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042111,
     "end_time": "2021-06-12T18:59:55.977832",
     "exception": false,
     "start_time": "2021-06-12T18:59:55.935721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notebooks that helped me , please give them a read .\n",
    "https://www.kaggle.com/poonaml/deep-neural-network-keras-way#Import-all-required-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-06-12T18:59:56.067273Z",
     "iopub.status.busy": "2021-06-12T18:59:56.066610Z",
     "iopub.status.idle": "2021-06-12T19:00:01.365463Z",
     "shell.execute_reply": "2021-06-12T19:00:01.364551Z",
     "shell.execute_reply.started": "2021-06-12T18:26:44.465126Z"
    },
    "papermill": {
     "duration": 5.34551,
     "end_time": "2021-06-12T19:00:01.365580",
     "exception": false,
     "start_time": "2021-06-12T18:59:56.020070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041964,
     "end_time": "2021-06-12T19:00:01.449500",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.407536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We begin by importing libraries that i would use generally. Also i would recommend all beginners to keep all their imports at one place for ease and presentation wise too ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:01.549345Z",
     "iopub.status.busy": "2021-06-12T19:00:01.548802Z",
     "iopub.status.idle": "2021-06-12T19:00:01.552936Z",
     "shell.execute_reply": "2021-06-12T19:00:01.552517Z",
     "shell.execute_reply.started": "2021-06-12T18:26:49.993222Z"
    },
    "papermill": {
     "duration": 0.060597,
     "end_time": "2021-06-12T19:00:01.553027",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.492430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    path = '/kaggle/input/digit-recognizer/'\n",
    "    train = pd.read_csv(path+'train.csv')\n",
    "    test = pd.read_csv(path+'test.csv')\n",
    "    train_X,train_y = train.drop(columns=['label']),train['label']\n",
    "    train_X = np.array(train_X)\n",
    "    \n",
    "    train_y = np.array(train_y)\n",
    "    \n",
    "    test_X = test\n",
    "    test_X = np.array(test_X)\n",
    "    print(train_X.shape)\n",
    "    train_X = train_X.reshape((train_X.shape[0],28,28,1))\n",
    "    print(train_X.shape)\n",
    "    test_X = test_X.reshape((test_X.shape[0],28,28,1))\n",
    "    train_y = to_categorical(train_y)\n",
    "    return train_X, train_y,test_X\n",
    "def normalize(trainX,testX):\n",
    "    trainX_trans = trainX / 255\n",
    "    testX_trans = testX / 255\n",
    "    return trainX_trans,testX_trans\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041184,
     "end_time": "2021-06-12T19:00:01.636081",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.594897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I have made functions that will help me load my data , normalise it , use encoding . So that i dont have to do same thing over and over again . This practise helped me to monitor more things at once and keep my code clean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.041106,
     "end_time": "2021-06-12T19:00:01.718842",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.677736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The load data function simply loads the dataset. \n",
    "\n",
    "I also reshaped the dataset's (which was converted into numpy array as they are fast and easy to deal) each sameple from 784 into 28 x 28 x 1 shape. For those who dont know the reason for this step can look at this article which exlain in detail and other details about convulotion layer.\n",
    "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\n",
    "\n",
    "I used to_categorical to implement one hot encoding in my labels .(It made 3 to a array (0,0,0,1,0,0,0,0,0,0) ).\n",
    "\n",
    "And Normalize function normalizes the image pixels , as the value of pixels can be from 0 to 255.(I Reduced it to 0 to 1.This step was important and it did increase my score in the leader board).\n",
    "\n",
    "Make model created a model and returned it . The model i used was a simple convnet with 3 Conv2D layers ,1 Dense layer ,2 Max Pooling layer and a Dense layer for showing probabiliy of being the digit (Hence it outputs an array of size 10,probability for each).\n",
    "\n",
    "Optimizer is the common RMSprop one can try SGD too.Loss function is crosscategorical_crossentropy which is best suited for such conditions and accuracy as metrics to monitor performance of my model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041515,
     "end_time": "2021-06-12T19:00:01.802187",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.760672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.041448,
     "end_time": "2021-06-12T19:00:01.885518",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.844070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:01.978913Z",
     "iopub.status.busy": "2021-06-12T19:00:01.978350Z",
     "iopub.status.idle": "2021-06-12T19:00:09.241283Z",
     "shell.execute_reply": "2021-06-12T19:00:09.240466Z",
     "shell.execute_reply.started": "2021-06-12T18:26:50.014091Z"
    },
    "papermill": {
     "duration": 7.31439,
     "end_time": "2021-06-12T19:00:09.241453",
     "exception": false,
     "start_time": "2021-06-12T19:00:01.927063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 494,794\n",
      "Trainable params: 494,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:09.334617Z",
     "iopub.status.busy": "2021-06-12T19:00:09.333793Z",
     "iopub.status.idle": "2021-06-12T19:00:39.494674Z",
     "shell.execute_reply": "2021-06-12T19:00:39.495125Z",
     "shell.execute_reply.started": "2021-06-12T18:26:57.219808Z"
    },
    "papermill": {
     "duration": 30.211205,
     "end_time": "2021-06-12T19:00:39.495248",
     "exception": false,
     "start_time": "2021-06-12T19:00:09.284043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600\n",
      "66/66 [==============================] - 1s 15ms/step - loss: 0.6528 - accuracy: 0.8186 - val_loss: 0.1614 - val_accuracy: 0.9496\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.1104 - accuracy: 0.9662 - val_loss: 0.0703 - val_accuracy: 0.9783\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0607 - accuracy: 0.9805 - val_loss: 0.0894 - val_accuracy: 0.9727\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0427 - accuracy: 0.9862 - val_loss: 0.0579 - val_accuracy: 0.9844\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0324 - accuracy: 0.9898 - val_loss: 0.0334 - val_accuracy: 0.9900\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 0.0140 - val_accuracy: 0.9955\n",
      "33600\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 0.0185 - val_accuracy: 0.9936\n",
      "33600\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0139 - accuracy: 0.9956 - val_loss: 0.0116 - val_accuracy: 0.9963\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0124 - accuracy: 0.9958 - val_loss: 0.0092 - val_accuracy: 0.9973\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 0.0229 - val_accuracy: 0.9917\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0056 - val_accuracy: 0.9983\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.0066 - val_accuracy: 0.9981\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0083 - val_accuracy: 0.9973\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0022 - val_accuracy: 0.9994\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0024 - val_accuracy: 0.9992\n",
      "33600\n",
      "66/66 [==============================] - 1s 12ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0032 - val_accuracy: 0.9986\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0111 - val_accuracy: 0.9964\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0060 - val_accuracy: 0.9976\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0021 - val_accuracy: 0.9993\n",
      "33600\n",
      "66/66 [==============================] - 1s 11ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0017 - val_accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    print(len(train_indices))\n",
    "    his = model.fit(trainX[train_indices],trainy[train_indices],validation_data=(trainX[val_indices],trainy[val_indices]),epochs=1,batch_size=512)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113925,
     "end_time": "2021-06-12T19:00:39.723634",
     "exception": false,
     "start_time": "2021-06-12T19:00:39.609709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Instead of running a simple model.fit ,I used Stratified Shuffle Split for 2 reasons:-\n",
    "1. It divides the data properly , assume i have 50 images of cat and dogs. And i decided to use 50 for training ,25 for validation and 25 for testing . There is no way to tell if 50 training images are of 25 cats and dog each .Maybe its 40 cats and 10 dogs ,Surely this is a problem as it wont be truly proportional sample. This problem is taken care by Stratified Shuffle Split .\n",
    "\n",
    "2. I got better score using this method.POINT BLANK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:39.959100Z",
     "iopub.status.busy": "2021-06-12T19:00:39.958438Z",
     "iopub.status.idle": "2021-06-12T19:00:40.138992Z",
     "shell.execute_reply": "2021-06-12T19:00:40.138458Z",
     "shell.execute_reply.started": "2021-06-12T18:27:27.806294Z"
    },
    "papermill": {
     "duration": 0.301237,
     "end_time": "2021-06-12T19:00:40.139085",
     "exception": false,
     "start_time": "2021-06-12T19:00:39.837848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 20.0, 0.96, 1.05)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(score_acc)\n",
    "plt.plot(score_val_accuracy)\n",
    "plt.axis([0,20,.96,1.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:40.377014Z",
     "iopub.status.busy": "2021-06-12T19:00:40.376109Z",
     "iopub.status.idle": "2021-06-12T19:00:41.335304Z",
     "shell.execute_reply": "2021-06-12T19:00:41.334284Z",
     "shell.execute_reply.started": "2021-06-12T18:27:27.993295Z"
    },
    "papermill": {
     "duration": 1.079905,
     "end_time": "2021-06-12T19:00:41.335433",
     "exception": false,
     "start_time": "2021-06-12T19:00:40.255528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:41.576542Z",
     "iopub.status.busy": "2021-06-12T19:00:41.575993Z",
     "iopub.status.idle": "2021-06-12T19:00:41.803653Z",
     "shell.execute_reply": "2021-06-12T19:00:41.803143Z",
     "shell.execute_reply.started": "2021-06-12T18:27:28.981410Z"
    },
    "papermill": {
     "duration": 0.351845,
     "end_time": "2021-06-12T19:00:41.803764",
     "exception": false,
     "start_time": "2021-06-12T19:00:41.451919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('CONV 2D.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.115747,
     "end_time": "2021-06-12T19:00:42.035670",
     "exception": false,
     "start_time": "2021-06-12T19:00:41.919923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The model above gave me a good 99. something accuracy which i was suprised to see as which my normal Nueral network i got 97. something at max.\n",
    "I would like the readers who are starting in kaggle to learn more about Convulation Network as it blew me away. I was reluctant to use it as i thought of it as some advanced topic and kept on delaying to learn it .I wish you would not make the same mistake as me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.115266,
     "end_time": "2021-06-12T19:00:42.269205",
     "exception": false,
     "start_time": "2021-06-12T19:00:42.153939",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "AUGMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.131894,
     "end_time": "2021-06-12T19:00:42.516731",
     "exception": false,
     "start_time": "2021-06-12T19:00:42.384837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now i read about data augmentation and this was easy to understand but i was dumbstruck on how to use it and keras documentaion did help me.\n",
    "If you are hearing about Data Augmentation for first time , please go through the link to get an Idea about this and i Promise you will love it.\n",
    "https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.133074,
     "end_time": "2021-06-12T19:00:42.785074",
     "exception": false,
     "start_time": "2021-06-12T19:00:42.652000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This was just a rough code as i wanted to see the images it generated .Curiousity got the best of me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:43.037064Z",
     "iopub.status.busy": "2021-06-12T19:00:43.036165Z",
     "iopub.status.idle": "2021-06-12T19:00:47.227039Z",
     "shell.execute_reply": "2021-06-12T19:00:47.224934Z",
     "shell.execute_reply.started": "2021-06-12T18:27:29.347521Z"
    },
    "papermill": {
     "duration": 4.312471,
     "end_time": "2021-06-12T19:00:47.227138",
     "exception": false,
     "start_time": "2021-06-12T19:00:42.914667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                401472    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 494,794\n",
      "Trainable params: 494,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:47.464798Z",
     "iopub.status.busy": "2021-06-12T19:00:47.464210Z",
     "iopub.status.idle": "2021-06-12T19:00:47.467591Z",
     "shell.execute_reply": "2021-06-12T19:00:47.468089Z",
     "shell.execute_reply.started": "2021-06-12T18:27:33.050013Z"
    },
    "papermill": {
     "duration": 0.124091,
     "end_time": "2021-06-12T19:00:47.468196",
     "exception": false,
     "start_time": "2021-06-12T19:00:47.344105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "datagen = image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:00:47.712169Z",
     "iopub.status.busy": "2021-06-12T19:00:47.711311Z",
     "iopub.status.idle": "2021-06-12T19:01:05.512444Z",
     "shell.execute_reply": "2021-06-12T19:01:05.511690Z",
     "shell.execute_reply.started": "2021-06-12T18:27:33.058050Z"
    },
    "papermill": {
     "duration": 17.92613,
     "end_time": "2021-06-12T19:01:05.512550",
     "exception": false,
     "start_time": "2021-06-12T19:00:47.586420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 1s 19ms/step - loss: 0.5775 - accuracy: 0.8424 - val_loss: 0.2804 - val_accuracy: 0.9099\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.1071 - accuracy: 0.9677 - val_loss: 0.0915 - val_accuracy: 0.9707\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0602 - accuracy: 0.9806 - val_loss: 0.0466 - val_accuracy: 0.9854\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0393 - accuracy: 0.9884 - val_loss: 0.0532 - val_accuracy: 0.9850\n",
      " 66/512 [==>...........................] - 2s 23ms/step - loss: 0.0287 - accuracy: 0.9908 - val_loss: 0.0275 - val_accuracy: 0.9914\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.0291 - val_accuracy: 0.9902\n",
      " 66/512 [==>...........................] - 1s 17ms/step - loss: 0.0183 - accuracy: 0.9938 - val_loss: 0.0248 - val_accuracy: 0.9921\n",
      " 66/512 [==>...........................] - 1s 17ms/step - loss: 0.0131 - accuracy: 0.9959 - val_loss: 0.0182 - val_accuracy: 0.9930\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.0116 - val_accuracy: 0.9961\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0090 - accuracy: 0.9972 - val_loss: 0.0060 - val_accuracy: 0.9982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(10,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:05.842731Z",
     "iopub.status.busy": "2021-06-12T19:01:05.841783Z",
     "iopub.status.idle": "2021-06-12T19:01:09.476577Z",
     "shell.execute_reply": "2021-06-12T19:01:09.477010Z",
     "shell.execute_reply.started": "2021-06-12T18:27:50.920278Z"
    },
    "papermill": {
     "duration": 3.803272,
     "end_time": "2021-06-12T19:01:09.477131",
     "exception": false,
     "start_time": "2021-06-12T19:01:05.673859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 3s 3ms/step - loss: 0.0045 - accuracy: 0.9987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.004490276798605919, 0.99871426820755]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:09.842307Z",
     "iopub.status.busy": "2021-06-12T19:01:09.841317Z",
     "iopub.status.idle": "2021-06-12T19:01:10.847627Z",
     "shell.execute_reply": "2021-06-12T19:01:10.846936Z",
     "shell.execute_reply.started": "2021-06-12T18:27:54.458026Z"
    },
    "papermill": {
     "duration": 1.191067,
     "end_time": "2021-06-12T19:01:10.847785",
     "exception": false,
     "start_time": "2021-06-12T19:01:09.656718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG.csv',index=False)\n",
    "model.save('DATA AUG.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.178016,
     "end_time": "2021-06-12T19:01:11.203575",
     "exception": false,
     "start_time": "2021-06-12T19:01:11.025559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You will see that i save my files in csv and their names might be a bit confusing but i do this to keep track of what i did to get the results like data aug means i added augmentation to my code of data .It is a little unorthodox but i will make sure to find better way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.178692,
     "end_time": "2021-06-12T19:01:11.559505",
     "exception": false,
     "start_time": "2021-06-12T19:01:11.380813",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I used GPU for these fittings and it saved me hours of time .Hatsoff to kaggle GPU for this , i would also suggest you to learn how to use it as it will help to make your code execute faster but only if you are training deep Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:11.928002Z",
     "iopub.status.busy": "2021-06-12T19:01:11.926787Z",
     "iopub.status.idle": "2021-06-12T19:01:12.077876Z",
     "shell.execute_reply": "2021-06-12T19:01:12.078317Z",
     "shell.execute_reply.started": "2021-06-12T18:27:55.542488Z"
    },
    "papermill": {
     "duration": 0.340849,
     "end_time": "2021-06-12T19:01:12.078456",
     "exception": false,
     "start_time": "2021-06-12T19:01:11.737607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7694789890>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAHWCAYAAABJxC7VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeXjU1d338ffJnkASkgAhIUAAkR0DhMWt1apVtBVRa9G61Gqx7a3dbO+qT1vb3n2Utmq326Xa+ri14larrbRqrYobZiPsO2SbBBLICtkz5/njN2DELJNkMkvyeV3XXJOZ35Lvz6J8rtPvOcdYaxEREREREd8KC3QBIiIiIiJDkYK2iIiIiMggUNAWERERERkECtoiIiIiIoNAQVtEREREZBAoaIuIiIiIDIJeg7Yx5lFjTKUxZks3x40x5nfGmD3GmE3GmAWdjl1gjNnpOXZbp++TjTGvG2N2e96TfPM4IiIiIiLBwZsR7ceAC3o4vgyY5nmtAh4EMMaEA/d7js8CrjTGzPJccxvwhrV2GvCG57OIiIiIyJDRa9C21q4Dqns4ZTnwhHWsB0YZY9KAxcAea+0+a20rsMZz7rFrHvf8/DhwSX8fQEREREQkGPmiR3s8UNrpc5nnu+6+B0i11lYAeN7H+qAOEREREZGgEeGDe5guvrM9fN+3mxuzCqclhREjRiycMWNGX28hIiIiItIn+fn5h6y1YwZyD18E7TJgQqfPGUA5ENXN9wAHjTFp1toKT5tJZXc3t9Y+DDwMkJ2dbfPy8nxQsoiIiIhI94wxxQO9hy9aR14GrvWsPrIUqPO0g+QC04wxk40xUcBKz7nHrrnO8/N1wEs+qENEREREJGj0OqJtjHkaOAsYbYwpA+4EIgGstQ8Ba4ELgT1AI3C951i7MeZm4FUgHHjUWrvVc9vVwLPGmBuAEuALPnwmEREREZGAM9b2uW06YNQ6IiIiIiL+YIzJt9ZmD+QevujRFhEREZEhpK2tjbKyMpqbmwNdyqCLiYkhIyODyMhIn99bQVtEREREPqasrIz4+HgyMzMxpquF5IYGay2HDx+mrKyMyZMn+/z+vpgMKSIiIiJDSHNzMykpKUM6ZAMYY0hJSRm0kXsFbRERERH5hKEeso8ZzOdU0BYRERGRoFJbW8sDDzzQ5+suvPBCamtrB6Gi/lHQFhEREZGg0l3Q7ujo6PG6tWvXMmrUqMEqq880GVJEREREgsptt93G3r17ycrKIjIykpEjR5KWlkZhYSHbtm3jkksuobS0lObmZr71rW+xatUqADIzM8nLy+PIkSMsW7aMM844g/fff5/x48fz0ksvERsb69fnUNAWERERkW799O9b2VZe79N7zkpP4M7Pz+72+OrVq9myZQuFhYW89dZbXHTRRWzZsuX4yiCPPvooycnJNDU1sWjRIi677DJSUlI+do/du3fz9NNP88gjj3DFFVfwwgsvcPXVV/v0OXqjoC0iIiIiQW3x4sUfW37vd7/7HS+++CIApaWl7N69+xNBe/LkyWRlZQGwcOFCioqK/FbvMQraIiIiItKtnkae/WXEiBHHf37rrbf497//zQcffEBcXBxnnXVWl8vzRUdHH/85PDycpqYmv9TamSZDioiIiEhQiY+Pp6GhoctjdXV1JCUlERcXx44dO1i/fr2fq/OeRrRFREREJKikpKRw+umnM2fOHGJjY0lNTT1+7IILLuChhx5i3rx5TJ8+naVLlwaw0p4Za22ga/Badna2zcvLC3QZIiIiIkPa9u3bmTlzZqDL8JuuntcYk2+tzR7IfdU6IiIyGOrKYP1Dga5CREQCSK0jIiK+ZC1sfh5euRVsB8y6GBLSA12ViIgEgIK2iIivNFY7AXvrX2HCEljxkEK2iMgwpqAtIuILe/8Df/sGHK2Cc34Mp38bwsIDXZWIiASQgraIyEC0NcHrd0LOH2D0dLhyDaRnBboqEREJAgraIiL9Vb4B/roKDu2CJV+Hc++EyNhAVyUiIkFCq46IiPRVRzus+xX88VxoOQLX/A2WrVbIFhHxkdraWh544IF+Xfub3/yGxsZGH1fUPwraIiJ9Ub0P/t8y+M/PYdZy+Mb7MPXsQFclIjKkDJWgrdYRERFvWAsFj8O/7oDwCLjsTzD38kBXJSIyJN12223s3buXrKwszjvvPMaOHcuzzz5LS0sLK1as4Kc//SlHjx7liiuuoKysjI6ODn70ox9x8OBBysvLOfvssxk9ejRvvvlmQJ9DQVtEpDdHKuHlW2DXv2Dyp+GSByAxI9BViYj4xz9vgwObfXvPcXOdlrturF69mi1btlBYWMhrr73G888/T05ODtZaLr74YtatW0dVVRXp6em88sorANTV1ZGYmMh9993Hm2++yejRo31bcz+odUREpCc7XoEHToW9b8IFq51+bIVsERG/ee2113jttdeYP38+CxYsYMeOHezevZu5c+fy73//mx/84Ae88847JCYmBrrUT9CItohIV1oa4F+3w4YnYdw8uPQRGDvDq0uPtrRTWFrLroMNXH/65EEuVERkkPUw8uwP1lpuv/12brrppk8cy8/PZ+3atdx+++189rOf5cc//nEAKuyegraIyIlK1sOLN0FtCZx5K3z6NoiI6vb0g/XN5BXVkFtUTX5xDdsq6ulwW4yBFfPHMyqu+2tFROST4uPjaWhoAOD888/nRz/6EV/60pcYOXIkLpeLyMhI2tvbSU5O5uqrr2bkyJE89thjH7s2GFpHFLRFRI5pb4W3V8O7v4ZRE+H6f8LEpR87xe227K48cjxU5xVXU1rdBEBMZBinZIzi65+eysLMJBZMTCIxNjIQTyIiEtJSUlI4/fTTmTNnDsuWLeOqq67i1FNPBWDkyJE89dRT7Nmzh+9///uEhYURGRnJgw8+CMCqVatYtmwZaWlpAZ8Maay1AS2gL7Kzs21eXl6gyxCRoahyB/z1q3BgEyy4Fs6/C6LjaW7roLC01gnVnnBd39wOwOiR0WRPSiI7M4nszGRmpSUQFaGpLyIS+rZv387MmTMDXYbfdPW8xph8a232QO6rEW0RGd7cbmf79NfvhOh46i95nPcjlpL3eil5xTVsLa+jrcMZkDhp7EgumpfGwknJZE9KYlJKHMaYAD+AiIgEKwVtERm2bF0ZTc/dRFzZu2wbeRo/dN9EwZpIIJ+oiDBOyUjkhjOmkD0piYWTkkgaoV5rERHxnoK2iAwbLe0dbC6rI6+4hrAtL7Cy6jeE23Z+0P5VXjt6HgszU7h9idMKMmd8ItER4YEuWUREQpiCtogMWTVHWz0TFp3+6k2uOmLa6/l55P/j4vAPKIqdzdYlv2TV7CxWjx6hNhARkU6stcPiv4uDOV9RQVtEhoS6pja2V9QffxWU1LKn8ggAkeGGOeMTuXPWQS4rvYvolsNw1g/JPP07ZIbrP4MiIieKiYnh8OHDpKSkDOmwba3l8OHDxMTEDMr99TeMiIQUt9tSXN34sVC9vaIBV23T8XOSR0RxSkYiK+aPJ3tSEqeMiybm7Z/Dhw/C6JPhmmcgfX4An0JEJLhlZGRQVlZGVVVVoEsZdDExMWRkDM6OvwraIhK0jra0s+NAPdsqGo6H6p0HGmhs7QAgzMCUMSNZMCmJLy2dyMy0BGalJTA2PvqjEZjyQnh0FRzaCUu+Buf+BCJjA/ZMIiKhIDIyksmTtbPtQCloi0jAWWtx1TaxvaKBbeWeUeoD9RQfbjx+TnxMBDPTErgiewIz0+KZmZbAyanxxER2M2HR3eFsPPPW3TBiDFzzIkz9jJ+eSEREREFbRPysua2DXQcbjrd8bKuoZ0dF/fFNYIyBSclxzEpL4LIFGcxMS2BmWjzjR8V63ydYvd/ZQr30Q5h9KVx0L8QlD+JTiYiIfJKCtogMCmstlQ0tbOvUR729op59VUdweyZ4x0WFM2NcPJ8/Jd0TqBOYMS6eEdED+E+TKx+eWOH8fOkfYe7lTnoXERHxMwVtEfGZfVVHeDqnxBOuG6g+2nr82PhRscxMS+DCOeOOh+qJyXGEhfkwBLsKnJAdOwqu+zskTfLdvUVERPpIQVtEBszttjy5vpi7/7kdt4UZ4+I5b2bq8V7qGWkJJMZGDm4R5RvgyUsgNhG+/A8YNXFwf5+IiEgvFLRFZEDKa5v4/vMbeW/PYc6aPoZfXDaP1ITBWY+0+yIK4YnlEJMIX35FIVtERIKCgraI9Iu1lhc3uLjz5a10uC13XzqXlYsm+H9jg2MhOzoRrtNItoiIBA8FbRHps8NHWrjjxc28uvUgizKTuPcLWUxMifN/IRUbPSE73mkXUU+2iIgEEQVtEemT17Ye4I4XN1Pf1M4dF87ghjOmEO7LCY3eqtikkC0iIkFNQVtEvFLf3MbP/r6N5/PLmJWWwJ9vzGL6uPjAFHNgMzxxMUSO8KwukhmYOkRERHqgoC0ivXp/7yG+/9wmKuqauOUzJ3HLZ6YRFREWmGIObIbHL4bIOGckO1lbBIuISHDy6m9KY8wFxpidxpg9xpjbujieZIx50RizyRiTY4yZ0+nYt4wxW4wxW40x3+70/U+MMS5jTKHndaFvHklEfKW5rYOf/n0rVz3yIVERYTz/9dO49bPTAxiyt3hCdqxCtoiIBL1eR7SNMeHA/cB5QBmQa4x52Vq7rdNpdwCF1toVxpgZnvPP8QTurwKLgVbgX8aYV6y1uz3X/dpae48Pn0dEfGRjaS3ffbaQvVVHue7USdy2bCaxUeGBK+jgVqddJCLGaRdJnhK4WkRERLzgzbDUYmCPtXaftbYVWAMsP+GcWcAbANbaHUCmMSYVmAmst9Y2WmvbgbeBFT6rXkR8rq3DzX2v7+LSB9+nsbWDp25Ywk+XzwlwyN4Gj38ewqOckeyUqYGrRURExEveBO3xQGmnz2We7zrbCFwKYIxZDEwCMoAtwKeMMSnGmDjgQmBCp+tu9rSbPGqMSerqlxtjVhlj8owxeVVVVV49lIj0z+6DDax44D1+98Zulp+Szr++/SnOmDY6sEVVbu8Usl9RyBYRkZDhTdDuat0ue8Ln1UCSMaYQuAXYALRba7cDvwBeB/6FE8jbPdc8CEwFsoAK4N6ufrm19mFrbba1NnvMmDFelCsifeV2W/74zj4u+v27lNc289DVC7jvi1mDv216byp3OCE7LMLZjEYhW0REQog3q46U8fFR6AygvPMJ1tp64HoA42wLt9/zwlr7J+BPnmN3ee6HtfbgseuNMY8A/+jvQ4hI/5VWN/K95zby4f5qzp2Zyt2XzmVMfHSgy/KE7M+BCXfaRUafFOiKRERE+sSboJ0LTDPGTAZcwErgqs4nGGNGAY2eHu4bgXWe8I0xZqy1ttIYMxGnveRUz/dp1toKzy1W4LSZiIifWGt5Nq+Un/19G8YYfnX5PC5fmOH/LdS7UrXTGck2YZ6QPS3QFYmIiPRZr0HbWttujLkZeBUIBx611m41xnzNc/whnEmPTxhjOoBtwA2dbvGCMSYFaAP+y1pb4/n+l8aYLJw2lCLgJh89k4j0orKhmdtf2MwbOyo5dUoKv/rCPDKSArCFeleqdsFjnwNjnHYRhWwREQlRxtoT262DV3Z2ts3Lywt0GSIhbe3mCv7Pi5tpbO3gBxfM4MunZRIWiC3Uu3JoNzx2EVjrTHwcc3KgKxIRkWHKGJNvrc0eyD20M6TIMFHX2MaPX97CS4XlzMtI5L4rsjhp7MhAl/WRQ7udkWzrVsgWEZEhQUFbZBhYt6uK/35+E4eOtPCdc0/mG2dPJTI8QLs7duXQHk/I7nDaRcZMD3RFIiIiA6agLTKENba2c9fa7Ty1voSTxo7kkWuzmZuRGOiyPu7wXmd1EXe7M/Fx7IxAVyQiIuITCtoiQ1R+cTW3PruR4upGbjxjMt87fzoxkQHc3bErh/c6I9kdbc626mNnBroiERERn1HQFhliWto7+O2/d/PQ23tJS4zl6a8uZemUlECX9UnHQ3aL0y6SOivQFYmIiPiUgrbIELK9op7vPFPIjgMNfDF7Aj/83EziYwK8u2NXqvc562R3tDgj2QrZIiIyBCloiwwBNUdbefyDIu5/cw+JsVH88dpszp2VGuiyula9Hx77PLQ1eUL27EBXJCIiMigUtEVCWGFpLU9+UMzfN5XT2u7morlp/M8lc0geERXo0rpWvd9pF2lrhOtehnFzAl2RiIjIoFHQFgkxzW0dvLyxnKfWF7OprI4RUeFckZ3BNUszmT4uPtDlda+myGkXaTvqjGSPmxvoikRERAaVgrZIiCg6dJQ/f1jMs3ll1DW1MW3sSH62fDYr5o8Pzj7szmqKnXaRlgaFbBERGTYUtEWCWIfb8uaOSp5YX8y6XVVEhBnOnz2Oa06dxJLJyRgTJFun96Sm2GkXaal32kXS5gW6IhEREb9Q0BYJQoePtPBMXil/Xl+Cq7aJ1IRovnPuyaxcPIHUhJhAl+e92hJnM5qWerj2JUg7JdAViYiI+I2CtkiQsNZSUFLLkx8UsXbzAVo73Jw6JYUfXjSTc2elBteW6d6oLXVGspvrnJCdnhXoikRERPxKQVskwBpb23m5sJwn1xeztbye+OgIrloykauXTuSksUE8ubEntaXw2EXQXAvX/A3S5we6IhEREb9T0BYJkH1VR3hqfQnP5ZfS0NzOjHHx/N8Vc7gkazwjokP4X826MqddpKkWrv0bjF8Q6IpEREQCIoT/NhcJPe0dbt7YUclT64t5Z/chIsMNy+akcc2pk8ielBQakxu709oIJe/DK9+Dxhq49kWFbBERGdYUtEX8oKqhhWdyS/jLhyWU1zWTlhjD9z57MlcsmsDY+BCa3NiZ2w0HNsG+N2Hvm1Cy3tlSPWYUXPMijF8Y6ApFREQCSkFbZJBYa8krruGJD4r515YK2josZ04bzZ0Xz+acGWOJCLXJjeC0hex90wnX+96CxsPO92Nnw+KvwpSzYdJpEBUX0DJFRESCgYK2iI8dbWnnb4UunvygmB0HGoiPieCapZl8aelEpo4ZGejy+qalAYredcL13v/A4d3O9yNT4aTzYOrZMOUsiB8XyCpFRESCkoK2iI/sqWzgqfUlvJBfRkNLO7PSElh96VwuzkonLipE/lXraIfyDZ52kP9AWS642yEiFjJPh4VfdsL12FkQyv3kIiIifhAif/uLBK/39x7if/+zh/f3HiYqPIyL5qVx9dJJLJg4KjQmN1bv+2jEev870FIHGGdzmdNucdpBJi6FiOhAVyoiIhJSFLRF+mnXwQZW/3MH/9lRSVpiDP99wXS+mD2BlJFBHkibamD/OidY730Taoud7xMnwKyLnRHryWfBiJSAlikiIhLqFLRF+qiyvpn7Xt/Fs3mljIiO4PZlM7jutExiIsMDXVrX2luhLOejSYzlG8C6ISoeJp8Jp97shOuUk9QOIiIi4kMK2iJeOtrSzsPr9vHwun20u91cd1om3/zMNJJGRAW6tI+zFg7t+mjEuuhdaDsKJgzGZ8Onvu+0g2RkQ3hkoKsVEREZshS0RXrR3uHm2bwyfv3vXVQ1tHDR3DT++4LpTEoZEejSPmIt7PoXbP+7E64byp3vk6fAKSudEevMMyF2VGDrFBERGUYUtEW6Ya3lPzsqufufO9hTeYTsSUn84ZqFLJiYFOjSPu7wXlj7PWcEO2YUTPm0M2I99WxIygx0dSIiIsOWgrZIFzaV1XLX2u2s31fN5NEjeOjqhZw/OzW4VhFpb4H3fgfrfgXhUXDBL2DRjRCuf61FRESCgf5GFumktLqRe17byUuF5aSMiOJny2dz5eKJRAbbLo7718E/vutsIDPrErjgbkhID3RVIiIi0omCtghQ19jG/W/t4bH3ijAG/uvsqXzt01OJjwmyyYJHD8FrP4SNT8OoSXDVc3DyZwNdlYiIiHRBQVuGtZb2Dp78oJjf/2cP9c1tXLYgg1s/ezJpibGBLu3j3G7Y8CS8/mNoPQpn3gpnfg+i4gJdmYiIiHRDQVuGJWst/9hUwS9f3UFpdRNnThvN7ctmMis9IdClfdLBrfCP70DphzDxNPjcr2HsjEBXJSIiIr1Q0JZhJ2d/Nf937XY2ltYyY1w8T3xlMZ86eUygy/qk1qPw9i/gg/shOgGW3w9ZX9KmMiIiIiFCQVtCW84j0N4M81bCyJ7D8t6qI6z+5w5e33aQcQkx/OryeVy6IIPwsCAMrjv/CWu/D3WlMP9qOPdn2hJdREQkxChoS+iq2umEUSz8+ydw8gWw4FqYes7Hlriramjht2/s4umcUmIjw/n++dP5yumTiY0Kwi3T68rgnz+AHf+AMTPgy2sh8/RAVyUiIiL9oKAtoevdX0NkLFzzohNMN65x3uPTIOsqmmdfySNb4aG399LS7uZLSybyzXOmMXpkdKAr/6SOdsj5A7x5F7g74Jw74dSbISLItncXERERryloS2iqKYJNz8KSr8HEpc7rnDth17+wBU9i3/k1Me/cyyL3TEaMW87Zl9zI5PQg7MMGKMuDv38bDm6Gk86Di+7Rjo4iIiJDgIK2hKb3fgdh4XDazce/smERvB22hNVVidQ0f55bknO5PPwtllauhscfhLmXwfxrIH1+cEwobKqFN34GeY9C/Di44gmYeXFw1CYiIiIDpqAtoafhAGx4CrKuOr4b4tbyOu5eu4N39xxiYnIcP77qXC6cezXGWih+z1mDuvAvTqhNneME7nlXQFyy/+u3FjY/D6/eAY2HnFH5s++AmCBcWlBERET6zVhrA12D17Kzs21eXl6gy5BAe/X/wPoH4JYCysPGcc9rO3lxg4vE2Ei++ZlpXL10ElERXWyZ3lQLW56HgiehohDCo2DG52DBNTD5LAjzwzbrh/fCK9+FfW85I+uf+w2kZw3+7xUREZE+McbkW2uzB3IPjWhLaGmshrz/B3Mu5/UDcXz3mXW0dLhZ9akpfOOsk0iM7WHL9NhRsOhG53VgsxO4Nz0DW/8KiRNh/pecdapHTfB93e0t8O5v4J17ISIaLrwHsr/itL+IiIjIkKQRbQktb94Fb/+Cx+ev4c4P3Mwdn8j9Vy1gYko/tyJva3ZWKtnwlDPKDDD1bKe1ZMZFTigeqH1vO6PYh/fA7Evh/LsgIW3g9xUREZFBoxFtGV6a67HrH6Ig9nTu/MDN5Qsz+Pklc4iJHMCocGQMzL3cedUUO33chX+G56+H2GSY90WntSR1dt/vfaQSXvuhM2qelAlXvwAnndv/WkVERCSkaERbQkbVP3/BmA/vYkX7z7n0cxdz9ZKJmMFYocPd4Yxub3gSdrwCHa2QvsAJ3HMug5jEXq53Q8Hj8O87obURzvg2nHmrs+a3iIiIhARfjGgraEtIWLthH4v+dhZ7wjKJ+vLfWDjJT6uFHD0Mm591+rkrt0JELMy+xGktmXTaJ5fiO7AF/vEdKMuBzDPhovtgzMn+qVVERER8Rq0jMuS1d7j55as7aX7vQS6MrCPiCz8lyV8hG2BECiz9urMEX3mBE7i3vAAbn4bkqTD/ameZwaiR8PZq+OABZ9LlJQ/BKSu1JraIiMgw5tWItjHmAuC3QDjwR2vt6hOOJwGPAlOBZuAr1totnmPfAr4KGOARa+1vPN8nA88AmUARcIW1tqanOjSiPbxUH23llqcLyNlzkNz475OQmknYDa8GPry2NsK2l5zWkuL3wIQ74brxMCy4Fs79aWDW5xYRERGf8cWIdq8LBxtjwoH7gWXALOBKY8ysE067Ayi01s4DrsUJ5Rhj5uCE7MXAKcDnjDHTPNfcBrxhrZ0GvOH5LALA5rI6Pv/7d8ktquEvS4oZ1XaQsE9/P/AhGyAqDrKuhOvXwi0FcPo3IWMxfOVVuPj3CtkiIiICeBG0cULyHmvtPmttK7AGWH7CObNwwjLW2h1ApjEmFZgJrLfWNlpr24G3gRWea5YDj3t+fhy4ZEBPIkPG8/llXPbQ+1hreX7VYhaVPQ7j5gXnih0pU+Hcn8BVa2Di0kBXIyIiIkHEm6A9Hijt9LnM811nG4FLAYwxi4FJQAawBfiUMSbFGBMHXAgc2w0k1VpbAeB5H9vfh5ChobXdzY9f2sL3nttI9qQk/n7LGcyrf9tZf/rMW4NjNFtERETES95Mhuwq3ZzY2L0a+K0xphDYDGwA2q21240xvwBeB47gBPL2vhRojFkFrAKYOHFiXy6VEFJZ38w3/lxAXnENqz41hf8+fzoRYQbeuQ9GnwwzLw50iSIiIiJ94k3QLuOjUWhwRqrLO59gra0HrgcwzsLG+z0vrLV/Av7kOXaX534AB40xadbaCmNMGlDZ1S+31j4MPAzOZEjvHktCSX5xNV9/qoCG5nZ+f+V8Pn9KunNg16twcDNc8iCEefN/voiIiIgED2/SSy4wzRgz2RgTBawEXu58gjFmlOcYwI3AOk/4xhgz1vM+Eae95GnPeS8D13l+vg54aSAPIqHHWsuT64tZ+fB6YqPCefG/TvsoZFsL6+6BURNh7hcCW6iIiIhIP/Q6om2tbTfG3Ay8irO836PW2q3GmK95jj+EM+nxCWNMB7ANuKHTLV4wxqQAbcB/dVrCbzXwrDHmBqAEUJoaRprbOvjR37bwXH4ZZ08fw2++OJ/EuMiPTih6x9n05aJ7ITyy+xuJiIiIBCntDCl+56pt4utP5bOprI5vfuYkvn3uyYSFnTAV4InlULkdvrUJImMCU6iIiIgMW9oZUkLO+3sPcfNfNtDa7ubhaxby2dnjPnlSWT7sewvO+x+FbBEREQlZCtriF9Za/vTufu7+5w4mjx7BH65ZyNQxI7s++Z17IGYUZH/Fv0WKiIiI+JCCtgy6xtZ2fvDCZv6+sZxlc8bxqy+cwsjobv7oHdwKO9fCWbdDdDdBXERERCQEKGjLoCo+fJSbnsxn18EG/vuC6Xz901MxPW088859EDUSFq/yX5EiIiIig0BBWwbNmzsr+dbTGwgLMzx2/WI+dfKYni84vBe2/hVOvRnikv1TpIiIiMggUdAWn3O7Lfe/uYf7/r2LmeMS+MM1C5mQHNf7he/9BsIinaAtIiIiEuIUtMWnGprb+O6zG3l920FWzB/PXSvmEhsV3vuFdWVQ+DQs/DLEpw56nSIiIiKDTaSXP3gAACAASURBVEFbfGZPZQOrnsyn+HAjd35+Fl8+LbPnfuzO3v9fwMLp3xzUGkVERET8RUFbfOJfWw5w67OFxEaF85cbl7BkSor3Fx+pgvzHYN4XnS3XRURERIYABW0ZkA635b7Xd3L/m3vJmjCKB69eQFpibN9usv4BaG+GM74zOEWKiIiIBICCtvRbbWMr31xTyLpdVVy5eAI/uXg20RFe9GN31lQLuX+EWcth9LTBKVREREQkABS0pV/Kahq58pH1HKxr4e5L53Ll4n62fOQ+Ai31cOatvi1QREREJMAUtKVfHlm3j4P1LTxz01LmT0zq301aj8IHD8C0z0LaPN8WKCIiIhJgYYEuQEJPc1sHL25wsWzOuP6HbID8x6GpGs78nu+KExEREQkSCtrSZ2s3V1Df3M7KRQNYIaS9Bd7/HWSeCROX+K44ERERkSChoC19tianlMmjR7B0ygC2SS/8CzRUwJnf9V1hIiIiIkFEQVv6ZE/lEXKKqvniogneb0Zzoo52Z7v19AUw5WzfFigiIiISJBS0pU/W5JQQEWa4bEFG/2+y9a9QUwSf+h70N6yLiIiIBDkFbfFaS3sHLxSUcd6sVMbER/fvJm43vHMfjJkJJy/zbYEiIiIiQURBW7z22taD1DS2sbK/a2YD7FwLVduddbPD9MdPREREhi4lHfHamtwSxo+K5cyTRvfvBtbCO/dAUibMXuHT2kRERESCjYK2eKX48FHe23OYLy6aQFhYP/uq970J5RvgjO9AuPZKEhERkaFNQVu88kxuKWEGvpA9gEmQ6+6F+HQ45UrfFSYiIiISpBS0pVdtHW6eyy/jMzPGkpYY27+blKyH4nfhtFsgop8TKUVERERCiIK29Oo/OyqpamgZ2E6Q79wLcSmw8DrfFSYiIiISxBS0pVdrckpITYjmrOlj+neDio2w+zVY+g2IGuHb4kRERESClIK29MhV28Tbu6q4InsCEeH9/OPyzr0QnQCLbvRtcSIiIiJBTEFbevRsbikWuCJ7Qv9uULULtr0Mi78KsaN8WpuIiIhIMFPQlm51uC3P5ZVyxkmjmZAc17+bvPtriIhx2kZEREREhhEFbenWul1VlNc1c2V/d4KsKYZNz8DCL8OIfm5yIyIiIhKiFLSlW0/nlJAyIopzZ6b27wbv/w5MmLOkn4iIiMgwo6AtXaqsb+aNHZVcvjCDqIh+/DFpOAAFT0LWlZA43vcFioiIiAQ5BW3p0nP5ZXS4LV9c1M9JkB/cD+42OP3bvi1MREREJEQoaMsnuN2WZ3JLWTI5mSljRvb9Bo3VkPcozLkMUqb6vkARERGREKCgLZ/wwb7DlFQ3ctWSfk6C/PAP0HoEzviubwsTERERCSEK2vIJT+eUMCoukvNnj+v7xS0N8OFDMP0iSJ3l++JEREREQoSCtnzM4SMtvLb1ICvmjycmMrzvN8h7FJpr4cxbfV+ciIiISAhR0JaP+WuBi9YOd//Wzm5rgvf/F6acBRkLfV2aiIiISEiJCHQBEjystTydW8KCiaM4OTW+7zfY8BQcrYQzH/V9cSIiIiIhRiPaclxuUQ37qo6ysj+j2R1t8N7vIGMxZJ7h++JEREREQoyCthy3JqeE+OgIPjcvre8Xb3oW6krgU98DY3xfnIiIiEiIUdAWAOoa23hlcwUXZ6UTF9XHjiJ3B7z7a0idC9M+OzgFioiIiIQYBW0B4G+FLlra+zkJcvvLcHg3nPldjWaLiIiIeChoizMJMqeEOeMTmDM+sa8Xwzv3Qso0mLV8cAoUERERCUFadSQUbfgzVBRCYobzSvC8x4+DsL6vfb2xrI4dBxr4+SVz+l7L7tfhwGZY/kC/freIiIjIUOVV0DbGXAD8FggH/mitXX3C8STgUWAq0Ax8xVq7xXPsO8CNgAU2A9dba5uNMT8BvgpUeW5zh7V27YCfaKhzu+FftztbnNuOjx8z4ZCQ/lEAT8yAhPGQOMHzeTzEjPpEe8eanBJiI8NZnpXet1qshXfuce4/74oBPpiIiIjI0NJr0DbGhAP3A+cBZUCuMeZla+22TqfdARRaa1cYY2Z4zj/HGDMe+CYwy1rbZIx5FlgJPOa57tfW2nt89zjDQPU+aKmDi3/vtGrUuaCuDOrLnPe6Mue70hzY+iK42z9+fdTITgE8g5aR6ZiNdXx76snEN5ZCxHiIiPauluL3oPRDuPAeCI/0/bOKiIiIhDBvRrQXA3ustfsAjDFrgOVA56A9C7gbwFq7wxiTaYxJ7fQ7Yo0xbUAcUO6r4oclV77zPn4hxCQ6r9RZXZ/r7oAjlVDvgrrSj0L4sZ8PbCL6aBV3hwFFwO88140Y+9EI+LHR8M4j4yPGQFgYrLvHOXf+1YP/3CIiIiIhxpugPR4o7fS5DFhywjkbgUuBd40xi4FJQIa1Nt8Ycw9QAjQBr1lrX+t03c3GmGuBPOBWa21NP59j+HDlQ+QIGDOj93PDwiEhzXllZHd5yuW/f4P4lioevXQc5tjoeF2pE86rdsKeN6Ct8eMXhUdBfBrUFsO5P4XIWB88mIiIiMjQ4k3Q7mq9NnvC59XAb40xhTh92BuAdk/v9nJgMlALPGeMudpa+xTwIPA/nnv9D3Av8JVP/HJjVgGrACZO7MfSc0ONKx/Ss3wy8XBreR15rmbu/PwSzJTJXZ9kLTTVfNSW0nl0PD0LFt0w4DpEREREhiJvgnYZMKHT5wxOaP+w1tYD1wMYYwyw3/M6H9hvra3yHPsrcBrwlLX24LHrjTGPAP/o6pdbax8GHgbIzs4+MeAPL+2tcGATLLnJJ7dbk1NKVEQYK+aP7/4kYyAu2XmlzfPJ7xUREREZDrxZRzsXmGaMmWyMicKZzPhy5xOMMaM8x8BZYWSdJ3yXAEuNMXGeAH4OsN1zTed9vlcAWwb2KMPAwS3Q0er0Zw9QU2sHfyt0ceGccYyKi+r9AhERERHpk15HtK217caYm4FXcZb3e9Rau9UY8zXP8YeAmcATxpgOnEmSN3iOfWiMeR4oANpxWkoe9tz6l8aYLJzWkSLAN8O0Q1l5gfPug6D9yuYKGprbWdmfnSBFREREpFderaPtWd967QnfPdTp5w+Aad1ceydwZxffX9OnSgVcBRA32ln9Y4DW5JQwZfQIlkxO9kFhIiIiInIibcEeSlz5zmi26Wp+qvd2H2wgr7iGLy6agBngvURERESkawraoaK53lluzwdtI2tyS4kMN1y2MMMHhYmIiIhIVxS0Q0VFIWAHHLSb2zr4a0EZ581KZfRIL3eAFBEREZE+U9AOFcd3hFwwoNu8uvUANY1tXKlJkCIiIiKDSkE7VLgKIGmys571AKzJKWVCciynTx3to8JEREREpCsK2qHCVTDg0eyiQ0f5YN9hvpg9gbAwTYIUERERGUwK2qGg4QDUlw24P3tNbinhYYYvZA98eUARERER6ZmCdihwDXyjmrYON8/nl3H29LGkJsT4qDARERER6Y6Cdihw5YMJh3Hz+n2LN7Yf5NCRFq5crNFsEREREX9Q0A4FrnxInQVRcf2+xdM5pYxLiOHTJ4/xYWEiIiIi0h0F7WBnLZQXDKhtpKymkXW7q7giO4OIcP1PLiIiIuIPSl3BrnofNNdBev9XHHk2rwyAKxapbURERETEXxS0g93xjWr6N6Ld3uHmubxSzpw2hoyk/reeiIiIiEjfKGgHO1c+RMbBmBn9uvztXVVU1DVzpUazRURERPxKQTvYufIhLQvCI/p1+dM5pYweGc25s1J9XJiIiIiI9ERBO5i1t0LFpn7vCHmwvpk3d1Zy+cIMIjUJUkRERMSvlL6CWeVW6Gjpd3/2c3mldLgtK9U2IiIiIuJ3CtrBbAA7QrrdlmfySjl1SgqZo0f4uDARERER6Y2CdjBzFUBcCoya2OdL39t7iNLqJlZqJ0gRERGRgFDQDmaufGc025g+X7omp5RRcZGcP3vcIBQmIiIiIr1R0A5WLQ1QtaNfbSOHj7Tw2rYDXDo/g5jI8EEoTkRERER6o6AdrMoLAduvoP1CQRltHZYr1TYiIiIiEjAK2sHq2I6Qfdx63VrLmpxSFk5KYlpq/CAUJiIiIiLeUNAOVuUFkJQJI1L6dNmH+6vZd+iolvQTERERCTAF7WDlKujzaDbAmpwS4qMjuGhe2iAUJSIiIiLeUtAORg0Hoa60z/3ZtY2trN1ygEvmjycuqn9btouIiIiIbyhoB6Py/m1U8+IGF63tbq2dLSIiIhIEFLSDkSsfTDikzfP6kmOTIOdlJDI7PXEQixMRERERbyhoByNXPoydBVHeb52+obSWnQcbWLmo77tIioiIiIjvKWgHG2udiZDj+zYRck1OCXFR4VyclT5IhYmIiIhIXyhoB5vqfdBc26eg3dDcxt83VvD5eemMjNYkSBEREZFgoKAdbFx9nwj58sZymto6NAlSREREJIgoaAcbVz5ExMKYmV5fsianlBnj4smaMGoQCxMRERGRvlDQDjaufEjPgnDvWkC2uOrY7Kpj5aIJGGMGuTgRERER8ZaCdjDpaIMDm/rUNvJ0TgnREWGsmJ8xiIWJiIiISF8paAeTym3Q3uz1RMjG1nZeKiznwrlpJMZFDnJxIiIiItIXCtrBxJXvvKd7F7T/samCIy3tXLlYa2eLiIiIBBsF7WDiyofYZEjK9Or0NTklTB0zgkWZSYNbl4iIiIj0mYJ2MHEVOP3ZXkxq3HWwgYKSWlYumqhJkCIiIiJBSEE7WLQ0QOV2rydCPp1TQmS44dIF4we5MBERERHpDwXtYFGxEbBeBe3mtg5e3ODis7PHkTIyevBrExEREZE+U9AOFscmQnqx4sirWw9Q29jGlYs0CVJEREQkWCloBwtXAYyaBCNG93rqM7mlTEiO5bSpKX4oTERERET6Q0E7WLgKvBrNbm7rILeomgvnpBEWpkmQIiIiIsFKQTsYHKmEuhKv+rM3lNTS1mFZPDnZD4WJiIiISH8paAcDV4Hz7kXQzi2qxhjInqSgLSIiIhLMvAraxpgLjDE7jTF7jDG3dXE8yRjzojFmkzEmxxgzp9Ox7xhjthpjthhjnjbGxHi+TzbGvG6M2e15H767rrjywYRB2im9nppbVM301HhtuS4iIiIS5HoN2saYcOB+YBkwC7jSGDPrhNPuAAqttfOAa4Hfeq4dD3wTyLbWzgHCgZWea24D3rDWTgPe8Hwenlz5MHYWRI3o8bT2DjcFxTUsytRotoiIiEiw82ZEezGwx1q7z1rbCqwBlp9wziycsIy1dgeQaYxJ9RyLAGKNMRFAHFDu+X458Ljn58eBS/r9FKHMWij3biLktop6jrZ2sEj92SIiIiJBz5ugPR4o7fS5zPNdZxuBSwGMMYuBSUCGtdYF3AOUABVAnbX2Nc81qdbaCgDP+9iufrkxZpUxJs8Yk1dVVeXdU4WSmv3QVAPpvQftnP3VACzWiLaIiIhI0PMmaHe1hpw94fNqIMkYUwjcAmwA2j1918uByUA6MMIYc3VfCrTWPmytzbbWZo8ZM6Yvl4aGPk6EnJAcy7jEmEEuSkREREQGypugXQZM6PQ5g4/aPwCw1tZba6+31mbh9GiPAfYD5wL7rbVV1to24K/AaZ7LDhpj0gA875UDepJQ5cqHiFgYO7PH06y15BWpP1tEREQkVHgTtHOBacaYycaYKJzJjC93PsEYM8pzDOBGYJ21th6nZWSpMSbOGGOAc4DtnvNeBq7z/Hwd8NLAHiVEufKd1UbCe15FZG/VUQ4fbVXbiIiIiEiI6DVoW2vbgZuBV3FC8rPW2q3GmK8ZY77mOW0msNUYswNndZJvea79EHgeKAA2e37fw55rVgPnGWN2A+d5Pg8vHW1QsdHrthFAEyFFREREQkSENydZa9cCa0/47qFOP38ATOvm2juBO7v4/jDOCPfwVbkd2pu9WnEkd381o0dGMWV0z0sAioiIiEhw0M6QgeTKd969CNo5RdVkT0rG6cARERERkWCnoB1IrnyITYKkyT2eVlHXRFlNk9pGREREREKIgnYguQqc/uxeRqm1fraIiIhI6FHQDpSWI1C13euJkCOiwpmZFu+HwkRERETEFxS0A6ViI1i3d0F7fw0LJiUREa7/uURERERChZJboJR7doTsZev12sZWdh5sUNuIiIiISIhR0A4UVz4kToSRPW8rn1tUA2j9bBEREZFQo6AdKK5879bPLqomMtyQNWGUH4oSEREREV9R0A6EI1VQW+JVf3bO/mrmZYwiJjLcD4WJiIiIiK8oaAfCsf7sXoJ2Y2s7W1x1LFJ/toiIiEjIUdAOBFc+mDBIO6XH0wpLaml3WxZPTvJTYSIiIiLiKwrageAqgDEzIXpkj6flFFVjDCycpBFtERERkVCjoO1v1vZpIuT01HgSYyP9UJiIiIiI+JKCtr/VFEFTda9Bu63DTUFxLYu1rJ+IiIhISFLQ9jdXvvPey0TIreX1NLV1aCKkiIiISIhS0PY3VwFExMDYWT2elru/GkAj2iIiIiIhSkHb31z5zmoj4T33XecUVTMxOY7UhBg/FSYiIiIivqSg7U8d7VCxsde2EbfbkldUrdFsERERkRCmoO1PVduhvanXoL236gg1jW0sVn+2iIiISMhS0PanYxMh0+f3eFpOkdOfvUgj2iIiIiIhS0Hbn1z5EDMKkqf0eFru/mpGj4wmMyXOT4WJiIiIiK8paPuTq8BpGzGmx9Nyi2pYPDkJ08t5IiIiIhK8FLT9pfUoVG7rtT/bVduEq7ZJ62eLiIiIhDgFbX+p2AjW3WvQPrZ+toK2iIiISGhT0PYXV4Hz3svW6zlF1cRHRzAzLcEPRYmIiIjIYFHQ9hdXPiROgJFjezwtd381CyYlER6m/mwRERGRUKag7S+u/F5Hs2uOtrK78og2qhEREREZAhS0/eHoIagt7r0/u0j92SIiIiJDhYK2Pxzvz+49aEeFhzEvI9EPRYmIiIjIYFLQ9gdXPpgwSMvq8bScohpOmZBITGS4nwoTERERkcGioO0P5QUwZgZEj+z2lMbWdra66tQ2IiIiIjJEKGgPNmudEe30nidCbiippd1tWaSJkCIiIiJDgoL2YKsthsbDva+fvb8aY2DhpCQ/FSYiIiIig0lBe7C58p13LyZCzhyXQEJMpB+KEhEREZHBpqA92FwFEB4NqbO7PaW13U1BSY3WzxYREREZQhS0B5srH9JOgfDuR6q3lNfR3ObWREgRERGRIURBezB1tEPFxt7bRvZ7NqqZrP5sERERkaFCQXswVe2Atkav+rMzU+IYGx/jp8JEREREZLApaA+m4xMhu19xxO225BbVqG1EREREZIhR0B5MrnyISYTkKd2esrvyCHVNbVo/W0RERGSIUdAeTK4Cp23EmG5PySly+rMXa0RbREREZEhR0B4srY1Quc2riZBj4qOZlBLnp8JERERExB8UtAfLgU1gO3oM2tZacouqWZyZjOlh1FtEREREQo+C9mA5NhEyvfuJkGU1TVTUNbMoU8v6iYiIiAw1CtqDxZUPCRkQn9rtKblFx9bPVn+2iIiIyFDjVdA2xlxgjNlpjNljjLmti+NJxpgXjTGbjDE5xpg5nu+nG2MKO73qjTHf9hz7iTHG1enYhb59tABz5fe4rB84QTs+OoIZ4xL8VJSIiIiI+EuvQdsYEw7cDywDZgFXGmNmnXDaHUChtXYecC3wWwBr7U5rbZa1NgtYCDQCL3a67tfHjltr1w78cYLE0cNQU9TrRMic/dUszEwiPEz92SIiIiJDjTcj2ouBPdbafdbaVmANsPyEc2YBbwBYa3cAmcaYE3smzgH2WmuLB1hz8CsvcN57CNqHj7Swt+qoNqoRERERGaK8CdrjgdJOn8s833W2EbgUwBizGJgEZJxwzkrg6RO+u9nTbvKoMWbozAh0FQAG0rO6PSW3qAaAJerPFhERERmSvAnaXfU12BM+rwaSjDGFwC3ABqD9+A2MiQIuBp7rdM2DwFQgC6gA7u3ylxuzyhiTZ4zJq6qq8qLcIODKhzEzIDq+21Nyi6qJighjbkaiHwsTEREREX/xJmiXARM6fc4AyjufYK2tt9Ze7+nFvhYYA+zvdMoyoMBae7DTNQettR3WWjfwCE6LyidYax+21mZba7PHjBnj1UMFlLVeT4TMmjCK6IhwPxUmIiIiIv7kTdDOBaYZYyZ7RqZXAi93PsEYM8pzDOBGYJ21tr7TKVdyQtuIMSat08cVwJa+Fh+Uakug8VCPQftoSztby+u17bqIiIjIEBbR2wnW2nZjzM3Aq0A48Ki1dqsx5mue4w8BM4EnjDEdwDbghmPXG2PigPOAm0649S+NMVk4bShFXRwPTcc2qulhImRBSQ0dbqv1s0VERESGsF6DNoBn6b21J3z3UKefPwCmdXNtI5DSxffX9KnSUOHKh/BoGDu721Ny91cTZmDBxFF+LExERERE/Ek7Q/qaqwDS5kFEVLen5BRVMys9gfiYSD8WJiIiIiL+pKDtSx3tUFHYY9tIa7ubDSW1Wj9bREREZIhT0PalQzuhrRHSu58IudlVR0u7WxMhRURERIY4BW1f8mIiZG5RNQDZCtoiIiIiQ5qCti+58iEmEZKndHtK7v5qpowewZj4aD8WJiIiIiL+pqDtS658p20krOt/rG63Ja+4Rv3ZIiIiIsOAgravtDbCwW09to3sqmygrqlN62eLiIiIDAMK2r5yYDPYjp77s/c7/dmaCCkiIiIy9Clo+8rxiZDdrziSU1RDakI0E5Jj/VSUiIiIiASKgravuPIhYTzEj+vysLWW3P3VLMpMxhjj5+JERERExN8UtH3Fld/jaHZZTRMH6ptZrP5sERERkWFBQdsXGquhZn+P/dkfevqzteKIiIiIyPCgoO0LrgLnvZeJkAkxEUxPjfdTUSIiIiISSAravlBeABhIy+r2lNyiarIzkwkLU3+2iIiIyHCgoO0LrnwYMx1iEro8XNXQwr5DR9U2IiIiIjKMKGgPlLUf7QjZjbwiz/rZk5P8VZWIiIiIBJiC9kDVlcLRql7Wz64mOiKMueNH+bEwEREREQkkBe2BOr5RTQ8TIYuqyZowiqgI/eMWERERGS6U/AbKlQ/hUZA6p8vDDc1tbCuv1/rZIiIiIsOMgvZAuTbAuHkQEdXl4YKSWtxW62eLiIiIDDcK2gPh7oDyDb2unx1mYMEkTYQUERERGU4UtAeiaie0He11IuTs9ERGRkf4sTARERERCTQF7YHoZSJkS3sHhaW1ahsRERERGYYUtAfClQ/RiZA8tcvDm8vqaG13a/1sERERkWFIQXsgXPkwfj6Edf2PMcezUU22RrRFREREhh0F7f5qa4LKbb1OhJwyZgSjR0b7sTARERERCQYK2v11YDO427sN2h1uS15xDUu0fraIiIjIsKSg3V/HJkKmd73iyM4DDTQ0t2sipIiIiMgwpaDdX658iE+HhLQuD+d6+rMVtEVERESGJwXt/nLl97p+dlpiDBlJsX4sSkRERESChYJ2fzRWQ/W+bvuzrbXk7q9mUWYyxhg/FyciIiIiwUBBuz/KNzjv3QTtkupGKhtaWKSJkCIiIiLDloJ2f7gKAAPpWV0eztnv9GcvVn+2iIiIyLCloN0frnwYfTLEJHZ5OLfo/7d3r7GWlfd9x7//czVzMTPEZsIt4CRTx4QkQKejuBcrEk1lUGRsq6mwqhq5WA5SQfWLSkWOGrnv7LRpRSvLiMSkOLLsJG6JUUTiWKiS39jMwTBg4xkHzBzCmDGMmTFz9kzY+1z+fbHWOazZsy/PntlnzpnO9yNt7b1u56z96JnFj+f817OOcekl0+y+fNt5PjFJkiRtFgbtUWUOvRFybv44/+i6nUxMWJ8tSZJ0sTJoj+qNw3Dytb712a8tvMmhn5x0Wj9JkqSLnEF7VKsPqukzov3k/HEAb4SUJEm6yBm0R/Wj78DkDOy6oefmfYeO8bbpCW64snf9tiRJki4OBu1RvfI0/OyvwNRsz81z88e46ZqdzEzZtJIkSRcz0+AoVparoN2nPvvEm4scOHLCshFJkiQZtEfyk7+FTguu7F2f/Z2XjrOSzp8tSZIkg/Zojh6s3nf9cs/Nc4eOMTkR3PRzO87jSUmSJGkzMmiPor1QvV/SO0jPzR/jhivfztbZqfN4UpIkSdqMDNqjaLeq95kzn/j45uIyz7z8hvNnS5IkCTBoj6ZTB+3Z7WdsevbwG3SWV7wRUpIkSUBh0I6I90fEDyLihYi4r8f2nRHxSEQ8GxH7IuKGev27I2J/43UiIj5Zb7ssIr4REc/X7zvH+9XWQXsBJmdhcvqMTXPzxwAc0ZYkSRJQELQjYhL4HHArcD3wkYi4vmu3TwH7M/NXgY8C9wNk5g8y88bMvBH4h8Ap4JH6mPuAxzNzN/B4vby5dVowe2bZCFQPqvnFy7dx2daZ83xSkiRJ2oxKRrT3Ai9k5ouZ2QG+Atzetc/1VGGZzDwIXBcRu7r2uQX4YWa+VC/fDjxcf34Y+OBZnP/51W71rM9eXkmeeum4o9mSJElaUxK0rwJebiwfrtc1PQN8GCAi9gLXAld37XMH8OXG8q7MPAJQv19eftobpNPqWZ994MgJFtpL7H3X5q9+kSRJ0vlRErSjx7rsWv4MsDMi9gP3Ak8DS2s/IGIG+ADw56OeYER8IiKejIgnjx49Ourh49XpPaJtfbYkSZK6lUz4fBi4prF8NfBKc4fMPAF8DCAiAjhUv1bdCjyVma821r0aEVdk5pGIuAJ4rdcvz8wHgQcB9uzZ0x3wz692C7acGabn5o9x5aVv4+qdWzbgpCRJkrQZlYxozwG7I+Jd9cj0HcCjzR0iYke9DeDjwDfr8L3qI5xeNkL9M+6sP98JfG3Ukz/veoxoZyb7Dh13Wj9JkiSdZuiIdmYuRcQ9wNeBSeChzHwuIu6utz8AvAf4YkQsA98H7lo97Kyu2wAAEKNJREFUPiK2AL8J/E7Xj/4M8GcRcRfwd8Bvj+H7rK/2mbOOzL9+ip+02paNSJIk6TRFzwrPzMeAx7rWPdD4/C1gd59jTwE/02P961QzkVw4Oi2YOf1myLlDVX32Xke0JUmS1OCTIUtl9pxHe9/8MXZsmeYX39l7fm1JkiRdnAzapRZPQa6cUaM9N3+MPddexsREr8lZJEmSdLEyaJdqt6r3xoj2ayfe5KXXTzl/tiRJks5g0C7VqYN2o0Z7n/NnS5IkqQ+Ddqn2QvXeGNGeO3SMS6YnueGqSzfopCRJkrRZGbRLrY1ob11btW/+ODdfu4PpSZtRkiRJpzMhlmqfXjryxt8vcvDHJywbkSRJUk8G7VKd02+GfOql42TCXoO2JEmSejBol1qt0a6n99s3f4ypieCmn3PGEUmSJJ3JoF2qa0R77tAxbrjqUi6ZmdzAk5IkSdJmZdAutVajvY03F5d59vAbPnZdkiRJfRm0S3VaML0FJiZ55uWf0lle8UZISZIk9WXQLtVeWKvPnqsfVLPnWuuzJUmS1JtBu1SntVafvW/+OP9g1zZ2bp3Z4JOSJEnSZmXQLtVuwcw2lpZXeOql45aNSJIkaSCDdqlOC2a3c+DIAq32kjdCSpIkaSCDdqm6RntfXZ/tiLYkSZIGMWiX6pyE2W3MHTrGVTsu4codl2z0GUmSJGkTM2iX6rTImW3MzR+zbESSJElDGbRLtVu8sTzL6yc7lo1IkiRpKIN2iZUVWDzJ0c40AL9y1aUbfEKSJEna7AzaJTrV49fbE1sA2DI7uZFnI0mSpAuAQbtEHbTfrIP2zKTNJkmSpMFMjCXaq0G7mmlkZspmkyRJ0mAmxhKdBQDejGpEe9oRbUmSJA1hYixRj2j/fVQj2tOTsZFnI0mSpAuAQbtEXaN9KiwdkSRJUhkTY4l6RPsU9Yj2hM0mSZKkwUyMJeoa7VNcwtREMDFh6YgkSZIGM2iXqEe0T/I2y0YkSZJUxNRYotMCgpMrM844IkmSpCKmxhLtFsxso7Pi1H6SJEkqY2os0VmA2W0sLq8w49R+kiRJKmDQLrE6or20Yo22JEmSipgaS3RaayPalo5IkiSphKmxRD2ibdCWJElSKVNjiU4LZrfTWU5LRyRJklTE1FiivVDXaC8z44i2JEmSCpgaS6zVaCfTU846IkmSpOEM2iU6J63RliRJ0khMjcMsL8HSm1WN9tKKpSOSJEkqYmocprNQvc9so7O8wrQ3Q0qSJKmAqXGYdqt6X3sypE0mSZKk4UyNw3TqoD2zjcWlNGhLkiSpiKlxmLUR7e116YizjkiSJGk4g/YwjRrtxSVnHZEkSVKZotQYEe+PiB9ExAsRcV+P7Tsj4pGIeDYi9kXEDY1tOyLiqxFxMCIORMR76/WfjogfRcT++nXb+L7WGDVqtDvWaEuSJKnQ1LAdImIS+Bzwm8BhYC4iHs3M7zd2+xSwPzM/FBG/VO9/S73tfuCvM/NfRsQMsKVx3H/PzP86ji+ybuoa7axnHfER7JIkSSpRkhr3Ai9k5ouZ2QG+Atzetc/1wOMAmXkQuC4idkXE24H3AV+ot3Uy86djO/vzoR7RXp7aSiaWjkiSJKlISWq8Cni5sXy4Xtf0DPBhgIjYC1wLXA38PHAU+OOIeDoi/igitjaOu6cuN3koInb2+uUR8YmIeDIinjx69GjZtxqnukZ7cao6bYO2JEmSSpSkxl7TbGTX8meAnRGxH7gXeBpYoipNuRn4fGbeBJwEVmu8Pw/8AnAjcAT4g16/PDMfzMw9mbnnne98Z8Hpjlm7BRNTdJgGsHREkiRJRYbWaFONYF/TWL4aeKW5Q2aeAD4GEBEBHKpfW4DDmflEvetXqYN2Zr66enxE/CHwl2f3FdZZp1U/FbL6f4uZSaf3kyRJ0nAlw7NzwO6IeFd9M+MdwKPNHeqZRWbqxY8D38zME5n5Y+DliHh3ve0W4Pv1MVc0fsSHgO+dw/dYP+0WzG5ncXkFsHREkiRJZYaOaGfmUkTcA3wdmAQeysznIuLuevsDwHuAL0bEMlWQvqvxI+4FvlQH8RepR76B34+IG6nKUOaB3xnPVxqzzkI1h3YdtC0dkSRJUomS0hEy8zHgsa51DzQ+fwvY3efY/cCeHuv/zUhnulHarWoO7SVHtCVJklTO1DhMpwUzW+lYOiJJkqQRmBqHabfq0pH6Zsgpb4aUJEnScAbtYTqn3ww5Mzm5wSckSZKkC4FBe5h2dTPkWzXajmhLkiRpOIP2IJn1iPa2t2q0nXVEkiRJBUyNgyx3YGWpqtFeWi0dsckkSZI0nKlxkHarep/dvjai7TzakiRJKmFqHKSzUL03Hljj9H6SJEkqYWocZG1EexuLS9X0ft4MKUmSpBIG7UE6ddCe2WbpiCRJkkZiahykWaPtzZCSJEkagalxEGu0JUmSdJZMjYM0a7QtHZEkSdIITI2DNGu069KRqQlvhpQkSdJwBu1BTptHO5mZnCDCoC1JkqThDNqDdBZgchYmp1lcXrFsRJIkScVMjoO0WzC7DYDF5RXn0JYkSVIxg/YgnRbMVEG7s7TijCOSJEkqZnIcpN2C2e0AdJYN2pIkSSpnchyks7A2or24nMxaoy1JkqRCJsdBGjXanaVlR7QlSZJUzOQ4SKNGe3E5mZ7yZkhJkiSVMWgP0jXryIwj2pIkSSpkchyk04KZ+mZIZx2RJEnSCEyO/WRWQXu1RtsH1kiSJGkEJsd+Fk9BrsDM1mrR6f0kSZI0ApNjP+1W9b56M+RSWqMtSZKkYibHfjp10G4+sMbSEUmSJBUyOfbTOX1Eu7oZ0un9JEmSVMag3c9q6Uhjej+fDClJkqRSJsd+1ka0q9IRb4aUJEnSKEyO/bQXqvfZZumIzSVJkqQyJsd+umq0F5fTebQlSZJUzOTYT6NGOzOrWUcc0ZYkSVIhk2M/jRHtxeWsPjrriCRJkgoZtPtpL8D0FpiYZHF5BcARbUmSJBUzOfbTaTXqs6ugbY22JEmSSpkc+2m33ppxxBFtSZIkjcjk2E9jRLuzVI9oG7QlSZJUyOTYT7sFs6sPq6lvhrR0RJIkSYVMjv10Fs6o0bZ0RJIkSaVMjv00a7SXVoO20/tJkiSpjEG7n2aN9uqItqUjkiRJKmRy7KdZo12PaM9aOiJJkqRCJsdeVlZg8WSjRru6GdIRbUmSJJUqSo4R8f6I+EFEvBAR9/XYvjMiHomIZyNiX0Tc0Ni2IyK+GhEHI+JARLy3Xn9ZRHwjIp6v33eO72udo9XHr6/No70MeDOkJEmSyg1NjhExCXwOuBW4HvhIRFzftdungP2Z+avAR4H7G9vuB/46M38J+DXgQL3+PuDxzNwNPF4vbw6rQXttHu16ej+DtiRJkgqVJMe9wAuZ+WJmdoCvALd37XM9VVgmMw8C10XEroh4O/A+4Av1tk5m/rQ+5nbg4frzw8AHz+mbjFN7dUR7dR7t1UewO+uIJEmSypQE7auAlxvLh+t1Tc8AHwaIiL3AtcDVwM8DR4E/joinI+KPImJrfcyuzDwCUL9fftbfYty6RrSdR1uSJEmjmirYp9cwbnYtfwa4PyL2A98FngaWgGngZuDezHwiIu6nKhH5T6UnGBGfAD5RL7Yj4nulx56z/3zraYvXffa8/ebz5R3ATzb6JP4/YVuOl+05Xrbn+NiW42V7jpftOV7vPtcfUBK0DwPXNJavBl5p7pCZJ4CPAUREAIfq1xbgcGY+Ue/6Vd6qxX41Iq7IzCMRcQXwWq9fnpkPAg/WP/vJzNxT8sU0nO05PrbleNme42V7jo9tOV6253jZnuMVEU+e688oqYWYA3ZHxLsiYga4A3i060R21NsAPg58MzNPZOaPgZcjYvX/CG4Bvl9/fhS4s/58J/C1c/gekiRJ0qYydEQ7M5ci4h7g68Ak8FBmPhcRd9fbHwDeA3wxIpapgvRdjR9xL/ClOoi/SD3yTVVu8mcRcRfwd8Bvj+k7SZIkSRuupHSEzHwMeKxr3QONz98Cdvc5dj9wxp8xMvN1qhHuUTw44v4azPYcH9tyvGzP8bI9x8e2HC/bc7xsz/E65/aMzO77GiVJkiSdK+erkyRJktbBpgzaBY98j4j4H/X2ZyPi5o04z80uIq6JiP8bEQci4rmI+Pc99vmNiHgjIvbXr9/biHO9UETEfER8t26rM+5Gtm+Wi4h3N/rd/og4ERGf7NrH/jlARDwUEa81pz2NiMsi4hsR8Xz9vrPPsQOvsxebPm35XyLiYP1v+ZGI2NHn2IHXhYtRn/b8dET8qPHv+bY+x9o3u/Rpzz9ttOV8PcVyr2Ptnw39stG6XTszc1O9qG64/CHVw25mqB6Gc33XPrcBf0U1x/evA09s9HlvxhdwBXBz/Xk78Lc92vI3gL/c6HO9UF7APPCOAdvtm2fXrpPAj4Fru9bbPwe32/uonlXwvca63wfuqz/fB3y2T3sPvM5ebK8+bfkvgKn682d7tWW9beB14WJ89WnPTwP/Ychx9s3C9uza/gfA7/XZZv88vT16ZqP1unZuxhHtkke+3w58MSvfBnbUc3GrITOPZOZT9ecF4ABnPtVT42XfPDu3AD/MzJc2+kQuJJn5TeBY1+rbgYfrzw8DH+xxaMl19qLSqy0z828yc6le/DbVcyRUoE/fLGHf7GFQe0ZEAP8K+PJ5PakL1IBstC7Xzs0YtEse+V6yjxoi4jrgJuCJHpvfGxHPRMRfRcQvn9cTu/Ak8DcR8Z2onlrazb55du6g/38k7J+j2ZWZR6D6DwpweY997Kej+7dUf63qZdh1QW+5py7FeajPn+btm6P7Z8Crmfl8n+32zz66stG6XDs3Y9AueeR7yT6qRcQ24H8Dn8zqKZ5NT1H9uf7XgP8J/MX5Pr8LzD/JzJuBW4F/FxHv69pu3xxRVHPsfwD48x6b7Z/rw346goj4XWAJ+FKfXYZdF1T5PPALwI3AEapyh272zdF9hMGj2fbPHoZko76H9Vg3sH9uxqA99JHvhfsIiIhpqo70pcz8P93bs3qCZ6v+/BgwHRHvOM+necHIzFfq99eAR6j+jNRk3xzdrcBTmflq9wb751l5dbVcqX5/rcc+9tNCEXEn8FvAv866SLNbwXVBQGa+mpnLmbkC/CG928m+OYKImAI+DPxpv33sn2fqk43W5dq5GYP20Ee+18sfrWd4+HXgjdXhfr2lrtv6AnAgM/9bn31+tt6PiNhL1SdeP39neeGIiK0RsX31M9WNUt/r2s2+Obq+ozH2z7PyKHBn/flO4Gs99im5zl70IuL9wH8EPpCZp/rsU3JdEGvhZdWH6N1O9s3R/HPgYGYe7rXR/nmmAdlofa6dG333Z587Qm+jugv0h8Dv1uvuBu6uPwfwuXr7d4E9G33Om/EF/FOqP2k8C+yvX7d1teU9wHNUd85+G/jHG33em/VFdZfxM/XrOfvmWNp0C1VwvrSxzv5Z3n5fpvoT/CLVSMtdwM8AjwPP1++X1fteCTzWOPaM6+zF/OrTli9Q1WOuXj8f6G7LfteFi/3Vpz3/pL4uPksVTq7obs962b5Z0J71+v+1er1s7Gv/HNyW/bLRulw7fTKkJEmStA42Y+mIJEmSdMEzaEuSJEnrwKAtSZIkrQODtiRJkrQODNqSJEnSOjBoS5IkSevAoC1JkiStA4O2JEmStA7+HwmVyolYDOYYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(score_acc,label='train')\n",
    "plt.plot(score_val_accuracy,label='test')\n",
    "plt.axis([0,20,.96,1.])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:12.446765Z",
     "iopub.status.busy": "2021-06-12T19:01:12.440905Z",
     "iopub.status.idle": "2021-06-12T19:01:12.574972Z",
     "shell.execute_reply": "2021-06-12T19:01:12.575399Z",
     "shell.execute_reply.started": "2021-06-12T18:27:55.829269Z"
    },
    "papermill": {
     "duration": 0.318204,
     "end_time": "2021-06-12T19:01:12.575517",
     "exception": false,
     "start_time": "2021-06-12T19:01:12.257313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f769479fed0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(score_loss,label='train')\n",
    "plt.plot(score_val_loss,label='test')\n",
    "plt.axis([0,11,.0,.1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.180097,
     "end_time": "2021-06-12T19:01:12.935550",
     "exception": false,
     "start_time": "2021-06-12T19:01:12.755453",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Some plots to see the performance of my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:13.306436Z",
     "iopub.status.busy": "2021-06-12T19:01:13.305547Z",
     "iopub.status.idle": "2021-06-12T19:01:46.331786Z",
     "shell.execute_reply": "2021-06-12T19:01:46.330812Z",
     "shell.execute_reply.started": "2021-06-12T18:27:56.063166Z"
    },
    "papermill": {
     "duration": 33.216238,
     "end_time": "2021-06-12T19:01:46.331908",
     "exception": false,
     "start_time": "2021-06-12T19:01:13.115670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0080 - val_accuracy: 0.9979\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0072 - accuracy: 0.9975 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
      " 66/512 [==>...........................] - 1s 20ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0042 - val_accuracy: 0.9988\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0046 - accuracy: 0.9983 - val_loss: 0.0044 - val_accuracy: 0.9988\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.0014 - val_accuracy: 0.9996\n",
      " 66/512 [==>...........................] - 1s 17ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0014 - val_accuracy: 0.9996\n",
      " 66/512 [==>...........................] - 1s 17ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0019 - val_accuracy: 0.9993\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0011 - val_accuracy: 0.9996\n",
      " 66/512 [==>...........................] - 1s 18ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0819 - val_accuracy: 0.9820\n",
      " 66/512 [==>...........................] - 1s 18ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0012 - val_accuracy: 0.9996\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0017 - val_accuracy: 0.9992\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0025 - val_accuracy: 0.9989\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.0014 - val_accuracy: 0.9998\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0012 - val_accuracy: 0.9998\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0063 - val_accuracy: 0.9987\n",
      " 66/512 [==>...........................] - 1s 17ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 9.7591e-04 - val_accuracy: 0.9995\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 5.0022e-04 - val_accuracy: 0.9999\n",
      " 66/512 [==>...........................] - 1s 17ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 4.0217e-04 - val_accuracy: 0.9998\n",
      " 66/512 [==>...........................] - 1s 19ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 3.0062e-04 - val_accuracy: 1.0000\n",
      " 66/512 [==>...........................] - 1s 16ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.0017 - val_accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:46.871750Z",
     "iopub.status.busy": "2021-06-12T19:01:46.870826Z",
     "iopub.status.idle": "2021-06-12T19:01:47.845206Z",
     "shell.execute_reply": "2021-06-12T19:01:47.844139Z",
     "shell.execute_reply.started": "2021-06-12T18:28:29.283047Z"
    },
    "papermill": {
     "duration": 1.246402,
     "end_time": "2021-06-12T19:01:47.845328",
     "exception": false,
     "start_time": "2021-06-12T19:01:46.598926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size.csv',index=False)\n",
    "model.save('DATA AUG_size.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.26525,
     "end_time": "2021-06-12T19:01:48.376180",
     "exception": false,
     "start_time": "2021-06-12T19:01:48.110930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here i increase number of splits as data aug do generate more than one image by altering it so i increased the splits,also i could see that it was not reaching upto its true potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:01:48.959427Z",
     "iopub.status.busy": "2021-06-12T19:01:48.958625Z",
     "iopub.status.idle": "2021-06-12T19:02:18.068914Z",
     "shell.execute_reply": "2021-06-12T19:02:18.068444Z",
     "shell.execute_reply.started": "2021-06-12T18:28:30.265537Z"
    },
    "papermill": {
     "duration": 29.426477,
     "end_time": "2021-06-12T19:02:18.069022",
     "exception": false,
     "start_time": "2021-06-12T19:01:48.642545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 584,170\n",
      "Trainable params: 584,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      " 66/512 [==>...........................] - 2s 31ms/step - loss: 0.7982 - accuracy: 0.7700 - val_loss: 0.1179 - val_accuracy: 0.9620\n",
      " 66/512 [==>...........................] - 2s 27ms/step - loss: 0.1148 - accuracy: 0.9640 - val_loss: 0.2677 - val_accuracy: 0.9190\n",
      " 66/512 [==>...........................] - 2s 33ms/step - loss: 0.0757 - accuracy: 0.9764 - val_loss: 0.0406 - val_accuracy: 0.9870\n",
      " 66/512 [==>...........................] - 2s 26ms/step - loss: 0.0420 - accuracy: 0.9870 - val_loss: 0.0807 - val_accuracy: 0.9751\n",
      " 66/512 [==>...........................] - 2s 27ms/step - loss: 0.0337 - accuracy: 0.9892 - val_loss: 0.0312 - val_accuracy: 0.9898\n",
      " 66/512 [==>...........................] - 2s 26ms/step - loss: 0.0265 - accuracy: 0.9917 - val_loss: 0.0176 - val_accuracy: 0.9946\n",
      " 66/512 [==>...........................] - 2s 27ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 0.0138 - val_accuracy: 0.9960\n",
      " 66/512 [==>...........................] - 2s 28ms/step - loss: 0.0159 - accuracy: 0.9952 - val_loss: 0.0098 - val_accuracy: 0.9971\n",
      " 66/512 [==>...........................] - 2s 27ms/step - loss: 0.0133 - accuracy: 0.9963 - val_loss: 0.0230 - val_accuracy: 0.9942\n",
      " 66/512 [==>...........................] - 2s 27ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.0131 - val_accuracy: 0.9955\n"
     ]
    }
   ],
   "source": [
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(10,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:02:18.749116Z",
     "iopub.status.busy": "2021-06-12T19:02:18.748217Z",
     "iopub.status.idle": "2021-06-12T19:02:22.516123Z",
     "shell.execute_reply": "2021-06-12T19:02:22.515265Z",
     "shell.execute_reply.started": "2021-06-12T18:28:59.509474Z"
    },
    "papermill": {
     "duration": 4.109922,
     "end_time": "2021-06-12T19:02:22.516227",
     "exception": false,
     "start_time": "2021-06-12T19:02:18.406305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 3s 3ms/step - loss: 0.0100 - accuracy: 0.9967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010028245858848095, 0.996738076210022]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:02:23.238307Z",
     "iopub.status.busy": "2021-06-12T19:02:23.237405Z",
     "iopub.status.idle": "2021-06-12T19:02:24.559846Z",
     "shell.execute_reply": "2021-06-12T19:02:24.558818Z",
     "shell.execute_reply.started": "2021-06-12T18:29:03.572310Z"
    },
    "papermill": {
     "duration": 1.685836,
     "end_time": "2021-06-12T19:02:24.559961",
     "exception": false,
     "start_time": "2021-06-12T19:02:22.874125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch.csv',index=False)\n",
    "model.save('DATA AUG_size_arch.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.353971,
     "end_time": "2021-06-12T19:02:25.267055",
     "exception": false,
     "start_time": "2021-06-12T19:02:24.913084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that i was in zone and read about VGG 16 please look into it if you have not and i saw the architecture / layers layout they used and i tried to give it a chance . This did certainly help me get past 99.2 accuracy and break my own record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:02:25.992883Z",
     "iopub.status.busy": "2021-06-12T19:02:25.992035Z",
     "iopub.status.idle": "2021-06-12T19:04:09.438125Z",
     "shell.execute_reply": "2021-06-12T19:04:09.437216Z",
     "shell.execute_reply.started": "2021-06-12T18:29:04.704232Z"
    },
    "papermill": {
     "duration": 103.818636,
     "end_time": "2021-06-12T19:04:09.438240",
     "exception": false,
     "start_time": "2021-06-12T19:02:25.619604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 584,170\n",
      "Trainable params: 584,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 10s 145ms/step - loss: 1.7190 - accuracy: 0.4411 - val_loss: 0.6080 - val_accuracy: 0.8360\n",
      " 66/512 [==>...........................] - 9s 136ms/step - loss: 0.4268 - accuracy: 0.8661 - val_loss: 0.2840 - val_accuracy: 0.9113\n",
      " 66/512 [==>...........................] - 9s 137ms/step - loss: 0.1828 - accuracy: 0.9425 - val_loss: 0.1054 - val_accuracy: 0.9656\n",
      " 66/512 [==>...........................] - 10s 147ms/step - loss: 0.1300 - accuracy: 0.9601 - val_loss: 0.1027 - val_accuracy: 0.9694\n",
      " 66/512 [==>...........................] - 9s 138ms/step - loss: 0.0985 - accuracy: 0.9698 - val_loss: 0.0821 - val_accuracy: 0.9724\n",
      " 66/512 [==>...........................] - 9s 139ms/step - loss: 0.0782 - accuracy: 0.9767 - val_loss: 0.0559 - val_accuracy: 0.9814\n",
      " 66/512 [==>...........................] - 9s 131ms/step - loss: 0.0603 - accuracy: 0.9808 - val_loss: 0.0698 - val_accuracy: 0.9780\n",
      " 66/512 [==>...........................] - 9s 139ms/step - loss: 0.0603 - accuracy: 0.9811 - val_loss: 0.0444 - val_accuracy: 0.9858\n",
      " 66/512 [==>...........................] - 9s 138ms/step - loss: 0.0522 - accuracy: 0.9831 - val_loss: 0.0368 - val_accuracy: 0.9881\n",
      " 66/512 [==>...........................] - 9s 132ms/step - loss: 0.0463 - accuracy: 0.9857 - val_loss: 0.0445 - val_accuracy: 0.9863\n"
     ]
    }
   ],
   "source": [
    "datagen = image.ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(10,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:04:10.515101Z",
     "iopub.status.busy": "2021-06-12T19:04:10.514099Z",
     "iopub.status.idle": "2021-06-12T19:04:14.595615Z",
     "shell.execute_reply": "2021-06-12T19:04:14.596038Z",
     "shell.execute_reply.started": "2021-06-12T18:30:47.362032Z"
    },
    "papermill": {
     "duration": 4.621531,
     "end_time": "2021-06-12T19:04:14.596160",
     "exception": false,
     "start_time": "2021-06-12T19:04:09.974629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0385 - accuracy: 0.9876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03851128742098808, 0.9876190423965454]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:04:15.713595Z",
     "iopub.status.busy": "2021-06-12T19:04:15.712718Z",
     "iopub.status.idle": "2021-06-12T19:04:16.849545Z",
     "shell.execute_reply": "2021-06-12T19:04:16.848860Z",
     "shell.execute_reply.started": "2021-06-12T18:30:51.287774Z"
    },
    "papermill": {
     "duration": 1.698337,
     "end_time": "2021-06-12T19:04:16.849654",
     "exception": false,
     "start_time": "2021-06-12T19:04:15.151317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch_custom_gen.csv',index=False)\n",
    "model.save('DATA AUG_size_arch_custom_gen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.555308,
     "end_time": "2021-06-12T19:04:17.959115",
     "exception": false,
     "start_time": "2021-06-12T19:04:17.403807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I thought to add some customizations to my Image generator , in order to give my score a push . I used the dafault example given in keras documentation .\n",
    "https://keras.io/api/preprocessing/image/#imagedatagenerator-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:04:19.099131Z",
     "iopub.status.busy": "2021-06-12T19:04:19.098216Z",
     "iopub.status.idle": "2021-06-12T19:07:41.765055Z",
     "shell.execute_reply": "2021-06-12T19:07:41.764329Z",
     "shell.execute_reply.started": "2021-06-12T18:30:52.539745Z"
    },
    "papermill": {
     "duration": 203.252465,
     "end_time": "2021-06-12T19:07:41.765220",
     "exception": false,
     "start_time": "2021-06-12T19:04:18.512755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 584,170\n",
      "Trainable params: 584,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 9s 138ms/step - loss: 1.6159 - accuracy: 0.4862 - val_loss: 0.7403 - val_accuracy: 0.7568\n",
      " 66/512 [==>...........................] - 9s 139ms/step - loss: 0.4761 - accuracy: 0.8539 - val_loss: 0.7224 - val_accuracy: 0.7811\n",
      " 66/512 [==>...........................] - 10s 144ms/step - loss: 0.2228 - accuracy: 0.9326 - val_loss: 0.1121 - val_accuracy: 0.9643\n",
      " 66/512 [==>...........................] - 9s 141ms/step - loss: 0.1400 - accuracy: 0.9562 - val_loss: 0.1216 - val_accuracy: 0.9656\n",
      " 66/512 [==>...........................] - 9s 133ms/step - loss: 0.1069 - accuracy: 0.9664 - val_loss: 0.1971 - val_accuracy: 0.9414\n",
      " 66/512 [==>...........................] - 9s 143ms/step - loss: 0.0905 - accuracy: 0.9716 - val_loss: 0.0646 - val_accuracy: 0.9807\n",
      " 66/512 [==>...........................] - 9s 140ms/step - loss: 0.0644 - accuracy: 0.9805 - val_loss: 0.0610 - val_accuracy: 0.9801\n",
      " 66/512 [==>...........................] - 9s 133ms/step - loss: 0.0599 - accuracy: 0.9811 - val_loss: 0.0641 - val_accuracy: 0.9796\n",
      " 66/512 [==>...........................] - 9s 134ms/step - loss: 0.0525 - accuracy: 0.9839 - val_loss: 0.0621 - val_accuracy: 0.9818\n",
      " 66/512 [==>...........................] - 10s 145ms/step - loss: 0.0488 - accuracy: 0.9845 - val_loss: 0.0530 - val_accuracy: 0.9846\n",
      " 66/512 [==>...........................] - 9s 133ms/step - loss: 0.0457 - accuracy: 0.9858 - val_loss: 0.0376 - val_accuracy: 0.9900\n",
      " 66/512 [==>...........................] - 9s 134ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.0486 - val_accuracy: 0.9851\n",
      " 66/512 [==>...........................] - 10s 145ms/step - loss: 0.0383 - accuracy: 0.9887 - val_loss: 0.0363 - val_accuracy: 0.9882\n",
      " 66/512 [==>...........................] - 9s 133ms/step - loss: 0.0355 - accuracy: 0.9890 - val_loss: 0.0335 - val_accuracy: 0.9880\n",
      " 66/512 [==>...........................] - 9s 139ms/step - loss: 0.0353 - accuracy: 0.9895 - val_loss: 0.0295 - val_accuracy: 0.9907\n",
      " 66/512 [==>...........................] - 10s 146ms/step - loss: 0.0350 - accuracy: 0.9896 - val_loss: 0.0262 - val_accuracy: 0.9914\n",
      " 66/512 [==>...........................] - 9s 142ms/step - loss: 0.0304 - accuracy: 0.9903 - val_loss: 0.0432 - val_accuracy: 0.9870\n",
      " 66/512 [==>...........................] - 9s 137ms/step - loss: 0.0303 - accuracy: 0.9903 - val_loss: 0.0257 - val_accuracy: 0.9919\n",
      " 66/512 [==>...........................] - 9s 142ms/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0422 - val_accuracy: 0.9879\n",
      " 66/512 [==>...........................] - 9s 138ms/step - loss: 0.0265 - accuracy: 0.9913 - val_loss: 0.0504 - val_accuracy: 0.9837\n"
     ]
    }
   ],
   "source": [
    "datagen = image.ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:07:43.874305Z",
     "iopub.status.busy": "2021-06-12T19:07:43.873438Z",
     "iopub.status.idle": "2021-06-12T19:07:47.730908Z",
     "shell.execute_reply": "2021-06-12T19:07:47.730453Z",
     "shell.execute_reply.started": "2021-06-12T18:34:13.673810Z"
    },
    "papermill": {
     "duration": 5.030012,
     "end_time": "2021-06-12T19:07:47.731002",
     "exception": false,
     "start_time": "2021-06-12T19:07:42.700990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0230 - accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.023048067465424538, 0.9923095107078552]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:07:49.673228Z",
     "iopub.status.busy": "2021-06-12T19:07:49.672316Z",
     "iopub.status.idle": "2021-06-12T19:07:50.848438Z",
     "shell.execute_reply": "2021-06-12T19:07:50.847410Z",
     "shell.execute_reply.started": "2021-06-12T18:34:17.884530Z"
    },
    "papermill": {
     "duration": 2.17911,
     "end_time": "2021-06-12T19:07:50.848553",
     "exception": false,
     "start_time": "2021-06-12T19:07:48.669443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch_custom_gen_20.csv',index=False)\n",
    "model.save('DATA AUG_size_arch_custom_gen_20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.935914,
     "end_time": "2021-06-12T19:07:52.722992",
     "exception": false,
     "start_time": "2021-06-12T19:07:51.787078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I thought it was not running my model enough as it was kind of underfiting so i changed it to 20 splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:07:54.832899Z",
     "iopub.status.busy": "2021-06-12T19:07:54.832049Z",
     "iopub.status.idle": "2021-06-12T19:11:19.522583Z",
     "shell.execute_reply": "2021-06-12T19:11:19.521678Z",
     "shell.execute_reply.started": "2021-06-12T18:34:19.045997Z"
    },
    "papermill": {
     "duration": 205.86065,
     "end_time": "2021-06-12T19:11:19.522696",
     "exception": false,
     "start_time": "2021-06-12T19:07:53.662046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 584,170\n",
      "Trainable params: 584,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 10s 145ms/step - loss: 1.7656 - accuracy: 0.4368 - val_loss: 0.6886 - val_accuracy: 0.7757\n",
      " 66/512 [==>...........................] - 9s 133ms/step - loss: 0.5138 - accuracy: 0.8384 - val_loss: 0.2046 - val_accuracy: 0.9346\n",
      " 66/512 [==>...........................] - 10s 145ms/step - loss: 0.2323 - accuracy: 0.9281 - val_loss: 0.1184 - val_accuracy: 0.9640\n",
      " 66/512 [==>...........................] - 9s 137ms/step - loss: 0.1662 - accuracy: 0.9493 - val_loss: 0.0958 - val_accuracy: 0.9708\n",
      " 66/512 [==>...........................] - 9s 138ms/step - loss: 0.1133 - accuracy: 0.9653 - val_loss: 0.0786 - val_accuracy: 0.9761\n",
      " 66/512 [==>...........................] - 9s 142ms/step - loss: 0.0968 - accuracy: 0.9714 - val_loss: 0.0699 - val_accuracy: 0.9793\n",
      " 66/512 [==>...........................] - 9s 132ms/step - loss: 0.0793 - accuracy: 0.9749 - val_loss: 0.0664 - val_accuracy: 0.9793\n",
      " 66/512 [==>...........................] - 9s 143ms/step - loss: 0.0666 - accuracy: 0.9796 - val_loss: 0.0443 - val_accuracy: 0.9869\n",
      " 66/512 [==>...........................] - 9s 134ms/step - loss: 0.0633 - accuracy: 0.9807 - val_loss: 0.0525 - val_accuracy: 0.9848\n",
      " 66/512 [==>...........................] - 9s 140ms/step - loss: 0.0556 - accuracy: 0.9829 - val_loss: 0.0376 - val_accuracy: 0.9880\n",
      " 66/512 [==>...........................] - 10s 146ms/step - loss: 0.0490 - accuracy: 0.9854 - val_loss: 0.0399 - val_accuracy: 0.9873\n",
      " 66/512 [==>...........................] - 9s 134ms/step - loss: 0.0504 - accuracy: 0.9848 - val_loss: 0.0419 - val_accuracy: 0.9874\n",
      " 66/512 [==>...........................] - 10s 146ms/step - loss: 0.0471 - accuracy: 0.9863 - val_loss: 0.0303 - val_accuracy: 0.9905\n",
      " 66/512 [==>...........................] - 10s 147ms/step - loss: 0.0418 - accuracy: 0.9871 - val_loss: 0.0341 - val_accuracy: 0.9895\n",
      " 66/512 [==>...........................] - 9s 137ms/step - loss: 0.0402 - accuracy: 0.9882 - val_loss: 0.0395 - val_accuracy: 0.9880\n",
      " 66/512 [==>...........................] - 10s 148ms/step - loss: 0.0395 - accuracy: 0.9886 - val_loss: 0.0264 - val_accuracy: 0.9921\n",
      " 66/512 [==>...........................] - 9s 139ms/step - loss: 0.0381 - accuracy: 0.9887 - val_loss: 0.0563 - val_accuracy: 0.9820\n",
      " 66/512 [==>...........................] - 9s 130ms/step - loss: 0.0363 - accuracy: 0.9891 - val_loss: 0.0295 - val_accuracy: 0.9914\n",
      " 66/512 [==>...........................] - 10s 148ms/step - loss: 0.0330 - accuracy: 0.9896 - val_loss: 0.0323 - val_accuracy: 0.9912\n",
      " 66/512 [==>...........................] - 9s 138ms/step - loss: 0.0326 - accuracy: 0.9902 - val_loss: 0.0369 - val_accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "datagen = image.ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2)\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    \n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:11:22.649742Z",
     "iopub.status.busy": "2021-06-12T19:11:22.648705Z",
     "iopub.status.idle": "2021-06-12T19:11:26.614549Z",
     "shell.execute_reply": "2021-06-12T19:11:26.613650Z",
     "shell.execute_reply.started": "2021-06-12T18:37:37.220442Z"
    },
    "papermill": {
     "duration": 5.789942,
     "end_time": "2021-06-12T19:11:26.614656",
     "exception": false,
     "start_time": "2021-06-12T19:11:20.824714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0205 - accuracy: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.020469848066568375, 0.9942857027053833]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:11:29.264240Z",
     "iopub.status.busy": "2021-06-12T19:11:29.263325Z",
     "iopub.status.idle": "2021-06-12T19:11:30.363936Z",
     "shell.execute_reply": "2021-06-12T19:11:30.362903Z",
     "shell.execute_reply.started": "2021-06-12T18:37:41.193240Z"
    },
    "papermill": {
     "duration": 2.43551,
     "end_time": "2021-06-12T19:11:30.364094",
     "exception": false,
     "start_time": "2021-06-12T19:11:27.928584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT.csv',index=False)\n",
    "model.save('DATA AUG_size_arch_custom_gen_20_DROPOUT.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.356129,
     "end_time": "2021-06-12T19:11:33.038407",
     "exception": false,
     "start_time": "2021-06-12T19:11:31.682278",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now i my model was giving 99.99 something accuracy but it was not performing as good in the scoreboard so i decided to add a dropout layer with probability .\n",
    "For those who heard this term the first time,it is a simple way to prevent overfitting in your model as it randomly selects some nodes and shuts them off , which makes the other node to learn on their own .Hence in a way no node resides on top as each learns the same amount .This link can help with a more better explaintation.\n",
    "https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:11:35.913392Z",
     "iopub.status.busy": "2021-06-12T19:11:35.912522Z",
     "iopub.status.idle": "2021-06-12T19:15:36.529006Z",
     "shell.execute_reply": "2021-06-12T19:15:36.528442Z",
     "shell.execute_reply.started": "2021-06-12T18:37:42.294178Z"
    },
    "papermill": {
     "duration": 241.958192,
     "end_time": "2021-06-12T19:15:36.529104",
     "exception": false,
     "start_time": "2021-06-12T19:11:34.570912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               295168    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 584,170\n",
      "Trainable params: 584,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 1.6461 - accuracy: 0.4606 - val_loss: 0.9197 - val_accuracy: 0.7418\n",
      " 66/512 [==>...........................] - 11s 171ms/step - loss: 0.5017 - accuracy: 0.8429 - val_loss: 0.2784 - val_accuracy: 0.9106\n",
      " 66/512 [==>...........................] - 11s 173ms/step - loss: 0.2383 - accuracy: 0.9268 - val_loss: 0.1277 - val_accuracy: 0.9625\n",
      " 66/512 [==>...........................] - 11s 168ms/step - loss: 0.1471 - accuracy: 0.9550 - val_loss: 0.0945 - val_accuracy: 0.9719\n",
      " 66/512 [==>...........................] - 10s 159ms/step - loss: 0.1210 - accuracy: 0.9624 - val_loss: 0.0922 - val_accuracy: 0.9713\n",
      " 66/512 [==>...........................] - 11s 173ms/step - loss: 0.0963 - accuracy: 0.9699 - val_loss: 0.0618 - val_accuracy: 0.9805\n",
      " 66/512 [==>...........................] - 11s 167ms/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 0.1162 - val_accuracy: 0.9643\n",
      " 66/512 [==>...........................] - 12s 179ms/step - loss: 0.0699 - accuracy: 0.9789 - val_loss: 0.0524 - val_accuracy: 0.9849\n",
      " 66/512 [==>...........................] - 10s 159ms/step - loss: 0.0630 - accuracy: 0.9805 - val_loss: 0.0837 - val_accuracy: 0.9706\n",
      " 66/512 [==>...........................] - 11s 167ms/step - loss: 0.0593 - accuracy: 0.9820 - val_loss: 0.0449 - val_accuracy: 0.9857\n",
      " 66/512 [==>...........................] - 12s 178ms/step - loss: 0.0540 - accuracy: 0.9837 - val_loss: 0.0359 - val_accuracy: 0.9898\n",
      " 66/512 [==>...........................] - 11s 163ms/step - loss: 0.0484 - accuracy: 0.9857 - val_loss: 0.0394 - val_accuracy: 0.9865\n",
      " 66/512 [==>...........................] - 10s 159ms/step - loss: 0.0477 - accuracy: 0.9857 - val_loss: 0.0326 - val_accuracy: 0.9910\n",
      " 66/512 [==>...........................] - 12s 177ms/step - loss: 0.0431 - accuracy: 0.9872 - val_loss: 0.0326 - val_accuracy: 0.9904\n",
      " 66/512 [==>...........................] - 11s 165ms/step - loss: 0.0409 - accuracy: 0.9874 - val_loss: 0.0295 - val_accuracy: 0.9902\n",
      " 66/512 [==>...........................] - 11s 163ms/step - loss: 0.0389 - accuracy: 0.9877 - val_loss: 0.0355 - val_accuracy: 0.9901\n",
      " 66/512 [==>...........................] - 12s 175ms/step - loss: 0.0364 - accuracy: 0.9890 - val_loss: 0.0500 - val_accuracy: 0.9862\n",
      " 66/512 [==>...........................] - 11s 165ms/step - loss: 0.0373 - accuracy: 0.9888 - val_loss: 0.0272 - val_accuracy: 0.9908\n",
      " 66/512 [==>...........................] - 10s 156ms/step - loss: 0.0372 - accuracy: 0.9888 - val_loss: 0.0277 - val_accuracy: 0.9924\n",
      " 66/512 [==>...........................] - 11s 169ms/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.0273 - val_accuracy: 0.9911\n"
     ]
    }
   ],
   "source": [
    "datagen = image.ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,zoom_range=0.08,shear_range=0.3)\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    \n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.25))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(20,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:15:40.350069Z",
     "iopub.status.busy": "2021-06-12T19:15:40.349057Z",
     "iopub.status.idle": "2021-06-12T19:15:44.153217Z",
     "shell.execute_reply": "2021-06-12T19:15:44.152783Z",
     "shell.execute_reply.started": "2021-06-12T18:41:33.106010Z"
    },
    "papermill": {
     "duration": 5.494389,
     "end_time": "2021-06-12T19:15:44.153309",
     "exception": false,
     "start_time": "2021-06-12T19:15:38.658920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 3s 3ms/step - loss: 0.0152 - accuracy: 0.9954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015238294377923012, 0.9954285621643066]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:15:47.869201Z",
     "iopub.status.busy": "2021-06-12T19:15:47.868271Z",
     "iopub.status.idle": "2021-06-12T19:15:49.084422Z",
     "shell.execute_reply": "2021-06-12T19:15:49.083330Z",
     "shell.execute_reply.started": "2021-06-12T18:41:36.958861Z"
    },
    "papermill": {
     "duration": 3.195739,
     "end_time": "2021-06-12T19:15:49.084535",
     "exception": false,
     "start_time": "2021-06-12T19:15:45.888796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen.csv',index=False)\n",
    "model.save('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.725307,
     "end_time": "2021-06-12T19:15:52.528228",
     "exception": false,
     "start_time": "2021-06-12T19:15:50.802921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here i added some touch to the image generator like zoom,shear transfomation ,etc .which made it past 99.3 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:15:55.990199Z",
     "iopub.status.busy": "2021-06-12T19:15:55.989322Z",
     "iopub.status.idle": "2021-06-12T19:22:05.460090Z",
     "shell.execute_reply": "2021-06-12T19:22:05.459126Z",
     "shell.execute_reply.started": "2021-06-12T18:41:38.151225Z"
    },
    "papermill": {
     "duration": 371.198171,
     "end_time": "2021-06-12T19:22:05.460223",
     "exception": false,
     "start_time": "2021-06-12T19:15:54.262052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 881,898\n",
      "Trainable params: 881,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 11s 166ms/step - loss: 1.6894 - accuracy: 0.4561 - val_loss: 0.5628 - val_accuracy: 0.8292\n",
      " 66/512 [==>...........................] - 11s 159ms/step - loss: 0.4431 - accuracy: 0.8648 - val_loss: 0.2710 - val_accuracy: 0.9133\n",
      " 66/512 [==>...........................] - 12s 175ms/step - loss: 0.1986 - accuracy: 0.9378 - val_loss: 0.1052 - val_accuracy: 0.9660\n",
      " 66/512 [==>...........................] - 12s 176ms/step - loss: 0.1361 - accuracy: 0.9570 - val_loss: 0.1074 - val_accuracy: 0.9687\n",
      " 66/512 [==>...........................] - 10s 156ms/step - loss: 0.1037 - accuracy: 0.9680 - val_loss: 0.0823 - val_accuracy: 0.9752\n",
      " 66/512 [==>...........................] - 11s 173ms/step - loss: 0.0847 - accuracy: 0.9740 - val_loss: 0.0588 - val_accuracy: 0.9806\n",
      " 66/512 [==>...........................] - 11s 168ms/step - loss: 0.0686 - accuracy: 0.9790 - val_loss: 0.0615 - val_accuracy: 0.9811\n",
      " 66/512 [==>...........................] - 12s 184ms/step - loss: 0.0596 - accuracy: 0.9826 - val_loss: 0.0689 - val_accuracy: 0.9792\n",
      " 66/512 [==>...........................] - 11s 170ms/step - loss: 0.0545 - accuracy: 0.9829 - val_loss: 0.0450 - val_accuracy: 0.9863\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.0495 - accuracy: 0.9854 - val_loss: 0.0413 - val_accuracy: 0.9880\n",
      " 66/512 [==>...........................] - 12s 186ms/step - loss: 0.0465 - accuracy: 0.9863 - val_loss: 0.0433 - val_accuracy: 0.9875\n",
      " 66/512 [==>...........................] - 12s 180ms/step - loss: 0.0431 - accuracy: 0.9865 - val_loss: 0.0333 - val_accuracy: 0.9895\n",
      " 66/512 [==>...........................] - 11s 165ms/step - loss: 0.0401 - accuracy: 0.9879 - val_loss: 0.0539 - val_accuracy: 0.9824\n",
      " 66/512 [==>...........................] - 13s 201ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0328 - val_accuracy: 0.9899\n",
      " 66/512 [==>...........................] - 11s 173ms/step - loss: 0.0376 - accuracy: 0.9880 - val_loss: 0.0362 - val_accuracy: 0.9904\n",
      " 66/512 [==>...........................] - 11s 171ms/step - loss: 0.0360 - accuracy: 0.9893 - val_loss: 0.0399 - val_accuracy: 0.9895\n",
      " 66/512 [==>...........................] - 12s 185ms/step - loss: 0.0319 - accuracy: 0.9901 - val_loss: 0.0334 - val_accuracy: 0.9905\n",
      " 66/512 [==>...........................] - 11s 172ms/step - loss: 0.0332 - accuracy: 0.9901 - val_loss: 0.0294 - val_accuracy: 0.9908\n",
      " 66/512 [==>...........................] - 13s 190ms/step - loss: 0.0315 - accuracy: 0.9904 - val_loss: 0.0245 - val_accuracy: 0.9940\n",
      " 66/512 [==>...........................] - 10s 158ms/step - loss: 0.0286 - accuracy: 0.9910 - val_loss: 0.0366 - val_accuracy: 0.9893\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.0316 - accuracy: 0.9912 - val_loss: 0.0228 - val_accuracy: 0.9929\n",
      " 66/512 [==>...........................] - 13s 190ms/step - loss: 0.0279 - accuracy: 0.9911 - val_loss: 0.0210 - val_accuracy: 0.9935\n",
      " 66/512 [==>...........................] - 10s 158ms/step - loss: 0.0270 - accuracy: 0.9920 - val_loss: 0.0226 - val_accuracy: 0.9920\n",
      " 66/512 [==>...........................] - 11s 162ms/step - loss: 0.0251 - accuracy: 0.9924 - val_loss: 0.0273 - val_accuracy: 0.9937\n",
      " 66/512 [==>...........................] - 13s 199ms/step - loss: 0.0256 - accuracy: 0.9925 - val_loss: 0.0185 - val_accuracy: 0.9940\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.0255 - accuracy: 0.9923 - val_loss: 0.0236 - val_accuracy: 0.9923\n",
      " 66/512 [==>...........................] - 10s 157ms/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 0.0216 - val_accuracy: 0.9940\n",
      " 66/512 [==>...........................] - 12s 183ms/step - loss: 0.0255 - accuracy: 0.9922 - val_loss: 0.0287 - val_accuracy: 0.9911\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.0184 - val_accuracy: 0.9937\n",
      " 66/512 [==>...........................] - 11s 164ms/step - loss: 0.0217 - accuracy: 0.9938 - val_loss: 0.0272 - val_accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "datagen = image.ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,zoom_range=0.08,shear_range=0.3)\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    \n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(30,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:22:10.016513Z",
     "iopub.status.busy": "2021-06-12T19:22:10.015612Z",
     "iopub.status.idle": "2021-06-12T19:22:13.945395Z",
     "shell.execute_reply": "2021-06-12T19:22:13.944601Z",
     "shell.execute_reply.started": "2021-06-12T18:47:22.237896Z"
    },
    "papermill": {
     "duration": 6.223402,
     "end_time": "2021-06-12T19:22:13.945501",
     "exception": false,
     "start_time": "2021-06-12T19:22:07.722099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0160 - accuracy: 0.9949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.016017397865653038, 0.9949285984039307]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:22:18.530432Z",
     "iopub.status.busy": "2021-06-12T19:22:18.529865Z",
     "iopub.status.idle": "2021-06-12T19:22:19.581789Z",
     "shell.execute_reply": "2021-06-12T19:22:19.580810Z",
     "shell.execute_reply.started": "2021-06-12T18:47:26.023173Z"
    },
    "papermill": {
     "duration": 3.362962,
     "end_time": "2021-06-12T19:22:19.581898",
     "exception": false,
     "start_time": "2021-06-12T19:22:16.218936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense-30.csv',index=False)\n",
    "model.save('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense-30.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.548388,
     "end_time": "2021-06-12T19:22:24.410066",
     "exception": false,
     "start_time": "2021-06-12T19:22:21.861678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here i did two things :-\n",
    "1. I increased splits to 30 .\n",
    "2. I increased nodes in Dense layer to 512 .\n",
    "\n",
    "My model was neither underfitting/overfitting but for some reason i had a feeling that i can do better .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:22:29.058002Z",
     "iopub.status.busy": "2021-06-12T19:22:29.057084Z",
     "iopub.status.idle": "2021-06-12T19:28:39.080958Z",
     "shell.execute_reply": "2021-06-12T19:28:39.081613Z",
     "shell.execute_reply.started": "2021-06-12T18:47:27.144864Z"
    },
    "papermill": {
     "duration": 372.361888,
     "end_time": "2021-06-12T19:28:39.081815",
     "exception": false,
     "start_time": "2021-06-12T19:22:26.719927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784)\n",
      "(42000, 28, 28, 1)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               590336    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 881,898\n",
      "Trainable params: 881,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/opt/conda/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66/512 [==>...........................] - 11s 164ms/step - loss: 1.8326 - accuracy: 0.3953 - val_loss: 0.9606 - val_accuracy: 0.7095\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.6166 - accuracy: 0.8071 - val_loss: 0.3416 - val_accuracy: 0.8952\n",
      " 66/512 [==>...........................] - 13s 190ms/step - loss: 0.2922 - accuracy: 0.9085 - val_loss: 0.2292 - val_accuracy: 0.9277\n",
      " 66/512 [==>...........................] - 11s 159ms/step - loss: 0.2048 - accuracy: 0.9362 - val_loss: 0.1253 - val_accuracy: 0.9615\n",
      " 66/512 [==>...........................] - 11s 170ms/step - loss: 0.1332 - accuracy: 0.9573 - val_loss: 0.2347 - val_accuracy: 0.9365\n",
      " 66/512 [==>...........................] - 13s 190ms/step - loss: 0.1092 - accuracy: 0.9669 - val_loss: 0.0756 - val_accuracy: 0.9768\n",
      " 66/512 [==>...........................] - 11s 164ms/step - loss: 0.0886 - accuracy: 0.9725 - val_loss: 0.0968 - val_accuracy: 0.9679\n",
      " 66/512 [==>...........................] - 11s 166ms/step - loss: 0.0827 - accuracy: 0.9745 - val_loss: 0.0610 - val_accuracy: 0.9805\n",
      " 66/512 [==>...........................] - 12s 186ms/step - loss: 0.0737 - accuracy: 0.9779 - val_loss: 0.0633 - val_accuracy: 0.9806\n",
      " 66/512 [==>...........................] - 11s 162ms/step - loss: 0.0658 - accuracy: 0.9796 - val_loss: 0.0568 - val_accuracy: 0.9823\n",
      " 66/512 [==>...........................] - 11s 167ms/step - loss: 0.0588 - accuracy: 0.9819 - val_loss: 0.0583 - val_accuracy: 0.9817\n",
      " 66/512 [==>...........................] - 12s 182ms/step - loss: 0.0579 - accuracy: 0.9813 - val_loss: 0.0521 - val_accuracy: 0.9840\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.0530 - accuracy: 0.9843 - val_loss: 0.0432 - val_accuracy: 0.9861\n",
      " 66/512 [==>...........................] - 11s 167ms/step - loss: 0.0526 - accuracy: 0.9839 - val_loss: 0.0414 - val_accuracy: 0.9875\n",
      " 66/512 [==>...........................] - 12s 180ms/step - loss: 0.0477 - accuracy: 0.9864 - val_loss: 0.0480 - val_accuracy: 0.9856\n",
      " 66/512 [==>...........................] - 11s 168ms/step - loss: 0.0468 - accuracy: 0.9851 - val_loss: 0.0513 - val_accuracy: 0.9863\n",
      " 66/512 [==>...........................] - 10s 159ms/step - loss: 0.0430 - accuracy: 0.9874 - val_loss: 0.0324 - val_accuracy: 0.9906\n",
      " 66/512 [==>...........................] - 12s 183ms/step - loss: 0.0440 - accuracy: 0.9869 - val_loss: 0.0376 - val_accuracy: 0.9877\n",
      " 66/512 [==>...........................] - 11s 169ms/step - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.0465 - val_accuracy: 0.9867\n",
      " 66/512 [==>...........................] - 11s 161ms/step - loss: 0.0403 - accuracy: 0.9887 - val_loss: 0.0706 - val_accuracy: 0.9810\n",
      " 66/512 [==>...........................] - 13s 190ms/step - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.0409 - val_accuracy: 0.9876\n",
      " 66/512 [==>...........................] - 11s 162ms/step - loss: 0.0395 - accuracy: 0.9882 - val_loss: 0.0289 - val_accuracy: 0.9902\n",
      " 66/512 [==>...........................] - 11s 160ms/step - loss: 0.0381 - accuracy: 0.9887 - val_loss: 0.0343 - val_accuracy: 0.9882\n",
      " 66/512 [==>...........................] - 13s 192ms/step - loss: 0.0352 - accuracy: 0.9893 - val_loss: 0.0351 - val_accuracy: 0.9908\n",
      " 66/512 [==>...........................] - 10s 159ms/step - loss: 0.0341 - accuracy: 0.9900 - val_loss: 0.0237 - val_accuracy: 0.9918\n",
      " 66/512 [==>...........................] - 11s 162ms/step - loss: 0.0349 - accuracy: 0.9894 - val_loss: 0.0275 - val_accuracy: 0.9908\n",
      " 66/512 [==>...........................] - 13s 199ms/step - loss: 0.0336 - accuracy: 0.9901 - val_loss: 0.0365 - val_accuracy: 0.9894\n",
      " 66/512 [==>...........................] - 11s 160ms/step - loss: 0.0329 - accuracy: 0.9896 - val_loss: 0.0303 - val_accuracy: 0.9904\n",
      " 66/512 [==>...........................] - 10s 159ms/step - loss: 0.0313 - accuracy: 0.9902 - val_loss: 0.0219 - val_accuracy: 0.9940\n",
      " 66/512 [==>...........................] - 12s 185ms/step - loss: 0.0324 - accuracy: 0.9902 - val_loss: 0.0317 - val_accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "datagen = image.ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.25,\n",
    "    height_shift_range=0.25,zoom_range=0.08,shear_range=0.3)\n",
    "def make_model():\n",
    "    keras.backend.clear_session()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',input_shape=(28,28,1),padding='same'))\n",
    "    model.add(Conv2D(32,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ##\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(64,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    ###\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(Conv2D(128,(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer = 'RMSprop',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "trainX, trainy,testX = load_data()\n",
    "trainX,testX = normalize(trainX,testX)\n",
    "model = make_model()\n",
    "model.summary()\n",
    "score_acc = []\n",
    "score_loss = []\n",
    "score_val_loss = []\n",
    "score_val_accuracy = []\n",
    "sss = StratifiedShuffleSplit(30,test_size=.2,random_state=115)\n",
    "for index, (train_indices, val_indices) in enumerate(sss.split(trainX,trainy)):\n",
    "    aa = trainX[train_indices]\n",
    "    ab = trainy[train_indices]\n",
    "    cc = trainX[val_indices]\n",
    "    cd = trainy[val_indices]\n",
    "    training_batch = datagen.flow(aa, ab, batch_size=512)\n",
    "    val_batches = datagen.flow(cc, cd, batch_size=128)\n",
    "    his = model.fit_generator(generator=training_batch, steps_per_epoch=512, epochs=1, \n",
    "                    validation_data=val_batches, validation_steps=128)\n",
    "    score_acc.append(his.history['accuracy'])\n",
    "    score_loss.append(his.history['loss'])\n",
    "    score_val_loss.append(his.history['val_loss'])\n",
    "    score_val_accuracy.append(his.history['val_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:28:48.391325Z",
     "iopub.status.busy": "2021-06-12T19:28:48.390423Z",
     "iopub.status.idle": "2021-06-12T19:28:52.370411Z",
     "shell.execute_reply": "2021-06-12T19:28:52.369942Z",
     "shell.execute_reply.started": "2021-06-12T18:53:11.447576Z"
    },
    "papermill": {
     "duration": 8.012876,
     "end_time": "2021-06-12T19:28:52.370512",
     "exception": false,
     "start_time": "2021-06-12T19:28:44.357636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313/1313 [==============================] - 4s 3ms/step - loss: 0.0150 - accuracy: 0.9957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015044058673083782, 0.9957380890846252]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:28:58.203914Z",
     "iopub.status.busy": "2021-06-12T19:28:58.203074Z",
     "iopub.status.idle": "2021-06-12T19:28:59.765137Z",
     "shell.execute_reply": "2021-06-12T19:28:59.764672Z",
     "shell.execute_reply.started": "2021-06-12T18:53:15.564035Z"
    },
    "papermill": {
     "duration": 4.506603,
     "end_time": "2021-06-12T19:28:59.765241",
     "exception": false,
     "start_time": "2021-06-12T19:28:55.258638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(model.predict(testX),axis=1)\n",
    "pred = pd.DataFrame(pred,columns=['Label'])\n",
    "xyz = pd.DataFrame(np.arange(1,testX.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "pred = pd.concat([xyz,pred],axis=1)\n",
    "pred.to_csv('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense_gen_30.csv',index=False)\n",
    "model.save('DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense_gen_30.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.839031,
     "end_time": "2021-06-12T19:29:05.479613",
     "exception": false,
     "start_time": "2021-06-12T19:29:02.640582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this i just tweaked the geneareator a bit and not much as i was getting good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.112994,
     "end_time": "2021-06-12T19:29:11.418054",
     "exception": false,
     "start_time": "2021-06-12T19:29:08.305060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now i tried to see where my models were giving different answers , and some images were even difficult for me to understand .\n",
    "LOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:17.182763Z",
     "iopub.status.busy": "2021-06-12T19:29:17.180789Z",
     "iopub.status.idle": "2021-06-12T19:29:17.183386Z",
     "shell.execute_reply": "2021-06-12T19:29:17.183788Z",
     "shell.execute_reply.started": "2021-06-12T18:58:20.640640Z"
    },
    "papermill": {
     "duration": 2.902981,
     "end_time": "2021-06-12T19:29:17.183897",
     "exception": false,
     "start_time": "2021-06-12T19:29:14.280916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ../input/digit-recogniser-my-answers/DATA AUG.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:23.186078Z",
     "iopub.status.busy": "2021-06-12T19:29:23.184203Z",
     "iopub.status.idle": "2021-06-12T19:29:23.190187Z",
     "shell.execute_reply": "2021-06-12T19:29:23.189542Z",
     "shell.execute_reply.started": "2021-06-12T18:58:21.043635Z"
    },
    "papermill": {
     "duration": 3.110981,
     "end_time": "2021-06-12T19:29:23.190289",
     "exception": false,
     "start_time": "2021-06-12T19:29:20.079308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/DATA AUG.csv\n",
      "/kaggle/working/DATA AUG_size_arch.csv\n",
      "/kaggle/working/DATA AUG.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen.h5\n",
      "/kaggle/working/__notebook__.ipynb\n",
      "/kaggle/working/CONV 2D.csv\n",
      "/kaggle/working/DATA AUG_size.csv\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20.csv\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT.csv\n",
      "/kaggle/working/DATA AUG_size_arch.h5\n",
      "/kaggle/working/DATA AUG_size.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense-30.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense-30.csv\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen.csv\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT_gen.csv\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT_gen.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense_gen_30.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT.h5\n",
      "/kaggle/working/DATA AUG_size_arch_custom_gen_20_DROPOUT_gen_dense_gen_30.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "files = []\n",
    "for dirname, _, filenames in os.walk('/kaggle/working'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        if 'csv' in filename:\n",
    "            files.append(os.path.join(dirname, filename))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:28.908630Z",
     "iopub.status.busy": "2021-06-12T19:29:28.907864Z",
     "iopub.status.idle": "2021-06-12T19:29:28.958831Z",
     "shell.execute_reply": "2021-06-12T19:29:28.959344Z",
     "shell.execute_reply.started": "2021-06-12T18:58:21.461771Z"
    },
    "papermill": {
     "duration": 2.928258,
     "end_time": "2021-06-12T19:29:28.959503",
     "exception": false,
     "start_time": "2021-06-12T19:29:26.031245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n",
      "[2 0 9 ... 3 9 2]\n"
     ]
    }
   ],
   "source": [
    "label_data = []\n",
    "for i in files:\n",
    "    aa = pd.read_csv(i,index_col=False).values[:,1]\n",
    "    print(aa)\n",
    "    label_data.append(aa)\n",
    "label_data = np.array(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:35.345509Z",
     "iopub.status.busy": "2021-06-12T19:29:35.344927Z",
     "iopub.status.idle": "2021-06-12T19:29:35.364323Z",
     "shell.execute_reply": "2021-06-12T19:29:35.364758Z",
     "shell.execute_reply.started": "2021-06-12T18:58:21.964801Z"
    },
    "papermill": {
     "duration": 2.90047,
     "end_time": "2021-06-12T19:29:35.364871",
     "exception": false,
     "start_time": "2021-06-12T19:29:32.464401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>27990</th>\n",
       "      <th>27991</th>\n",
       "      <th>27992</th>\n",
       "      <th>27993</th>\n",
       "      <th>27994</th>\n",
       "      <th>27995</th>\n",
       "      <th>27996</th>\n",
       "      <th>27997</th>\n",
       "      <th>27998</th>\n",
       "      <th>27999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  28000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3      4      5      6      7      8      9      ...  \\\n",
       "0      2      0      9      9      3      7      0      3      0      3  ...   \n",
       "1      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "2      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "3      2      0      9      9      3      9      0      3      0      3  ...   \n",
       "4      2      0      9      9      3      7      0      3      0      3  ...   \n",
       "5      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "6      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "7      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "8      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "9      2      0      9      0      3      7      0      3      0      3  ...   \n",
       "\n",
       "   27990  27991  27992  27993  27994  27995  27996  27997  27998  27999  \n",
       "0      7      6      1      9      7      9      7      3      9      2  \n",
       "1      7      6      1      9      7      9      7      3      9      2  \n",
       "2      7      6      1      9      7      9      7      3      9      2  \n",
       "3      7      6      1      9      7      9      7      3      9      2  \n",
       "4      7      6      1      9      7      9      7      3      9      2  \n",
       "5      7      6      1      9      7      9      7      3      9      2  \n",
       "6      7      6      7      9      7      9      7      3      9      2  \n",
       "7      7      6      1      9      7      9      7      3      9      2  \n",
       "8      7      6      1      9      7      9      7      3      9      2  \n",
       "9      7      6      1      9      7      9      7      3      9      2  \n",
       "\n",
       "[10 rows x 28000 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:41.076752Z",
     "iopub.status.busy": "2021-06-12T19:29:41.066768Z",
     "iopub.status.idle": "2021-06-12T19:29:41.118527Z",
     "shell.execute_reply": "2021-06-12T19:29:41.118078Z",
     "shell.execute_reply.started": "2021-06-12T18:59:21.300359Z"
    },
    "papermill": {
     "duration": 2.923988,
     "end_time": "2021-06-12T19:29:41.118623",
     "exception": false,
     "start_time": "2021-06-12T19:29:38.194635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "err = []\n",
    "for i,a in enumerate(zip(label_data[0,:],label_data[1,:],label_data[2,:],label_data[3,:],label_data[4,:],label_data[5,:],label_data[6,:],label_data[7,:])):\n",
    "    s = a[0]\n",
    "    for b in a:\n",
    "        if b!=s:\n",
    "            err.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:47.096349Z",
     "iopub.status.busy": "2021-06-12T19:29:47.095479Z",
     "iopub.status.idle": "2021-06-12T19:29:47.225099Z",
     "shell.execute_reply": "2021-06-12T19:29:47.225582Z",
     "shell.execute_reply.started": "2021-06-12T18:58:23.249394Z"
    },
    "papermill": {
     "duration": 3.018251,
     "end_time": "2021-06-12T19:29:47.225729",
     "exception": false,
     "start_time": "2021-06-12T19:29:44.207478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75f0515550>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(testX[165,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:53.013713Z",
     "iopub.status.busy": "2021-06-12T19:29:53.013117Z",
     "iopub.status.idle": "2021-06-12T19:29:53.018293Z",
     "shell.execute_reply": "2021-06-12T19:29:53.017896Z",
     "shell.execute_reply.started": "2021-06-12T18:58:23.720331Z"
    },
    "papermill": {
     "duration": 2.846656,
     "end_time": "2021-06-12T19:29:53.018398",
     "exception": false,
     "start_time": "2021-06-12T19:29:50.171742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:29:58.968631Z",
     "iopub.status.busy": "2021-06-12T19:29:58.966575Z",
     "iopub.status.idle": "2021-06-12T19:30:01.157194Z",
     "shell.execute_reply": "2021-06-12T19:30:01.156504Z",
     "shell.execute_reply.started": "2021-06-12T18:58:24.241137Z"
    },
    "papermill": {
     "duration": 5.050155,
     "end_time": "2021-06-12T19:30:01.157346",
     "exception": false,
     "start_time": "2021-06-12T19:29:56.107191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 9 0 0 9 9 0 0 0 \n",
      "3 9 0 0 9 9 0 0 0 \n",
      "3 9 0 0 9 9 0 0 0 \n",
      "3 9 0 0 9 9 0 0 0 \n",
      "3 9 0 0 9 9 0 0 0 \n",
      "5 7 7 7 9 7 7 7 7 \n",
      "39 6 6 6 6 5 6 6 5 \n",
      "39 6 6 6 6 5 6 6 5 \n",
      "47 2 2 2 2 3 2 2 2 \n",
      "58 2 2 2 2 4 2 2 2 \n",
      "59 0 0 0 0 9 0 0 0 \n",
      "83 0 0 0 0 0 0 8 8 \n",
      "83 0 0 0 0 0 0 8 8 \n",
      "128 8 7 8 8 4 7 7 7 \n",
      "128 8 7 8 8 4 7 7 7 \n",
      "128 8 7 8 8 4 7 7 7 \n",
      "128 8 7 8 8 4 7 7 7 \n",
      "128 8 7 8 8 4 7 7 7 \n",
      "138 3 3 3 3 3 3 3 5 \n",
      "165 3 7 2 2 3 2 3 3 \n",
      "165 3 7 2 2 3 2 3 3 \n",
      "165 3 7 2 2 3 2 3 3 \n",
      "165 3 7 2 2 3 2 3 3 \n",
      "217 6 6 6 6 6 6 6 5 \n",
      "241 6 6 6 6 6 6 6 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "275 3 5 5 5 5 5 5 5 \n",
      "346 6 6 6 6 5 6 6 5 \n",
      "346 6 6 6 6 5 6 6 5 \n",
      "383 4 9 4 4 4 4 4 9 \n",
      "383 4 9 4 4 4 4 4 9 \n",
      "434 0 0 6 6 6 6 6 6 \n",
      "434 0 0 6 6 6 6 6 6 \n",
      "434 0 0 6 6 6 6 6 6 \n",
      "434 0 0 6 6 6 6 6 6 \n",
      "434 0 0 6 6 6 6 6 6 \n",
      "434 0 0 6 6 6 6 6 6 \n",
      "450 9 5 9 5 9 9 9 9 \n",
      "450 9 5 9 5 9 9 9 9 \n",
      "460 6 6 6 6 6 1 6 6 \n",
      "509 2 0 2 2 2 2 2 2 \n",
      "511 5 6 5 5 5 6 5 5 \n",
      "511 5 6 5 5 5 6 5 5 \n",
      "538 7 7 7 4 4 4 7 4 \n",
      "538 7 7 7 4 4 4 7 4 \n",
      "538 7 7 7 4 4 4 7 4 \n",
      "538 7 7 7 4 4 4 7 4 \n",
      "554 0 0 0 0 3 0 5 5 \n",
      "554 0 0 0 0 3 0 5 5 \n",
      "554 0 0 0 0 3 0 5 5 \n",
      "604 6 6 6 6 6 6 6 5 \n",
      "627 3 3 3 3 3 3 3 5 \n",
      "640 0 0 0 0 3 0 2 8 \n",
      "640 0 0 0 0 3 0 2 8 \n",
      "640 0 0 0 0 3 0 2 8 \n",
      "645 7 7 7 2 2 2 2 2 \n",
      "645 7 7 7 2 2 2 2 2 \n",
      "645 7 7 7 2 2 2 2 2 \n",
      "645 7 7 7 2 2 2 2 2 \n",
      "645 7 7 7 2 2 2 2 2 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "647 4 8 8 8 8 8 8 8 \n",
      "676 7 7 7 7 7 7 7 3 \n",
      "756 7 7 7 7 7 7 7 9 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "760 7 3 3 3 3 3 3 3 \n",
      "789 8 8 8 8 8 8 8 5 \n",
      "844 8 1 8 8 4 2 2 2 \n",
      "844 8 1 8 8 4 2 2 2 \n",
      "844 8 1 8 8 4 2 2 2 \n",
      "844 8 1 8 8 4 2 2 2 \n",
      "844 8 1 8 8 4 2 2 2 \n",
      "897 4 4 4 4 4 4 7 4 \n",
      "904 9 9 4 4 4 4 4 9 \n",
      "904 9 9 4 4 4 4 4 9 \n",
      "904 9 9 4 4 4 4 4 9 \n",
      "904 9 9 4 4 4 4 4 9 \n",
      "904 9 9 4 4 4 4 4 9 \n",
      "912 4 4 4 9 4 4 4 4 \n",
      "917 6 6 6 6 5 6 6 5 \n",
      "917 6 6 6 6 5 6 6 5 \n",
      "919 5 5 5 5 8 0 5 5 \n",
      "919 5 5 5 5 8 0 5 5 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "941 0 2 3 2 2 2 2 2 \n",
      "1118 4 7 4 4 4 4 4 4 \n",
      "1126 9 8 9 8 8 8 8 8 \n",
      "1126 9 8 9 8 8 8 8 8 \n",
      "1126 9 8 9 8 8 8 8 8 \n",
      "1126 9 8 9 8 8 8 8 8 \n",
      "1126 9 8 9 8 8 8 8 8 \n",
      "1126 9 8 9 8 8 8 8 8 \n",
      "1150 0 8 6 6 6 0 6 8 \n",
      "1150 0 8 6 6 6 0 6 8 \n",
      "1150 0 8 6 6 6 0 6 8 \n",
      "1150 0 8 6 6 6 0 6 8 \n",
      "1150 0 8 6 6 6 0 6 8 \n",
      "1150 0 8 6 6 6 0 6 8 \n",
      "1156 4 4 4 4 4 4 4 9 \n",
      "1235 1 1 1 6 5 6 6 5 \n",
      "1235 1 1 1 6 5 6 6 5 \n",
      "1235 1 1 1 6 5 6 6 5 \n",
      "1235 1 1 1 6 5 6 6 5 \n",
      "1235 1 1 1 6 5 6 6 5 \n",
      "1261 9 9 9 9 9 9 7 9 \n",
      "1307 8 8 8 8 4 4 4 8 \n",
      "1307 8 8 8 8 4 4 4 8 \n",
      "1307 8 8 8 8 4 4 4 8 \n",
      "1331 0 0 0 6 6 0 6 0 \n",
      "1331 0 0 0 6 6 0 6 0 \n",
      "1331 0 0 0 6 6 0 6 0 \n",
      "1389 7 7 7 2 7 2 2 2 \n",
      "1389 7 7 7 2 7 2 2 2 \n",
      "1389 7 7 7 2 7 2 2 2 \n",
      "1389 7 7 7 2 7 2 2 2 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1402 9 8 8 8 8 8 8 8 \n",
      "1406 9 7 9 9 7 7 7 9 \n",
      "1406 9 7 9 9 7 7 7 9 \n",
      "1406 9 7 9 9 7 7 7 9 \n",
      "1406 9 7 9 9 7 7 7 9 \n",
      "1414 1 1 1 1 7 7 7 7 \n",
      "1414 1 1 1 1 7 7 7 7 \n",
      "1414 1 1 1 1 7 7 7 7 \n",
      "1414 1 1 1 1 7 7 7 7 \n",
      "1443 7 7 7 7 7 7 7 9 \n",
      "1468 3 3 7 3 3 3 3 3 \n",
      "1494 5 8 5 5 5 8 5 5 \n",
      "1494 5 8 5 5 5 8 5 5 \n",
      "1496 8 1 8 8 8 8 8 8 \n",
      "1503 2 8 2 2 2 0 2 2 \n",
      "1503 2 8 2 2 2 0 2 2 \n",
      "1537 9 1 9 9 4 9 9 9 \n",
      "1537 9 1 9 9 4 9 9 9 \n",
      "1596 1 1 1 1 1 1 6 1 \n",
      "1662 6 6 6 6 5 6 6 6 \n",
      "1686 9 9 9 9 4 4 4 9 \n",
      "1686 9 9 9 9 4 4 4 9 \n",
      "1686 9 9 9 9 4 4 4 9 \n",
      "1770 3 3 3 5 3 3 3 5 \n",
      "1770 3 3 3 5 3 3 3 5 \n",
      "1797 6 6 6 6 5 6 6 6 \n",
      "1821 6 6 6 6 6 8 2 6 \n",
      "1821 6 6 6 6 6 8 2 6 \n",
      "1861 2 1 2 2 1 7 7 1 \n",
      "1861 2 1 2 2 1 7 7 1 \n",
      "1861 2 1 2 2 1 7 7 1 \n",
      "1861 2 1 2 2 1 7 7 1 \n",
      "1861 2 1 2 2 1 7 7 1 \n",
      "1882 9 9 9 9 9 5 9 5 \n",
      "1882 9 9 9 9 9 5 9 5 \n",
      "1888 4 9 4 4 4 4 4 4 \n",
      "1900 0 6 6 6 0 0 0 0 \n",
      "1900 0 6 6 6 0 0 0 0 \n",
      "1900 0 6 6 6 0 0 0 0 \n",
      "1920 0 0 0 0 0 0 2 0 \n",
      "1935 1 1 1 1 1 1 7 1 \n",
      "1958 8 5 8 8 5 5 5 5 \n",
      "1958 8 5 8 8 5 5 5 5 \n",
      "1958 8 5 8 8 5 5 5 5 \n",
      "1958 8 5 8 8 5 5 5 5 \n",
      "1958 8 5 8 8 5 5 5 5 \n",
      "1959 5 5 5 5 4 5 5 5 \n",
      "2009 1 1 1 1 1 2 1 1 \n",
      "2087 2 2 2 2 3 2 2 3 \n",
      "2087 2 2 2 2 3 2 2 3 \n",
      "2093 3 3 5 5 3 3 3 3 \n",
      "2093 3 3 5 5 3 3 3 3 \n",
      "2117 6 6 6 6 6 6 6 5 \n",
      "2122 6 5 6 6 5 6 6 5 \n",
      "2122 6 5 6 6 5 6 6 5 \n",
      "2122 6 5 6 6 5 6 6 5 \n",
      "2125 2 6 2 2 4 2 2 2 \n",
      "2125 2 6 2 2 4 2 2 2 \n",
      "2145 8 0 0 8 0 0 0 8 \n",
      "2145 8 0 0 8 0 0 0 8 \n",
      "2145 8 0 0 8 0 0 0 8 \n",
      "2145 8 0 0 8 0 0 0 8 \n",
      "2145 8 0 0 8 0 0 0 8 \n",
      "2149 4 0 4 4 4 4 2 4 \n",
      "2149 4 0 4 4 4 4 2 4 \n",
      "2151 0 0 0 0 0 0 2 0 \n",
      "2208 5 6 6 6 6 6 6 5 \n",
      "2208 5 6 6 6 6 6 6 5 \n",
      "2208 5 6 6 6 6 6 6 5 \n",
      "2208 5 6 6 6 6 6 6 5 \n",
      "2208 5 6 6 6 6 6 6 5 \n",
      "2208 5 6 6 6 6 6 6 5 \n",
      "2218 7 7 7 7 7 7 7 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2251 0 4 4 4 4 4 4 9 \n",
      "2275 7 7 7 3 7 7 7 7 \n",
      "2277 5 8 8 8 5 8 8 8 \n",
      "2277 5 8 8 8 5 8 8 8 \n",
      "2277 5 8 8 8 5 8 8 8 \n",
      "2277 5 8 8 8 5 8 8 8 \n",
      "2277 5 8 8 8 5 8 8 8 \n",
      "2277 5 8 8 8 5 8 8 8 \n",
      "2330 8 8 8 8 8 8 8 5 \n",
      "2341 6 6 6 6 6 6 6 5 \n",
      "2383 6 6 6 6 5 6 6 6 \n",
      "2399 9 9 9 9 4 9 9 9 \n",
      "2409 9 9 9 2 7 7 7 9 \n",
      "2409 9 9 9 2 7 7 7 9 \n",
      "2409 9 9 9 2 7 7 7 9 \n",
      "2409 9 9 9 2 7 7 7 9 \n",
      "2416 9 8 9 8 8 8 8 8 \n",
      "2416 9 8 9 8 8 8 8 8 \n",
      "2416 9 8 9 8 8 8 8 8 \n",
      "2416 9 8 9 8 8 8 8 8 \n",
      "2416 9 8 9 8 8 8 8 8 \n",
      "2416 9 8 9 8 8 8 8 8 \n",
      "2440 4 1 4 4 4 4 4 4 \n",
      "2465 7 1 7 7 7 7 7 7 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2499 9 4 4 4 4 4 4 4 \n",
      "2518 8 0 9 8 8 8 8 5 \n",
      "2518 8 0 9 8 8 8 8 5 \n",
      "2518 8 0 9 8 8 8 8 5 \n",
      "2527 9 9 9 9 9 9 3 9 \n",
      "2553 8 7 7 7 7 7 7 8 \n",
      "2553 8 7 7 7 7 7 7 8 \n",
      "2553 8 7 7 7 7 7 7 8 \n",
      "2553 8 7 7 7 7 7 7 8 \n",
      "2553 8 7 7 7 7 7 7 8 \n",
      "2553 8 7 7 7 7 7 7 8 \n",
      "2579 1 1 1 1 1 7 7 1 \n",
      "2579 1 1 1 1 1 7 7 1 \n",
      "2584 7 7 7 2 2 7 2 2 \n",
      "2584 7 7 7 2 2 7 2 2 \n",
      "2584 7 7 7 2 2 7 2 2 \n",
      "2584 7 7 7 2 2 7 2 2 \n",
      "2608 6 6 6 6 5 6 6 5 \n",
      "2608 6 6 6 6 5 6 6 5 \n",
      "2611 8 6 8 8 8 8 8 5 \n",
      "2611 8 6 8 8 8 8 8 5 \n",
      "2613 6 6 6 6 6 6 6 5 \n",
      "2617 1 1 6 6 1 6 6 6 \n",
      "2617 1 1 6 6 1 6 6 6 \n",
      "2617 1 1 6 6 1 6 6 6 \n",
      "2617 1 1 6 6 1 6 6 6 \n",
      "2617 1 1 6 6 1 6 6 6 \n",
      "2637 3 3 3 2 3 2 3 3 \n",
      "2637 3 3 3 2 3 2 3 3 \n",
      "2646 3 8 8 3 3 3 3 3 \n",
      "2646 3 8 8 3 3 3 3 3 \n",
      "2650 1 1 1 1 1 1 7 1 \n",
      "2728 1 1 1 1 1 1 1 5 \n",
      "2731 8 8 8 8 4 8 8 8 \n",
      "2751 3 3 3 3 3 3 3 5 \n",
      "2835 4 4 4 4 4 4 4 9 \n",
      "2928 3 3 2 3 3 3 3 3 \n",
      "2929 5 5 5 5 8 5 5 5 \n",
      "2952 7 7 7 2 7 7 7 7 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "2959 8 6 6 6 6 6 6 6 \n",
      "3015 4 8 4 4 8 8 8 8 \n",
      "3015 4 8 4 4 8 8 8 8 \n",
      "3015 4 8 4 4 8 8 8 8 \n",
      "3015 4 8 4 4 8 8 8 8 \n",
      "3015 4 8 4 4 8 8 8 8 \n",
      "3035 6 6 6 6 5 6 6 6 \n",
      "3080 9 9 9 9 9 9 2 9 \n",
      "3081 5 6 6 6 5 6 6 5 \n",
      "3081 5 6 6 6 5 6 6 5 \n",
      "3081 5 6 6 6 5 6 6 5 \n",
      "3081 5 6 6 6 5 6 6 5 \n",
      "3081 5 6 6 6 5 6 6 5 \n",
      "3097 9 9 2 9 9 9 2 9 \n",
      "3097 9 9 2 9 9 9 2 9 \n",
      "3127 0 0 0 6 6 0 6 6 \n",
      "3127 0 0 0 6 6 0 6 6 \n",
      "3127 0 0 0 6 6 0 6 6 \n",
      "3127 0 0 0 6 6 0 6 6 \n",
      "3154 9 9 9 9 9 9 8 9 \n",
      "3162 3 9 3 3 3 3 3 3 \n",
      "3174 0 0 0 8 8 0 8 8 \n",
      "3174 0 0 0 8 8 0 8 8 \n",
      "3174 0 0 0 8 8 0 8 8 \n",
      "3174 0 0 0 8 8 0 8 8 \n",
      "3177 2 2 2 2 4 2 2 2 \n",
      "3191 7 7 7 5 7 7 7 7 \n",
      "3217 5 9 5 5 9 9 9 5 \n",
      "3217 5 9 5 5 9 9 9 5 \n",
      "3217 5 9 5 5 9 9 9 5 \n",
      "3217 5 9 5 5 9 9 9 5 \n",
      "3254 7 7 7 8 8 8 8 8 \n",
      "3254 7 7 7 8 8 8 8 8 \n",
      "3254 7 7 7 8 8 8 8 8 \n",
      "3254 7 7 7 8 8 8 8 8 \n",
      "3254 7 7 7 8 8 8 8 8 \n",
      "3255 1 1 1 7 1 1 1 1 \n",
      "3256 5 6 6 6 5 6 6 5 \n",
      "3256 5 6 6 6 5 6 6 5 \n",
      "3256 5 6 6 6 5 6 6 5 \n",
      "3256 5 6 6 6 5 6 6 5 \n",
      "3256 5 6 6 6 5 6 6 5 \n",
      "3277 4 4 4 4 4 4 4 9 \n",
      "3290 4 6 4 4 4 4 4 4 \n",
      "3295 2 2 2 2 2 2 1 2 \n",
      "3358 1 1 1 1 1 1 7 1 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3361 4 6 5 5 5 6 5 5 \n",
      "3373 7 7 7 2 7 7 7 7 \n",
      "3389 9 9 9 9 9 9 3 9 \n",
      "3399 4 7 7 4 7 7 7 7 \n",
      "3399 4 7 7 4 7 7 7 7 \n",
      "3399 4 7 7 4 7 7 7 7 \n",
      "3399 4 7 7 4 7 7 7 7 \n",
      "3399 4 7 7 4 7 7 7 7 \n",
      "3399 4 7 7 4 7 7 7 7 \n",
      "3419 0 0 0 0 9 0 0 0 \n",
      "3424 7 7 7 7 7 7 7 4 \n",
      "3438 9 9 9 9 9 9 3 9 \n",
      "3480 9 9 9 9 4 9 9 9 \n",
      "3485 5 5 5 5 8 8 8 5 \n",
      "3485 5 5 5 5 8 8 8 5 \n",
      "3485 5 5 5 5 8 8 8 5 \n",
      "3486 4 1 4 4 4 4 4 4 \n",
      "3490 5 8 5 5 5 5 5 5 \n",
      "3520 0 1 0 0 1 0 1 1 \n",
      "3520 0 1 0 0 1 0 1 1 \n",
      "3520 0 1 0 0 1 0 1 1 \n",
      "3520 0 1 0 0 1 0 1 1 \n",
      "3526 8 8 2 8 2 2 2 2 \n",
      "3526 8 8 2 8 2 2 2 2 \n",
      "3526 8 8 2 8 2 2 2 2 \n",
      "3526 8 8 2 8 2 2 2 2 \n",
      "3526 8 8 2 8 2 2 2 2 \n",
      "3527 3 8 8 8 8 8 3 8 \n",
      "3527 3 8 8 8 8 8 3 8 \n",
      "3527 3 8 8 8 8 8 3 8 \n",
      "3527 3 8 8 8 8 8 3 8 \n",
      "3527 3 8 8 8 8 8 3 8 \n",
      "3527 3 8 8 8 8 8 3 8 \n",
      "3529 5 5 5 5 5 5 9 5 \n",
      "3531 7 7 7 7 7 7 7 5 \n",
      "3629 6 6 4 6 6 6 6 5 \n",
      "3629 6 6 4 6 6 6 6 5 \n",
      "3670 7 7 7 7 7 7 3 3 \n",
      "3670 7 7 7 7 7 7 3 3 \n",
      "3676 9 9 9 9 9 7 7 9 \n",
      "3676 9 9 9 9 9 7 7 9 \n",
      "3700 5 5 5 5 5 6 5 5 \n",
      "3712 9 9 0 9 9 9 9 9 \n",
      "3720 0 0 0 0 0 0 2 0 \n",
      "3733 3 5 5 5 5 3 3 5 \n",
      "3733 3 5 5 5 5 3 3 5 \n",
      "3733 3 5 5 5 5 3 3 5 \n",
      "3733 3 5 5 5 5 3 3 5 \n",
      "3733 3 5 5 5 5 3 3 5 \n",
      "3764 6 6 6 6 5 6 6 5 \n",
      "3764 6 6 6 6 5 6 6 5 \n",
      "3808 9 8 9 9 8 8 8 8 \n",
      "3808 9 8 9 9 8 8 8 8 \n",
      "3808 9 8 9 9 8 8 8 8 \n",
      "3808 9 8 9 9 8 8 8 8 \n",
      "3808 9 8 9 9 8 8 8 8 \n",
      "3816 1 1 2 2 1 2 1 1 \n",
      "3816 1 1 2 2 1 2 1 1 \n",
      "3816 1 1 2 2 1 2 1 1 \n",
      "3844 9 9 9 9 9 7 7 9 \n",
      "3844 9 9 9 9 9 7 7 9 \n",
      "3860 7 7 2 2 2 2 2 2 \n",
      "3860 7 7 2 2 2 2 2 2 \n",
      "3860 7 7 2 2 2 2 2 2 \n",
      "3860 7 7 2 2 2 2 2 2 \n",
      "3860 7 7 2 2 2 2 2 2 \n",
      "3860 7 7 2 2 2 2 2 2 \n",
      "3865 9 8 0 9 9 0 9 9 \n",
      "3865 9 8 0 9 9 0 9 9 \n",
      "3865 9 8 0 9 9 0 9 9 \n",
      "3870 7 7 7 7 3 7 3 3 \n",
      "3870 7 7 7 7 3 7 3 3 \n",
      "3870 7 7 7 7 3 7 3 3 \n",
      "3897 6 6 6 6 6 6 6 5 \n",
      "4078 6 6 6 6 6 6 6 5 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4080 9 2 2 2 2 2 2 2 \n",
      "4107 2 2 2 2 2 2 2 5 \n",
      "4156 9 9 9 9 9 9 4 9 \n",
      "4158 7 7 7 9 7 7 7 7 \n",
      "4231 8 8 8 8 8 8 8 5 \n",
      "4239 1 1 1 1 1 1 7 1 \n",
      "4250 2 2 2 2 4 2 2 2 \n",
      "4260 7 7 7 7 7 7 7 2 \n",
      "4275 9 9 0 0 0 0 8 8 \n",
      "4275 9 9 0 0 0 0 8 8 \n",
      "4275 9 9 0 0 0 0 8 8 \n",
      "4275 9 9 0 0 0 0 8 8 \n",
      "4275 9 9 0 0 0 0 8 8 \n",
      "4275 9 9 0 0 0 0 8 8 \n",
      "4277 8 7 8 8 8 8 2 8 \n",
      "4277 8 7 8 8 8 8 2 8 \n",
      "4287 7 7 7 7 7 7 7 9 \n",
      "4302 7 7 7 9 7 7 7 7 \n",
      "4303 3 3 3 3 3 7 3 3 \n",
      "4304 6 6 6 6 5 6 6 6 \n",
      "4341 0 6 0 6 0 0 8 6 \n",
      "4341 0 6 0 6 0 0 8 6 \n",
      "4341 0 6 0 6 0 0 8 6 \n",
      "4341 0 6 0 6 0 0 8 6 \n",
      "4374 9 9 9 9 9 9 3 9 \n",
      "4427 9 9 9 9 9 9 3 9 \n",
      "4450 1 1 1 1 1 7 7 1 \n",
      "4450 1 1 1 1 1 7 7 1 \n",
      "4468 6 6 6 6 6 6 6 5 \n",
      "4490 6 6 6 6 6 6 6 5 \n",
      "4497 6 6 6 6 5 6 6 6 \n",
      "4509 8 8 6 8 8 8 8 8 \n",
      "4522 6 6 6 6 6 6 6 5 \n",
      "4535 0 0 0 0 0 0 8 0 \n",
      "4583 0 6 6 6 0 0 6 0 \n",
      "4583 0 6 6 6 0 0 6 0 \n",
      "4583 0 6 6 6 0 0 6 0 \n",
      "4583 0 6 6 6 0 0 6 0 \n",
      "4609 7 7 7 2 7 7 7 7 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4617 4 6 6 6 6 6 6 6 \n",
      "4625 0 0 0 0 0 0 0 8 \n",
      "4680 1 1 1 1 1 1 4 4 \n",
      "4680 1 1 1 1 1 1 4 4 \n",
      "4723 8 1 1 8 8 8 8 8 \n",
      "4723 8 1 1 8 8 8 8 8 \n",
      "4748 1 1 1 1 1 1 7 7 \n",
      "4748 1 1 1 1 1 1 7 7 \n",
      "4765 1 1 1 1 0 0 8 0 \n",
      "4765 1 1 1 1 0 0 8 0 \n",
      "4765 1 1 1 1 0 0 8 0 \n",
      "4765 1 1 1 1 0 0 8 0 \n",
      "4774 9 8 9 8 8 8 8 8 \n",
      "4774 9 8 9 8 8 8 8 8 \n",
      "4774 9 8 9 8 8 8 8 8 \n",
      "4774 9 8 9 8 8 8 8 8 \n",
      "4774 9 8 9 8 8 8 8 8 \n",
      "4774 9 8 9 8 8 8 8 8 \n",
      "4795 3 3 3 8 3 3 3 3 \n",
      "4797 7 7 7 7 7 7 7 9 \n",
      "4802 7 1 7 7 7 7 7 7 \n",
      "4806 4 6 4 4 4 4 4 4 \n",
      "4852 2 8 2 2 2 2 2 2 \n",
      "4927 9 9 9 9 1 7 7 9 \n",
      "4927 9 9 9 9 1 7 7 9 \n",
      "4927 9 9 9 9 1 7 7 9 \n",
      "4949 1 1 2 1 1 1 1 1 \n",
      "4994 8 6 8 8 6 6 6 6 \n",
      "4994 8 6 8 8 6 6 6 6 \n",
      "4994 8 6 8 8 6 6 6 6 \n",
      "4994 8 6 8 8 6 6 6 6 \n",
      "4994 8 6 8 8 6 6 6 6 \n",
      "5008 2 2 2 2 7 2 2 2 \n",
      "5038 2 3 3 2 3 3 3 3 \n",
      "5038 2 3 3 2 3 3 3 3 \n",
      "5038 2 3 3 2 3 3 3 3 \n",
      "5038 2 3 3 2 3 3 3 3 \n",
      "5038 2 3 3 2 3 3 3 3 \n",
      "5038 2 3 3 2 3 3 3 3 \n",
      "5065 9 9 9 9 9 9 9 5 \n",
      "5101 8 8 8 8 8 8 8 5 \n",
      "5102 6 6 6 6 5 6 6 6 \n",
      "5130 9 9 9 9 9 7 9 9 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5155 1 8 5 8 8 8 8 8 \n",
      "5157 5 5 5 5 5 5 3 5 \n",
      "5214 5 6 6 6 5 0 5 5 \n",
      "5214 5 6 6 6 5 0 5 5 \n",
      "5214 5 6 6 6 5 0 5 5 \n",
      "5214 5 6 6 6 5 0 5 5 \n",
      "5237 2 1 1 2 1 2 1 2 \n",
      "5237 2 1 1 2 1 2 1 2 \n",
      "5237 2 1 1 2 1 2 1 2 \n",
      "5237 2 1 1 2 1 2 1 2 \n",
      "5276 9 9 9 9 7 7 7 7 \n",
      "5276 9 9 9 9 7 7 7 7 \n",
      "5276 9 9 9 9 7 7 7 7 \n",
      "5276 9 9 9 9 7 7 7 7 \n",
      "5306 8 8 8 8 8 0 8 8 \n",
      "5330 3 3 3 5 3 3 3 3 \n",
      "5349 7 7 7 7 4 7 7 7 \n",
      "5350 0 0 6 6 6 6 6 6 \n",
      "5350 0 0 6 6 6 6 6 6 \n",
      "5350 0 0 6 6 6 6 6 6 \n",
      "5350 0 0 6 6 6 6 6 6 \n",
      "5350 0 0 6 6 6 6 6 6 \n",
      "5350 0 0 6 6 6 6 6 6 \n",
      "5383 3 3 5 3 3 3 3 3 \n",
      "5405 7 7 4 4 4 7 7 7 \n",
      "5405 7 7 4 4 4 7 7 7 \n",
      "5405 7 7 4 4 4 7 7 7 \n",
      "5443 6 6 6 6 4 6 6 6 \n",
      "5455 6 6 6 6 5 6 5 5 \n",
      "5455 6 6 6 6 5 6 5 5 \n",
      "5455 6 6 6 6 5 6 5 5 \n",
      "5456 7 7 7 9 7 7 7 7 \n",
      "5461 4 4 4 9 4 4 4 4 \n",
      "5465 2 2 2 2 7 2 2 2 \n",
      "5533 6 6 6 6 6 6 6 5 \n",
      "5554 4 4 4 9 4 9 4 9 \n",
      "5554 4 4 4 9 4 9 4 9 \n",
      "5554 4 4 4 9 4 9 4 9 \n",
      "5557 0 0 5 5 5 0 5 5 \n",
      "5557 0 0 5 5 5 0 5 5 \n",
      "5557 0 0 5 5 5 0 5 5 \n",
      "5557 0 0 5 5 5 0 5 5 \n",
      "5557 0 0 5 5 5 0 5 5 \n",
      "5561 4 9 4 4 4 4 4 4 \n",
      "5571 0 0 4 5 4 4 4 4 \n",
      "5571 0 0 4 5 4 4 4 4 \n",
      "5571 0 0 4 5 4 4 4 4 \n",
      "5571 0 0 4 5 4 4 4 4 \n",
      "5571 0 0 4 5 4 4 4 4 \n",
      "5571 0 0 4 5 4 4 4 4 \n",
      "5594 6 6 6 6 6 0 6 6 \n",
      "5611 5 5 5 5 3 5 5 5 \n",
      "5662 1 1 1 1 1 1 4 7 \n",
      "5662 1 1 1 1 1 1 4 7 \n",
      "5684 3 3 5 3 3 3 3 3 \n",
      "5700 0 0 6 6 5 0 6 5 \n",
      "5700 0 0 6 6 5 0 6 5 \n",
      "5700 0 0 6 6 5 0 6 5 \n",
      "5700 0 0 6 6 5 0 6 5 \n",
      "5700 0 0 6 6 5 0 6 5 \n",
      "5747 6 6 5 6 5 6 5 5 \n",
      "5747 6 6 5 6 5 6 5 5 \n",
      "5747 6 6 5 6 5 6 5 5 \n",
      "5747 6 6 5 6 5 6 5 5 \n",
      "5763 0 0 0 0 0 0 8 0 \n",
      "5775 9 9 9 9 9 9 3 9 \n",
      "5784 4 4 4 7 9 7 7 9 \n",
      "5784 4 4 4 7 9 7 7 9 \n",
      "5784 4 4 4 7 9 7 7 9 \n",
      "5784 4 4 4 7 9 7 7 9 \n",
      "5784 4 4 4 7 9 7 7 9 \n",
      "5822 7 7 7 7 7 7 7 9 \n",
      "5915 1 1 1 1 1 1 7 1 \n",
      "5939 0 0 0 0 0 0 0 8 \n",
      "5941 6 6 6 6 6 6 6 5 \n",
      "5966 0 0 9 0 4 2 0 2 \n",
      "5966 0 0 9 0 4 2 0 2 \n",
      "5966 0 0 9 0 4 2 0 2 \n",
      "5966 0 0 9 0 4 2 0 2 \n",
      "6000 1 1 0 0 0 0 0 0 \n",
      "6000 1 1 0 0 0 0 0 0 \n",
      "6000 1 1 0 0 0 0 0 0 \n",
      "6000 1 1 0 0 0 0 0 0 \n",
      "6000 1 1 0 0 0 0 0 0 \n",
      "6000 1 1 0 0 0 0 0 0 \n",
      "6070 2 3 2 2 3 3 3 3 \n",
      "6070 2 3 2 2 3 3 3 3 \n",
      "6070 2 3 2 2 3 3 3 3 \n",
      "6070 2 3 2 2 3 3 3 3 \n",
      "6070 2 3 2 2 3 3 3 3 \n",
      "6074 8 8 8 8 8 8 8 5 \n",
      "6110 4 4 4 4 4 4 4 9 \n",
      "6114 3 2 2 2 3 2 2 3 \n",
      "6114 3 2 2 2 3 2 2 3 \n",
      "6114 3 2 2 2 3 2 2 3 \n",
      "6114 3 2 2 2 3 2 2 3 \n",
      "6114 3 2 2 2 3 2 2 3 \n",
      "6120 7 7 7 7 4 7 7 7 \n",
      "6179 0 0 0 0 0 0 2 0 \n",
      "6180 4 4 4 4 4 4 4 9 \n",
      "6212 4 4 4 4 4 4 9 4 \n",
      "6239 6 6 6 6 6 0 6 6 \n",
      "6256 4 6 4 4 4 4 4 4 \n",
      "6259 6 6 6 6 6 0 6 6 \n",
      "6327 1 1 1 1 4 1 1 1 \n",
      "6328 2 2 4 2 2 2 2 2 \n",
      "6379 7 7 7 4 7 7 7 7 \n",
      "6400 4 9 4 4 4 4 4 4 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6401 8 2 2 2 2 2 2 2 \n",
      "6416 6 6 6 6 5 6 6 5 \n",
      "6416 6 6 6 6 5 6 6 5 \n",
      "6487 6 6 6 6 6 6 6 5 \n",
      "6527 9 9 9 8 8 8 8 8 \n",
      "6527 9 9 9 8 8 8 8 8 \n",
      "6527 9 9 9 8 8 8 8 8 \n",
      "6527 9 9 9 8 8 8 8 8 \n",
      "6527 9 9 9 8 8 8 8 8 \n",
      "6533 7 7 7 7 4 7 7 7 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6598 0 6 6 6 6 6 6 6 \n",
      "6602 5 5 5 5 5 5 3 5 \n",
      "6659 4 4 4 8 4 4 4 4 \n",
      "6666 2 2 2 2 4 2 2 2 \n",
      "6673 3 3 7 3 3 3 3 3 \n",
      "6682 8 8 8 8 0 0 8 8 \n",
      "6682 8 8 8 8 0 0 8 8 \n",
      "6691 4 1 4 4 4 4 4 4 \n",
      "6713 8 6 5 8 8 8 8 8 \n",
      "6713 8 6 5 8 8 8 8 8 \n",
      "6716 3 3 5 3 3 3 3 5 \n",
      "6716 3 3 5 3 3 3 3 5 \n",
      "6789 3 3 3 3 3 8 3 8 \n",
      "6789 3 3 3 3 3 8 3 8 \n",
      "6819 6 6 6 6 5 6 6 6 \n",
      "6906 3 3 3 3 3 9 3 3 \n",
      "6966 9 9 9 9 9 4 9 9 \n",
      "6979 4 4 4 4 4 9 4 9 \n",
      "6979 4 4 4 4 4 9 4 9 \n",
      "7026 1 1 1 1 4 4 4 4 \n",
      "7026 1 1 1 1 4 4 4 4 \n",
      "7026 1 1 1 1 4 4 4 4 \n",
      "7026 1 1 1 1 4 4 4 4 \n",
      "7035 2 8 2 2 2 2 2 2 \n",
      "7038 3 3 3 3 3 0 3 3 \n",
      "7063 7 7 8 8 8 8 7 8 \n",
      "7063 7 7 8 8 8 8 7 8 \n",
      "7063 7 7 8 8 8 8 7 8 \n",
      "7063 7 7 8 8 8 8 7 8 \n",
      "7063 7 7 8 8 8 8 7 8 \n",
      "7073 4 7 4 2 7 7 7 7 \n",
      "7073 4 7 4 2 7 7 7 7 \n",
      "7073 4 7 4 2 7 7 7 7 \n",
      "7073 4 7 4 2 7 7 7 7 \n",
      "7073 4 7 4 2 7 7 7 7 \n",
      "7073 4 7 4 2 7 7 7 7 \n",
      "7128 4 8 4 8 8 8 8 8 \n",
      "7128 4 8 4 8 8 8 8 8 \n",
      "7128 4 8 4 8 8 8 8 8 \n",
      "7128 4 8 4 8 8 8 8 8 \n",
      "7128 4 8 4 8 8 8 8 8 \n",
      "7128 4 8 4 8 8 8 8 8 \n",
      "7149 5 6 5 5 5 6 6 5 \n",
      "7149 5 6 5 5 5 6 6 5 \n",
      "7149 5 6 5 5 5 6 6 5 \n",
      "7153 9 9 9 9 9 4 9 9 \n",
      "7169 4 6 6 6 4 4 4 4 \n",
      "7169 4 6 6 6 4 4 4 4 \n",
      "7169 4 6 6 6 4 4 4 4 \n",
      "7179 1 1 1 1 1 1 6 1 \n",
      "7183 5 5 5 5 3 5 5 5 \n",
      "7242 2 7 2 2 7 2 2 2 \n",
      "7242 2 7 2 2 7 2 2 2 \n",
      "7264 3 3 5 5 3 3 3 5 \n",
      "7264 3 3 5 5 3 3 3 5 \n",
      "7264 3 3 5 5 3 3 3 5 \n",
      "7322 6 6 6 6 6 6 6 5 \n",
      "7341 2 7 2 7 2 2 2 2 \n",
      "7341 2 7 2 7 2 2 2 2 \n",
      "7385 6 6 6 6 6 6 6 5 \n",
      "7392 6 0 6 6 6 6 6 6 \n",
      "7407 8 8 8 8 8 8 8 4 \n",
      "7457 0 0 0 0 0 0 2 0 \n",
      "7461 9 9 4 9 4 4 4 4 \n",
      "7461 9 9 4 9 4 4 4 4 \n",
      "7461 9 9 4 9 4 4 4 4 \n",
      "7461 9 9 4 9 4 4 4 4 \n",
      "7461 9 9 4 9 4 4 4 4 \n",
      "7506 7 7 7 7 7 7 7 2 \n",
      "7593 9 9 9 9 9 9 3 9 \n",
      "7635 6 6 6 6 6 6 6 5 \n",
      "7657 5 6 6 5 5 0 6 5 \n",
      "7657 5 6 6 5 5 0 6 5 \n",
      "7657 5 6 6 5 5 0 6 5 \n",
      "7657 5 6 6 5 5 0 6 5 \n",
      "7678 8 8 8 8 8 8 3 3 \n",
      "7678 8 8 8 8 8 8 3 3 \n",
      "7692 6 6 6 8 6 6 6 6 \n",
      "7755 6 6 6 6 5 6 6 6 \n",
      "7837 3 3 3 2 3 3 3 3 \n",
      "7882 7 7 7 7 7 7 7 9 \n",
      "7916 0 0 0 0 0 0 2 0 \n",
      "7955 3 3 3 3 7 7 7 3 \n",
      "7955 3 3 3 3 7 7 7 3 \n",
      "7955 3 3 3 3 7 7 7 3 \n",
      "7966 9 9 4 9 4 9 9 9 \n",
      "7966 9 9 4 9 4 9 9 9 \n",
      "7977 9 9 9 9 9 0 9 9 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "7994 7 2 2 2 2 2 2 2 \n",
      "8014 2 2 2 2 4 2 2 2 \n",
      "8031 9 8 8 8 9 8 3 3 \n",
      "8031 9 8 8 8 9 8 3 3 \n",
      "8031 9 8 8 8 9 8 3 3 \n",
      "8031 9 8 8 8 9 8 3 3 \n",
      "8031 9 8 8 8 9 8 3 3 \n",
      "8031 9 8 8 8 9 8 3 3 \n",
      "8087 4 4 6 4 4 4 4 4 \n",
      "8119 9 3 9 9 3 3 3 9 \n",
      "8119 9 3 9 9 3 3 3 9 \n",
      "8119 9 3 9 9 3 3 3 9 \n",
      "8119 9 3 9 9 3 3 3 9 \n",
      "8163 8 8 8 8 3 8 3 3 \n",
      "8163 8 8 8 8 3 8 3 3 \n",
      "8163 8 8 8 8 3 8 3 3 \n",
      "8166 7 7 7 2 7 7 7 7 \n",
      "8183 6 6 6 6 6 6 6 5 \n",
      "8192 5 8 5 5 5 5 5 5 \n",
      "8199 0 0 6 6 0 0 0 0 \n",
      "8199 0 0 6 6 0 0 0 0 \n",
      "8255 3 3 3 3 3 7 3 3 \n",
      "8268 4 4 8 4 4 4 4 8 \n",
      "8268 4 4 8 4 4 4 4 8 \n",
      "8296 5 6 6 5 5 0 5 5 \n",
      "8296 5 6 6 5 5 0 5 5 \n",
      "8296 5 6 6 5 5 0 5 5 \n",
      "8303 5 5 5 8 5 5 8 5 \n",
      "8303 5 5 5 8 5 5 8 5 \n",
      "8306 2 2 2 2 4 2 2 2 \n",
      "8316 3 3 3 5 3 3 3 5 \n",
      "8316 3 3 3 5 3 3 3 5 \n",
      "8326 3 6 3 6 6 0 6 6 \n",
      "8326 3 6 3 6 6 0 6 6 \n",
      "8326 3 6 3 6 6 0 6 6 \n",
      "8326 3 6 3 6 6 0 6 6 \n",
      "8326 3 6 3 6 6 0 6 6 \n",
      "8326 3 6 3 6 6 0 6 6 \n",
      "8330 0 0 6 0 9 0 0 0 \n",
      "8330 0 0 6 0 9 0 0 0 \n",
      "8337 9 9 9 2 9 9 9 9 \n",
      "8339 5 5 5 5 3 5 5 5 \n",
      "8365 5 6 5 6 5 6 6 5 \n",
      "8365 5 6 5 6 5 6 6 5 \n",
      "8365 5 6 5 6 5 6 6 5 \n",
      "8365 5 6 5 6 5 6 6 5 \n",
      "8403 2 2 2 2 7 2 2 2 \n",
      "8456 9 9 9 9 9 8 8 9 \n",
      "8456 9 9 9 9 9 8 8 9 \n",
      "8458 4 4 4 9 4 4 4 9 \n",
      "8458 4 4 4 9 4 4 4 9 \n",
      "8481 2 7 2 7 7 2 2 2 \n",
      "8481 2 7 2 7 7 2 2 2 \n",
      "8481 2 7 2 7 7 2 2 2 \n",
      "8499 7 9 7 7 9 7 7 9 \n",
      "8499 7 9 7 7 9 7 7 9 \n",
      "8499 7 9 7 7 9 7 7 9 \n",
      "8525 7 7 7 7 7 7 7 4 \n",
      "8530 0 0 0 0 9 0 2 0 \n",
      "8530 0 0 0 0 9 0 2 0 \n",
      "8578 1 1 1 1 4 1 1 1 \n",
      "8595 4 4 4 4 4 4 4 9 \n",
      "8604 9 9 9 9 4 9 9 9 \n",
      "8605 2 2 2 2 2 7 2 2 \n",
      "8626 6 6 6 6 3 3 6 8 \n",
      "8626 6 6 6 6 3 3 6 8 \n",
      "8626 6 6 6 6 3 3 6 8 \n",
      "8634 4 6 4 4 4 4 4 4 \n",
      "8656 6 6 6 6 6 6 6 5 \n",
      "8667 2 8 2 2 2 2 2 2 \n",
      "8674 0 0 0 5 5 0 5 5 \n",
      "8674 0 0 0 5 5 0 5 5 \n",
      "8674 0 0 0 5 5 0 5 5 \n",
      "8674 0 0 0 5 5 0 5 5 \n",
      "8711 7 7 7 7 4 7 7 4 \n",
      "8711 7 7 7 7 4 7 7 4 \n",
      "8736 9 9 9 9 7 9 7 9 \n",
      "8736 9 9 9 9 7 9 7 9 \n",
      "8776 7 7 7 2 7 7 7 2 \n",
      "8776 7 7 7 2 7 7 7 2 \n",
      "8801 7 7 7 7 7 7 7 2 \n",
      "8821 3 1 3 3 3 3 3 3 \n",
      "8838 8 8 8 8 8 6 8 8 \n",
      "8861 7 7 7 7 4 7 7 7 \n",
      "8866 2 7 2 2 2 2 2 2 \n",
      "8868 7 7 7 9 7 7 7 5 \n",
      "8868 7 7 7 9 7 7 7 5 \n",
      "8901 7 7 7 7 7 7 7 9 \n",
      "8940 7 7 7 7 3 3 3 3 \n",
      "8940 7 7 7 7 3 3 3 3 \n",
      "8940 7 7 7 7 3 3 3 3 \n",
      "8940 7 7 7 7 3 3 3 3 \n",
      "8993 5 5 5 5 3 3 3 5 \n",
      "8993 5 5 5 5 3 3 3 5 \n",
      "8993 5 5 5 5 3 3 3 5 \n",
      "9014 4 4 4 4 4 4 7 4 \n",
      "9029 6 6 6 6 5 6 6 6 \n",
      "9031 6 6 6 6 5 6 6 5 \n",
      "9031 6 6 6 6 5 6 6 5 \n",
      "9040 2 7 2 2 7 7 7 2 \n",
      "9040 2 7 2 2 7 7 7 2 \n",
      "9040 2 7 2 2 7 7 7 2 \n",
      "9040 2 7 2 2 7 7 7 2 \n",
      "9051 0 2 0 2 0 0 2 5 \n",
      "9051 0 2 0 2 0 0 2 5 \n",
      "9051 0 2 0 2 0 0 2 5 \n",
      "9051 0 2 0 2 0 0 2 5 \n",
      "9115 0 0 0 0 9 0 0 0 \n",
      "9138 0 0 0 0 0 0 6 0 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9158 8 9 9 9 9 9 9 9 \n",
      "9202 9 9 8 8 7 2 7 2 \n",
      "9202 9 9 8 8 7 2 7 2 \n",
      "9202 9 9 8 8 7 2 7 2 \n",
      "9202 9 9 8 8 7 2 7 2 \n",
      "9202 9 9 8 8 7 2 7 2 \n",
      "9202 9 9 8 8 7 2 7 2 \n",
      "9276 3 5 5 3 3 3 3 5 \n",
      "9276 3 5 5 3 3 3 3 5 \n",
      "9276 3 5 5 3 3 3 3 5 \n",
      "9326 6 6 6 6 6 6 6 5 \n",
      "9335 7 9 4 7 4 7 7 9 \n",
      "9335 7 9 4 7 4 7 7 9 \n",
      "9335 7 9 4 7 4 7 7 9 \n",
      "9335 7 9 4 7 4 7 7 9 \n",
      "9372 1 1 1 1 1 1 1 4 \n",
      "9393 1 7 7 1 7 7 7 7 \n",
      "9393 1 7 7 1 7 7 7 7 \n",
      "9393 1 7 7 1 7 7 7 7 \n",
      "9393 1 7 7 1 7 7 7 7 \n",
      "9393 1 7 7 1 7 7 7 7 \n",
      "9393 1 7 7 1 7 7 7 7 \n",
      "9397 2 2 2 2 3 2 2 2 \n",
      "9443 8 1 8 8 8 8 8 8 \n",
      "9508 6 6 6 6 5 6 6 6 \n",
      "9545 7 7 7 7 7 7 7 9 \n",
      "9550 7 7 7 7 7 7 7 8 \n",
      "9575 5 5 5 5 5 8 5 5 \n",
      "9577 9 9 9 8 9 9 9 9 \n",
      "9582 2 2 8 2 2 2 2 2 \n",
      "9585 2 2 2 2 2 2 2 5 \n",
      "9597 1 1 1 8 1 1 1 4 \n",
      "9597 1 1 1 8 1 1 1 4 \n",
      "9645 9 9 9 9 9 9 3 9 \n",
      "9663 9 9 9 9 9 4 4 9 \n",
      "9663 9 9 9 9 9 4 4 9 \n",
      "9673 5 8 5 5 8 8 8 8 \n",
      "9673 5 8 5 5 8 8 8 8 \n",
      "9673 5 8 5 5 8 8 8 8 \n",
      "9673 5 8 5 5 8 8 8 8 \n",
      "9673 5 8 5 5 8 8 8 8 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9744 7 3 3 3 3 3 3 3 \n",
      "9772 4 4 4 4 4 4 6 4 \n",
      "9814 9 4 4 4 4 4 4 9 \n",
      "9814 9 4 4 4 4 4 4 9 \n",
      "9814 9 4 4 4 4 4 4 9 \n",
      "9814 9 4 4 4 4 4 4 9 \n",
      "9814 9 4 4 4 4 4 4 9 \n",
      "9814 9 4 4 4 4 4 4 9 \n",
      "9881 2 7 7 2 2 2 2 2 \n",
      "9881 2 7 7 2 2 2 2 2 \n",
      "9883 6 6 6 6 6 6 6 5 \n",
      "9900 0 8 0 8 8 8 8 8 \n",
      "9900 0 8 0 8 8 8 8 8 \n",
      "9900 0 8 0 8 8 8 8 8 \n",
      "9900 0 8 0 8 8 8 8 8 \n",
      "9900 0 8 0 8 8 8 8 8 \n",
      "9900 0 8 0 8 8 8 8 8 \n",
      "9915 4 1 4 2 4 4 4 4 \n",
      "9915 4 1 4 2 4 4 4 4 \n",
      "9924 4 1 8 8 4 7 1 2 \n",
      "9924 4 1 8 8 4 7 1 2 \n",
      "9924 4 1 8 8 4 7 1 2 \n",
      "9924 4 1 8 8 4 7 1 2 \n",
      "9924 4 1 8 8 4 7 1 2 \n",
      "9924 4 1 8 8 4 7 1 2 \n",
      "9981 8 6 5 8 5 6 6 6 \n",
      "9981 8 6 5 8 5 6 6 6 \n",
      "9981 8 6 5 8 5 6 6 6 \n",
      "9981 8 6 5 8 5 6 6 6 \n",
      "9981 8 6 5 8 5 6 6 6 \n",
      "9981 8 6 5 8 5 6 6 6 \n",
      "10040 6 1 1 6 6 6 6 6 \n",
      "10040 6 1 1 6 6 6 6 6 \n",
      "10118 7 7 7 7 7 7 7 9 \n",
      "10128 7 7 7 9 7 7 7 7 \n",
      "10135 9 9 9 9 9 4 9 9 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10160 9 1 1 1 1 1 8 5 \n",
      "10165 6 6 6 6 6 6 6 5 \n",
      "10216 4 4 4 9 4 4 4 9 \n",
      "10216 4 4 4 9 4 4 4 9 \n",
      "10252 1 1 1 1 1 1 1 9 \n",
      "10273 7 7 7 7 7 7 7 2 \n",
      "10295 2 1 2 2 2 2 2 2 \n",
      "10301 7 3 7 7 7 7 7 7 \n",
      "10318 6 6 6 6 5 6 6 5 \n",
      "10318 6 6 6 6 5 6 6 5 \n",
      "10362 5 8 5 5 8 5 8 5 \n",
      "10362 5 8 5 5 8 5 8 5 \n",
      "10362 5 8 5 5 8 5 8 5 \n",
      "10394 7 7 7 4 4 7 7 7 \n",
      "10394 7 7 7 4 4 7 7 7 \n",
      "10396 9 3 9 3 3 3 3 3 \n",
      "10396 9 3 9 3 3 3 3 3 \n",
      "10396 9 3 9 3 3 3 3 3 \n",
      "10396 9 3 9 3 3 3 3 3 \n",
      "10396 9 3 9 3 3 3 3 3 \n",
      "10396 9 3 9 3 3 3 3 3 \n",
      "10429 5 3 3 5 3 5 5 5 \n",
      "10429 5 3 3 5 3 5 5 5 \n",
      "10429 5 3 3 5 3 5 5 5 \n",
      "10434 9 9 9 9 4 4 4 9 \n",
      "10434 9 9 9 9 4 4 4 9 \n",
      "10434 9 9 9 9 4 4 4 9 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10503 9 5 5 5 5 5 5 5 \n",
      "10512 7 9 7 7 7 7 7 7 \n",
      "10536 2 2 2 2 2 2 7 2 \n",
      "10786 9 9 5 5 9 9 5 5 \n",
      "10786 9 9 5 5 9 9 5 5 \n",
      "10786 9 9 5 5 9 9 5 5 \n",
      "10786 9 9 5 5 9 9 5 5 \n",
      "10797 0 0 0 0 5 0 8 5 \n",
      "10797 0 0 0 0 5 0 8 5 \n",
      "10797 0 0 0 0 5 0 8 5 \n",
      "10804 0 0 0 0 0 0 0 2 \n",
      "10839 1 1 1 1 9 1 9 9 \n",
      "10839 1 1 1 1 9 1 9 9 \n",
      "10839 1 1 1 1 9 1 9 9 \n",
      "10860 0 3 0 5 3 3 3 3 \n",
      "10860 0 3 0 5 3 3 3 3 \n",
      "10860 0 3 0 5 3 3 3 3 \n",
      "10860 0 3 0 5 3 3 3 3 \n",
      "10860 0 3 0 5 3 3 3 3 \n",
      "10860 0 3 0 5 3 3 3 3 \n",
      "10865 8 8 9 8 8 8 8 8 \n",
      "10870 6 6 6 6 6 6 6 5 \n",
      "10910 2 2 2 2 3 3 2 3 \n",
      "10910 2 2 2 2 3 3 2 3 \n",
      "10910 2 2 2 2 3 3 2 3 \n",
      "10983 6 6 6 6 5 6 6 6 \n",
      "11062 5 8 5 5 5 8 5 5 \n",
      "11062 5 8 5 5 5 8 5 5 \n",
      "11078 7 7 7 4 7 7 7 7 \n",
      "11106 5 1 5 5 5 5 5 5 \n",
      "11123 3 3 3 2 1 2 3 3 \n",
      "11123 3 3 3 2 1 2 3 3 \n",
      "11123 3 3 3 2 1 2 3 3 \n",
      "11175 7 7 7 7 7 7 7 9 \n",
      "11188 1 1 1 1 1 1 1 9 \n",
      "11231 5 3 5 5 3 5 5 5 \n",
      "11231 5 3 5 5 3 5 5 5 \n",
      "11256 1 1 1 1 1 1 5 1 \n",
      "11266 0 0 0 0 0 0 2 0 \n",
      "11272 7 7 7 7 1 7 7 7 \n",
      "11307 8 8 0 8 8 0 8 8 \n",
      "11307 8 8 0 8 8 0 8 8 \n",
      "11338 1 1 2 1 2 2 2 2 \n",
      "11338 1 1 2 1 2 2 2 2 \n",
      "11338 1 1 2 1 2 2 2 2 \n",
      "11338 1 1 2 1 2 2 2 2 \n",
      "11338 1 1 2 1 2 2 2 2 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11352 8 5 5 5 5 5 5 5 \n",
      "11370 9 9 8 9 8 8 8 9 \n",
      "11370 9 9 8 9 8 8 8 9 \n",
      "11370 9 9 8 9 8 8 8 9 \n",
      "11370 9 9 8 9 8 8 8 9 \n",
      "11371 4 1 4 4 4 4 4 4 \n",
      "11382 4 1 4 4 4 4 4 4 \n",
      "11393 8 8 8 8 8 8 3 8 \n",
      "11425 7 7 7 9 7 7 7 5 \n",
      "11425 7 7 7 9 7 7 7 5 \n",
      "11507 4 4 4 4 4 4 4 9 \n",
      "11524 9 9 9 9 9 7 9 9 \n",
      "11535 2 2 2 2 4 2 2 2 \n",
      "11539 0 6 6 6 5 0 6 6 \n",
      "11539 0 6 6 6 5 0 6 6 \n",
      "11539 0 6 6 6 5 0 6 6 \n",
      "11539 0 6 6 6 5 0 6 6 \n",
      "11539 0 6 6 6 5 0 6 6 \n",
      "11539 0 6 6 6 5 0 6 6 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11587 3 5 5 5 5 5 5 5 \n",
      "11596 4 4 4 4 7 7 7 7 \n",
      "11596 4 4 4 4 7 7 7 7 \n",
      "11596 4 4 4 4 7 7 7 7 \n",
      "11596 4 4 4 4 7 7 7 7 \n",
      "11615 7 7 7 2 7 7 7 7 \n",
      "11622 4 1 4 4 4 4 4 4 \n",
      "11641 0 0 0 0 0 0 0 8 \n",
      "11654 2 1 1 1 1 2 1 1 \n",
      "11654 2 1 1 1 1 2 1 1 \n",
      "11654 2 1 1 1 1 2 1 1 \n",
      "11654 2 1 1 1 1 2 1 1 \n",
      "11654 2 1 1 1 1 2 1 1 \n",
      "11654 2 1 1 1 1 2 1 1 \n",
      "11666 3 3 3 8 8 8 3 3 \n",
      "11666 3 3 3 8 8 8 3 3 \n",
      "11666 3 3 3 8 8 8 3 3 \n",
      "11695 9 8 9 9 9 8 8 8 \n",
      "11695 9 8 9 9 9 8 8 8 \n",
      "11695 9 8 9 9 9 8 8 8 \n",
      "11695 9 8 9 9 9 8 8 8 \n",
      "11697 3 3 3 3 8 3 3 8 \n",
      "11697 3 3 3 3 8 3 3 8 \n",
      "11718 2 2 2 2 1 2 1 1 \n",
      "11718 2 2 2 2 1 2 1 1 \n",
      "11718 2 2 2 2 1 2 1 1 \n",
      "11752 5 5 5 6 5 6 6 5 \n",
      "11752 5 5 5 6 5 6 6 5 \n",
      "11752 5 5 5 6 5 6 6 5 \n",
      "11762 8 5 8 8 8 8 8 5 \n",
      "11762 8 5 8 8 8 8 8 5 \n",
      "11764 6 6 6 6 6 6 6 5 \n",
      "11784 4 4 4 4 4 4 1 4 \n",
      "11801 5 5 3 5 5 5 5 5 \n",
      "11890 4 4 4 9 4 4 4 4 \n",
      "11930 6 6 6 6 5 6 6 6 \n",
      "11984 2 7 2 2 2 2 2 2 \n",
      "12035 7 7 7 5 7 7 7 7 \n",
      "12086 0 0 0 5 3 0 2 3 \n",
      "12086 0 0 0 5 3 0 2 3 \n",
      "12086 0 0 0 5 3 0 2 3 \n",
      "12086 0 0 0 5 3 0 2 3 \n",
      "12165 9 9 9 9 9 7 9 9 \n",
      "12206 8 1 1 8 8 8 8 8 \n",
      "12206 8 1 1 8 8 8 8 8 \n",
      "12208 6 6 6 6 6 6 6 5 \n",
      "12277 7 7 7 7 0 0 0 3 \n",
      "12277 7 7 7 7 0 0 0 3 \n",
      "12277 7 7 7 7 0 0 0 3 \n",
      "12277 7 7 7 7 0 0 0 3 \n",
      "12296 5 5 5 5 8 5 5 5 \n",
      "12303 3 3 5 5 3 3 3 3 \n",
      "12303 3 3 5 5 3 3 3 3 \n",
      "12309 7 7 7 7 4 7 7 7 \n",
      "12320 1 1 1 1 4 1 4 4 \n",
      "12320 1 1 1 1 4 1 4 4 \n",
      "12320 1 1 1 1 4 1 4 4 \n",
      "12338 6 6 6 6 6 6 6 5 \n",
      "12344 4 4 4 4 4 4 4 9 \n",
      "12420 2 2 2 2 2 7 2 2 \n",
      "12467 1 1 1 1 2 2 2 2 \n",
      "12467 1 1 1 1 2 2 2 2 \n",
      "12467 1 1 1 1 2 2 2 2 \n",
      "12467 1 1 1 1 2 2 2 2 \n",
      "12527 2 7 2 2 2 2 2 2 \n",
      "12535 3 3 1 3 3 3 3 3 \n",
      "12538 9 9 4 4 4 4 4 4 \n",
      "12538 9 9 4 4 4 4 4 4 \n",
      "12538 9 9 4 4 4 4 4 4 \n",
      "12538 9 9 4 4 4 4 4 4 \n",
      "12538 9 9 4 4 4 4 4 4 \n",
      "12538 9 9 4 4 4 4 4 4 \n",
      "12550 4 4 4 4 4 4 9 9 \n",
      "12550 4 4 4 4 4 4 9 9 \n",
      "12551 7 7 7 7 7 7 7 3 \n",
      "12583 4 4 4 4 4 4 4 9 \n",
      "12618 5 5 3 5 5 5 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12620 3 5 5 2 8 8 5 5 \n",
      "12635 0 0 6 0 0 0 0 0 \n",
      "12666 4 6 4 6 6 6 6 6 \n",
      "12666 4 6 4 6 6 6 6 6 \n",
      "12666 4 6 4 6 6 6 6 6 \n",
      "12666 4 6 4 6 6 6 6 6 \n",
      "12666 4 6 4 6 6 6 6 6 \n",
      "12666 4 6 4 6 6 6 6 6 \n",
      "12674 0 0 0 0 4 0 0 0 \n",
      "12725 6 1 6 6 1 6 6 6 \n",
      "12725 6 1 6 6 1 6 6 6 \n",
      "12726 4 7 4 7 7 7 7 7 \n",
      "12726 4 7 4 7 7 7 7 7 \n",
      "12726 4 7 4 7 7 7 7 7 \n",
      "12726 4 7 4 7 7 7 7 7 \n",
      "12726 4 7 4 7 7 7 7 7 \n",
      "12726 4 7 4 7 7 7 7 7 \n",
      "12730 2 2 2 2 4 2 2 2 \n",
      "12754 3 3 3 3 3 3 3 5 \n",
      "12761 4 1 4 4 4 4 4 4 \n",
      "12771 9 9 9 9 9 4 9 9 \n",
      "12829 7 7 7 3 7 7 7 7 \n",
      "12838 9 9 9 7 7 7 7 9 \n",
      "12838 9 9 9 7 7 7 7 9 \n",
      "12838 9 9 9 7 7 7 7 9 \n",
      "12838 9 9 9 7 7 7 7 9 \n",
      "12854 4 9 4 4 4 4 4 9 \n",
      "12854 4 9 4 4 4 4 4 9 \n",
      "12859 6 6 6 6 5 6 6 6 \n",
      "12864 9 9 9 9 9 0 9 8 \n",
      "12864 9 9 9 9 9 0 9 8 \n",
      "12865 8 8 3 8 8 8 8 8 \n",
      "12868 7 7 7 2 7 7 7 7 \n",
      "12873 9 9 9 9 9 9 8 8 \n",
      "12873 9 9 9 9 9 9 8 8 \n",
      "12884 2 2 2 2 7 2 2 2 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12893 9 8 0 8 8 8 8 8 \n",
      "12896 8 8 8 8 5 8 8 5 \n",
      "12896 8 8 8 8 5 8 8 5 \n",
      "12991 5 7 5 5 7 7 7 7 \n",
      "12991 5 7 5 5 7 7 7 7 \n",
      "12991 5 7 5 5 7 7 7 7 \n",
      "12991 5 7 5 5 7 7 7 7 \n",
      "12991 5 7 5 5 7 7 7 7 \n",
      "13043 3 3 3 3 3 3 3 5 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13127 4 6 6 6 6 6 6 6 \n",
      "13129 9 9 9 9 9 9 7 9 \n",
      "13208 7 7 7 3 7 7 7 7 \n",
      "13275 0 0 0 0 9 9 0 0 \n",
      "13275 0 0 0 0 9 9 0 0 \n",
      "13290 9 7 7 9 7 7 7 9 \n",
      "13290 9 7 7 9 7 7 7 9 \n",
      "13290 9 7 7 9 7 7 7 9 \n",
      "13290 9 7 7 9 7 7 7 9 \n",
      "13290 9 7 7 9 7 7 7 9 \n",
      "13326 5 1 5 5 5 5 5 5 \n",
      "13366 7 7 7 7 7 7 7 9 \n",
      "13383 9 9 9 9 9 9 9 5 \n",
      "13421 4 9 4 4 4 4 4 4 \n",
      "13434 2 1 2 1 1 2 1 1 \n",
      "13434 2 1 2 1 1 2 1 1 \n",
      "13434 2 1 2 1 1 2 1 1 \n",
      "13434 2 1 2 1 1 2 1 1 \n",
      "13434 2 1 2 1 1 2 1 1 \n",
      "13447 7 7 7 7 7 7 7 9 \n",
      "13457 7 7 7 2 7 7 7 7 \n",
      "13458 6 1 6 6 1 6 6 6 \n",
      "13458 6 1 6 6 1 6 6 6 \n",
      "13468 8 8 8 8 8 8 8 5 \n",
      "13478 2 2 2 2 4 2 1 1 \n",
      "13478 2 2 2 2 4 2 1 1 \n",
      "13478 2 2 2 2 4 2 1 1 \n",
      "13554 5 5 3 5 5 5 5 5 \n",
      "13556 6 6 6 6 6 6 6 5 \n",
      "13559 5 3 5 5 3 5 3 5 \n",
      "13559 5 3 5 5 3 5 3 5 \n",
      "13559 5 3 5 5 3 5 3 5 \n",
      "13562 9 1 3 5 3 9 9 9 \n",
      "13562 9 1 3 5 3 9 9 9 \n",
      "13562 9 1 3 5 3 9 9 9 \n",
      "13562 9 1 3 5 3 9 9 9 \n",
      "13576 7 8 8 8 8 7 8 8 \n",
      "13576 7 8 8 8 8 7 8 8 \n",
      "13576 7 8 8 8 8 7 8 8 \n",
      "13576 7 8 8 8 8 7 8 8 \n",
      "13576 7 8 8 8 8 7 8 8 \n",
      "13576 7 8 8 8 8 7 8 8 \n",
      "13586 9 2 2 9 7 2 2 2 \n",
      "13586 9 2 2 9 7 2 2 2 \n",
      "13586 9 2 2 9 7 2 2 2 \n",
      "13586 9 2 2 9 7 2 2 2 \n",
      "13586 9 2 2 9 7 2 2 2 \n",
      "13586 9 2 2 9 7 2 2 2 \n",
      "13590 9 8 9 9 8 8 8 8 \n",
      "13590 9 8 9 9 8 8 8 8 \n",
      "13590 9 8 9 9 8 8 8 8 \n",
      "13590 9 8 9 9 8 8 8 8 \n",
      "13590 9 8 9 9 8 8 8 8 \n",
      "13593 6 6 6 6 6 0 6 5 \n",
      "13593 6 6 6 6 6 0 6 5 \n",
      "13616 2 1 1 2 1 1 2 2 \n",
      "13616 2 1 1 2 1 1 2 2 \n",
      "13616 2 1 1 2 1 1 2 2 \n",
      "13616 2 1 1 2 1 1 2 2 \n",
      "13617 5 5 5 5 5 5 3 5 \n",
      "13635 0 0 0 0 0 0 2 0 \n",
      "13713 7 7 7 2 7 7 7 7 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13730 8 9 4 5 5 5 5 5 \n",
      "13868 9 9 9 9 9 9 9 5 \n",
      "14014 3 3 1 3 3 3 3 3 \n",
      "14015 4 9 9 4 4 4 4 9 \n",
      "14015 4 9 9 4 4 4 4 9 \n",
      "14015 4 9 9 4 4 4 4 9 \n",
      "14066 0 0 0 6 0 0 0 0 \n",
      "14105 7 1 4 7 4 7 7 4 \n",
      "14105 7 1 4 7 4 7 7 4 \n",
      "14105 7 1 4 7 4 7 7 4 \n",
      "14105 7 1 4 7 4 7 7 4 \n",
      "14144 6 6 6 6 5 0 6 5 \n",
      "14144 6 6 6 6 5 0 6 5 \n",
      "14144 6 6 6 6 5 0 6 5 \n",
      "14234 9 9 9 8 9 9 9 9 \n",
      "14245 8 8 8 8 8 8 3 8 \n",
      "14262 6 1 6 6 6 6 6 6 \n",
      "14302 7 7 7 7 7 7 7 2 \n",
      "14306 3 3 3 3 3 3 2 3 \n",
      "14335 0 0 0 0 0 0 8 0 \n",
      "14376 7 7 7 2 7 7 7 7 \n",
      "14412 2 2 2 2 4 2 2 2 \n",
      "14459 9 4 9 9 4 4 4 9 \n",
      "14459 9 4 9 9 4 4 4 9 \n",
      "14459 9 4 9 9 4 4 4 9 \n",
      "14459 9 4 9 9 4 4 4 9 \n",
      "14465 4 4 4 4 4 4 4 9 \n",
      "14512 1 1 1 1 1 5 5 5 \n",
      "14512 1 1 1 1 1 5 5 5 \n",
      "14512 1 1 1 1 1 5 5 5 \n",
      "14535 4 4 4 4 9 4 4 9 \n",
      "14535 4 4 4 4 9 4 4 9 \n",
      "14547 3 3 2 3 3 3 3 3 \n",
      "14558 7 7 7 7 7 7 7 9 \n",
      "14568 2 2 2 2 2 2 7 2 \n",
      "14579 2 2 2 2 2 2 2 5 \n",
      "14592 2 1 2 2 1 2 2 2 \n",
      "14592 2 1 2 2 1 2 2 2 \n",
      "14636 2 2 2 2 3 2 2 2 \n",
      "14642 0 0 0 9 9 9 9 0 \n",
      "14642 0 0 0 9 9 9 9 0 \n",
      "14642 0 0 0 9 9 9 9 0 \n",
      "14642 0 0 0 9 9 9 9 0 \n",
      "14649 7 3 3 3 7 7 7 3 \n",
      "14649 7 3 3 3 7 7 7 3 \n",
      "14649 7 3 3 3 7 7 7 3 \n",
      "14649 7 3 3 3 7 7 7 3 \n",
      "14719 7 9 7 7 7 7 7 7 \n",
      "14729 9 9 3 9 9 9 9 9 \n",
      "14742 0 0 4 2 0 0 2 0 \n",
      "14742 0 0 4 2 0 0 2 0 \n",
      "14742 0 0 4 2 0 0 2 0 \n",
      "14772 6 6 6 6 6 6 6 5 \n",
      "14798 9 9 9 9 4 9 4 9 \n",
      "14798 9 9 9 9 4 9 4 9 \n",
      "14807 5 5 3 5 5 5 5 5 \n",
      "14840 1 1 1 1 7 1 7 1 \n",
      "14840 1 1 1 1 7 1 7 1 \n",
      "14895 5 5 5 5 3 0 5 5 \n",
      "14895 5 5 5 5 3 0 5 5 \n",
      "14979 6 6 6 6 6 6 6 5 \n",
      "14992 9 0 4 4 4 4 9 9 \n",
      "14992 9 0 4 4 4 4 9 9 \n",
      "14992 9 0 4 4 4 4 9 9 \n",
      "14992 9 0 4 4 4 4 9 9 \n",
      "14992 9 0 4 4 4 4 9 9 \n",
      "15034 5 1 1 5 1 5 5 5 \n",
      "15034 5 1 1 5 1 5 5 5 \n",
      "15034 5 1 1 5 1 5 5 5 \n",
      "15072 9 8 9 9 8 8 8 8 \n",
      "15072 9 8 9 9 8 8 8 8 \n",
      "15072 9 8 9 9 8 8 8 8 \n",
      "15072 9 8 9 9 8 8 8 8 \n",
      "15072 9 8 9 9 8 8 8 8 \n",
      "15076 3 3 3 3 3 3 3 5 \n",
      "15095 4 1 4 2 4 4 2 2 \n",
      "15095 4 1 4 2 4 4 2 2 \n",
      "15095 4 1 4 2 4 4 2 2 \n",
      "15095 4 1 4 2 4 4 2 2 \n",
      "15096 2 2 2 2 1 2 2 1 \n",
      "15096 2 2 2 2 1 2 2 1 \n",
      "15134 9 8 8 8 8 8 8 9 \n",
      "15134 9 8 8 8 8 8 8 9 \n",
      "15134 9 8 8 8 8 8 8 9 \n",
      "15134 9 8 8 8 8 8 8 9 \n",
      "15134 9 8 8 8 8 8 8 9 \n",
      "15134 9 8 8 8 8 8 8 9 \n",
      "15158 3 3 0 3 3 8 3 8 \n",
      "15158 3 3 0 3 3 8 3 8 \n",
      "15158 3 3 0 3 3 8 3 8 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15232 0 2 2 5 5 5 5 5 \n",
      "15301 3 3 8 3 3 3 3 3 \n",
      "15329 2 8 2 2 8 2 2 2 \n",
      "15329 2 8 2 2 8 2 2 2 \n",
      "15344 9 9 9 9 8 8 8 8 \n",
      "15344 9 9 9 9 8 8 8 8 \n",
      "15344 9 9 9 9 8 8 8 8 \n",
      "15344 9 9 9 9 8 8 8 8 \n",
      "15346 5 5 5 5 5 6 5 5 \n",
      "15359 6 6 6 6 8 8 8 8 \n",
      "15359 6 6 6 6 8 8 8 8 \n",
      "15359 6 6 6 6 8 8 8 8 \n",
      "15359 6 6 6 6 8 8 8 8 \n",
      "15413 8 8 8 8 5 5 1 5 \n",
      "15413 8 8 8 8 5 5 1 5 \n",
      "15413 8 8 8 8 5 5 1 5 \n",
      "15413 8 8 8 8 5 5 1 5 \n",
      "15438 8 8 8 8 8 8 8 9 \n",
      "15440 4 4 4 4 4 8 4 4 \n",
      "15620 2 2 2 2 4 2 2 2 \n",
      "15621 6 6 6 6 6 6 6 5 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15669 9 8 8 8 8 2 2 8 \n",
      "15706 7 7 7 7 7 7 7 5 \n",
      "15716 8 8 8 8 8 8 8 5 \n",
      "15717 2 2 2 2 8 2 2 3 \n",
      "15717 2 2 2 2 8 2 2 3 \n",
      "15740 7 7 7 7 7 7 7 8 \n",
      "15763 9 9 9 9 9 4 9 9 \n",
      "15765 6 6 6 6 5 6 6 6 \n",
      "15834 1 1 1 1 1 1 7 1 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "15961 9 5 5 5 5 5 5 5 \n",
      "16016 4 9 9 4 4 9 4 9 \n",
      "16016 4 9 9 4 4 9 4 9 \n",
      "16016 4 9 9 4 4 9 4 9 \n",
      "16016 4 9 9 4 4 9 4 9 \n",
      "16023 3 3 3 3 3 3 3 5 \n",
      "16024 9 2 9 9 9 9 9 9 \n",
      "16051 0 0 0 0 0 0 2 0 \n",
      "16091 9 9 9 9 4 9 9 9 \n",
      "16094 8 8 0 8 8 8 8 8 \n",
      "16107 2 2 2 2 6 2 2 2 \n",
      "16136 3 7 7 3 3 3 3 3 \n",
      "16136 3 7 7 3 3 3 3 3 \n",
      "16142 8 8 8 8 8 8 8 5 \n",
      "16148 0 0 0 0 0 0 6 0 \n",
      "16159 1 1 1 1 1 1 1 4 \n",
      "16176 5 5 5 5 9 0 5 5 \n",
      "16176 5 5 5 5 9 0 5 5 \n",
      "16204 3 3 3 9 9 0 3 0 \n",
      "16204 3 3 3 9 9 0 3 0 \n",
      "16204 3 3 3 9 9 0 3 0 \n",
      "16204 3 3 3 9 9 0 3 0 \n",
      "16232 9 7 9 7 7 7 7 2 \n",
      "16232 9 7 9 7 7 7 7 2 \n",
      "16232 9 7 9 7 7 7 7 2 \n",
      "16232 9 7 9 7 7 7 7 2 \n",
      "16232 9 7 9 7 7 7 7 2 \n",
      "16232 9 7 9 7 7 7 7 2 \n",
      "16267 6 6 6 6 1 6 6 6 \n",
      "16281 5 5 5 5 5 6 6 5 \n",
      "16281 5 5 5 5 5 6 6 5 \n",
      "16294 7 7 7 9 7 7 7 7 \n",
      "16308 6 6 6 6 5 6 6 5 \n",
      "16308 6 6 6 6 5 6 6 5 \n",
      "16323 0 0 0 0 9 0 0 0 \n",
      "16351 9 9 9 8 9 9 9 9 \n",
      "16365 8 6 6 8 5 6 5 5 \n",
      "16365 8 6 6 8 5 6 5 5 \n",
      "16365 8 6 6 8 5 6 5 5 \n",
      "16365 8 6 6 8 5 6 5 5 \n",
      "16365 8 6 6 8 5 6 5 5 \n",
      "16365 8 6 6 8 5 6 5 5 \n",
      "16368 1 1 1 1 1 1 7 4 \n",
      "16368 1 1 1 1 1 1 7 4 \n",
      "16447 9 9 9 8 9 9 9 9 \n",
      "16452 8 9 4 8 4 4 4 4 \n",
      "16452 8 9 4 8 4 4 4 4 \n",
      "16452 8 9 4 8 4 4 4 4 \n",
      "16452 8 9 4 8 4 4 4 4 \n",
      "16452 8 9 4 8 4 4 4 4 \n",
      "16452 8 9 4 8 4 4 4 4 \n",
      "16475 1 1 7 1 7 7 7 7 \n",
      "16475 1 1 7 1 7 7 7 7 \n",
      "16475 1 1 7 1 7 7 7 7 \n",
      "16475 1 1 7 1 7 7 7 7 \n",
      "16475 1 1 7 1 7 7 7 7 \n",
      "16490 1 1 7 1 7 7 7 7 \n",
      "16490 1 1 7 1 7 7 7 7 \n",
      "16490 1 1 7 1 7 7 7 7 \n",
      "16490 1 1 7 1 7 7 7 7 \n",
      "16490 1 1 7 1 7 7 7 7 \n",
      "16499 0 0 0 0 0 0 2 0 \n",
      "16514 4 9 4 4 4 4 8 4 \n",
      "16514 4 9 4 4 4 4 8 4 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16541 9 8 8 8 8 8 8 8 \n",
      "16544 5 7 5 5 3 0 5 5 \n",
      "16544 5 7 5 5 3 0 5 5 \n",
      "16544 5 7 5 5 3 0 5 5 \n",
      "16553 8 8 0 8 8 8 8 5 \n",
      "16553 8 8 0 8 8 8 8 5 \n",
      "16555 5 8 8 5 8 8 8 5 \n",
      "16555 5 8 8 5 8 8 8 5 \n",
      "16555 5 8 8 5 8 8 8 5 \n",
      "16555 5 8 8 5 8 8 8 5 \n",
      "16555 5 8 8 5 8 8 8 5 \n",
      "16587 6 6 6 6 5 6 6 6 \n",
      "16608 9 9 9 9 9 9 4 9 \n",
      "16614 2 7 2 2 7 7 7 7 \n",
      "16614 2 7 2 2 7 7 7 7 \n",
      "16614 2 7 2 2 7 7 7 7 \n",
      "16614 2 7 2 2 7 7 7 7 \n",
      "16614 2 7 2 2 7 7 7 7 \n",
      "16627 0 9 0 0 0 0 0 0 \n",
      "16648 5 3 3 5 5 5 5 5 \n",
      "16648 5 3 3 5 5 5 5 5 \n",
      "16666 4 1 4 4 4 4 4 4 \n",
      "16683 6 6 6 6 5 6 6 5 \n",
      "16683 6 6 6 6 5 6 6 5 \n",
      "16729 7 7 7 7 7 7 7 9 \n",
      "16746 7 7 7 7 7 7 7 2 \n",
      "16755 7 7 7 7 7 7 7 9 \n",
      "16765 2 2 2 2 1 2 2 2 \n",
      "16856 1 1 1 1 1 2 1 1 \n",
      "16897 6 6 6 6 0 6 6 6 \n",
      "16913 0 0 0 2 0 0 0 0 \n",
      "16924 7 7 7 7 4 7 7 7 \n",
      "16931 6 6 6 6 5 6 6 5 \n",
      "16931 6 6 6 6 5 6 6 5 \n",
      "16935 3 5 3 3 3 3 3 5 \n",
      "16935 3 5 3 3 3 3 3 5 \n",
      "16956 5 3 5 5 5 0 5 5 \n",
      "16956 5 3 5 5 5 0 5 5 \n",
      "16970 6 1 6 6 5 6 6 5 \n",
      "16970 6 1 6 6 5 6 6 5 \n",
      "16970 6 1 6 6 5 6 6 5 \n",
      "17000 1 3 3 1 3 9 8 9 \n",
      "17000 1 3 3 1 3 9 8 9 \n",
      "17000 1 3 3 1 3 9 8 9 \n",
      "17000 1 3 3 1 3 9 8 9 \n",
      "17000 1 3 3 1 3 9 8 9 \n",
      "17000 1 3 3 1 3 9 8 9 \n",
      "17066 9 9 9 8 9 8 8 9 \n",
      "17066 9 9 9 8 9 8 8 9 \n",
      "17066 9 9 9 8 9 8 8 9 \n",
      "17077 5 5 5 5 9 5 5 5 \n",
      "17093 4 9 4 4 4 4 4 4 \n",
      "17107 6 6 6 6 6 6 6 5 \n",
      "17137 9 9 4 9 9 9 9 9 \n",
      "17160 3 3 3 2 3 3 2 2 \n",
      "17160 3 3 3 2 3 3 2 2 \n",
      "17160 3 3 3 2 3 3 2 2 \n",
      "17218 4 2 4 4 4 4 4 4 \n",
      "17244 1 1 1 1 1 1 2 2 \n",
      "17244 1 1 1 1 1 1 2 2 \n",
      "17263 7 7 7 7 7 7 7 2 \n",
      "17288 7 1 7 7 7 7 7 7 \n",
      "17406 8 9 9 8 4 4 9 9 \n",
      "17406 8 9 9 8 4 4 9 9 \n",
      "17406 8 9 9 8 4 4 9 9 \n",
      "17406 8 9 9 8 4 4 9 9 \n",
      "17406 8 9 9 8 4 4 9 9 \n",
      "17406 8 9 9 8 4 4 9 9 \n",
      "17408 3 3 2 3 3 3 3 3 \n",
      "17434 1 1 1 8 8 1 8 8 \n",
      "17434 1 1 1 8 8 1 8 8 \n",
      "17434 1 1 1 8 8 1 8 8 \n",
      "17434 1 1 1 8 8 1 8 8 \n",
      "17453 6 6 6 6 5 0 6 5 \n",
      "17453 6 6 6 6 5 0 6 5 \n",
      "17453 6 6 6 6 5 0 6 5 \n",
      "17484 0 0 0 0 0 0 7 0 \n",
      "17505 6 6 6 6 4 6 2 6 \n",
      "17505 6 6 6 6 4 6 2 6 \n",
      "17553 4 2 4 4 4 4 4 4 \n",
      "17583 1 6 6 1 1 1 2 6 \n",
      "17583 1 6 6 1 1 1 2 6 \n",
      "17583 1 6 6 1 1 1 2 6 \n",
      "17583 1 6 6 1 1 1 2 6 \n",
      "17589 8 8 8 8 8 8 8 5 \n",
      "17621 9 9 9 9 4 9 9 9 \n",
      "17709 8 6 8 8 5 6 8 5 \n",
      "17709 8 6 8 8 5 6 8 5 \n",
      "17709 8 6 8 8 5 6 8 5 \n",
      "17709 8 6 8 8 5 6 8 5 \n",
      "17789 2 1 2 2 1 2 2 5 \n",
      "17789 2 1 2 2 1 2 2 5 \n",
      "17789 2 1 2 2 1 2 2 5 \n",
      "17856 4 6 4 4 4 6 6 4 \n",
      "17856 4 6 4 4 4 6 6 4 \n",
      "17856 4 6 4 4 4 6 6 4 \n",
      "17875 7 1 7 7 7 7 7 7 \n",
      "17883 6 6 6 6 6 0 6 5 \n",
      "17883 6 6 6 6 6 0 6 5 \n",
      "17889 1 1 1 1 1 1 1 8 \n",
      "17898 2 2 2 2 2 2 2 9 \n",
      "17931 3 3 3 8 3 3 3 3 \n",
      "17946 9 9 9 4 4 9 4 9 \n",
      "17946 9 9 9 4 4 9 4 9 \n",
      "17946 9 9 9 4 4 9 4 9 \n",
      "17951 5 1 5 5 5 5 5 5 \n",
      "17972 7 7 7 7 7 7 7 2 \n",
      "17981 8 8 8 5 5 5 5 5 \n",
      "17981 8 8 8 5 5 5 5 5 \n",
      "17981 8 8 8 5 5 5 5 5 \n",
      "17981 8 8 8 5 5 5 5 5 \n",
      "17981 8 8 8 5 5 5 5 5 \n",
      "18016 8 8 8 8 8 2 2 8 \n",
      "18016 8 8 8 8 8 2 2 8 \n",
      "18041 1 1 2 2 1 1 1 1 \n",
      "18041 1 1 2 2 1 1 1 1 \n",
      "18049 9 7 7 9 9 9 9 9 \n",
      "18049 9 7 7 9 9 9 9 9 \n",
      "18071 7 7 7 7 7 7 7 9 \n",
      "18107 4 4 4 4 9 4 9 9 \n",
      "18107 4 4 4 4 9 4 9 9 \n",
      "18107 4 4 4 4 9 4 9 9 \n",
      "18114 6 6 6 6 5 6 6 6 \n",
      "18117 5 8 5 5 8 8 8 5 \n",
      "18117 5 8 5 5 8 8 8 5 \n",
      "18117 5 8 5 5 8 8 8 5 \n",
      "18117 5 8 5 5 8 8 8 5 \n",
      "18124 3 3 3 3 3 3 5 3 \n",
      "18130 1 1 0 0 0 0 2 8 \n",
      "18130 1 1 0 0 0 0 2 8 \n",
      "18130 1 1 0 0 0 0 2 8 \n",
      "18130 1 1 0 0 0 0 2 8 \n",
      "18130 1 1 0 0 0 0 2 8 \n",
      "18130 1 1 0 0 0 0 2 8 \n",
      "18138 9 9 9 9 9 9 7 9 \n",
      "18166 1 1 1 1 1 7 7 4 \n",
      "18166 1 1 1 1 1 7 7 4 \n",
      "18166 1 1 1 1 1 7 7 4 \n",
      "18180 4 4 4 8 8 8 8 8 \n",
      "18180 4 4 4 8 8 8 8 8 \n",
      "18180 4 4 4 8 8 8 8 8 \n",
      "18180 4 4 4 8 8 8 8 8 \n",
      "18180 4 4 4 8 8 8 8 8 \n",
      "18223 9 9 7 9 7 7 7 7 \n",
      "18223 9 9 7 9 7 7 7 7 \n",
      "18223 9 9 7 9 7 7 7 7 \n",
      "18223 9 9 7 9 7 7 7 7 \n",
      "18223 9 9 7 9 7 7 7 7 \n",
      "18227 4 4 4 4 4 4 4 9 \n",
      "18275 9 9 4 9 9 9 9 9 \n",
      "18279 6 6 6 6 6 6 6 5 \n",
      "18301 0 0 0 0 0 0 0 5 \n",
      "18315 6 6 6 6 6 6 6 5 \n",
      "18328 6 0 6 6 6 0 6 6 \n",
      "18328 6 0 6 6 6 0 6 6 \n",
      "18336 8 8 2 2 2 2 2 2 \n",
      "18336 8 8 2 2 2 2 2 2 \n",
      "18336 8 8 2 2 2 2 2 2 \n",
      "18336 8 8 2 2 2 2 2 2 \n",
      "18336 8 8 2 2 2 2 2 2 \n",
      "18336 8 8 2 2 2 2 2 2 \n",
      "18427 2 2 2 2 2 8 2 2 \n",
      "18430 5 7 5 5 3 5 7 5 \n",
      "18430 5 7 5 5 3 5 7 5 \n",
      "18430 5 7 5 5 3 5 7 5 \n",
      "18454 9 9 9 9 9 9 7 9 \n",
      "18462 4 1 4 6 4 4 2 2 \n",
      "18462 4 1 4 6 4 4 2 2 \n",
      "18462 4 1 4 6 4 4 2 2 \n",
      "18462 4 1 4 6 4 4 2 2 \n",
      "18482 7 2 2 2 3 7 7 5 \n",
      "18482 7 2 2 2 3 7 7 5 \n",
      "18482 7 2 2 2 3 7 7 5 \n",
      "18482 7 2 2 2 3 7 7 5 \n",
      "18482 7 2 2 2 3 7 7 5 \n",
      "18507 6 6 6 6 6 6 8 8 \n",
      "18507 6 6 6 6 6 6 8 8 \n",
      "18515 7 7 7 4 4 4 4 4 \n",
      "18515 7 7 7 4 4 4 4 4 \n",
      "18515 7 7 7 4 4 4 4 4 \n",
      "18515 7 7 7 4 4 4 4 4 \n",
      "18515 7 7 7 4 4 4 4 4 \n",
      "18554 5 5 5 5 3 5 5 5 \n",
      "18598 3 7 7 3 7 7 7 3 \n",
      "18598 3 7 7 3 7 7 7 3 \n",
      "18598 3 7 7 3 7 7 7 3 \n",
      "18598 3 7 7 3 7 7 7 3 \n",
      "18598 3 7 7 3 7 7 7 3 \n",
      "18623 7 7 7 7 7 7 7 8 \n",
      "18626 1 1 1 1 1 1 7 4 \n",
      "18626 1 1 1 1 1 1 7 4 \n",
      "18633 1 1 7 8 8 8 2 8 \n",
      "18633 1 1 7 8 8 8 2 8 \n",
      "18633 1 1 7 8 8 8 2 8 \n",
      "18633 1 1 7 8 8 8 2 8 \n",
      "18633 1 1 7 8 8 8 2 8 \n",
      "18633 1 1 7 8 8 8 2 8 \n",
      "18639 5 6 5 5 5 5 5 5 \n",
      "18647 4 7 4 4 4 7 7 4 \n",
      "18647 4 7 4 4 4 7 7 4 \n",
      "18647 4 7 4 4 4 7 7 4 \n",
      "18666 1 1 1 1 1 1 7 1 \n",
      "18672 4 4 4 9 4 4 4 9 \n",
      "18672 4 4 4 9 4 4 4 9 \n",
      "18687 6 6 6 6 6 6 6 5 \n",
      "18719 2 8 2 8 2 2 2 2 \n",
      "18719 2 8 2 8 2 2 2 2 \n",
      "18720 6 6 6 6 6 6 6 5 \n",
      "18732 2 2 2 2 2 2 2 5 \n",
      "18735 1 1 1 1 1 1 1 8 \n",
      "18751 6 6 6 6 4 6 6 6 \n",
      "18753 5 5 5 5 5 0 5 5 \n",
      "18761 6 6 6 6 5 6 6 6 \n",
      "18773 6 6 6 6 4 6 6 6 \n",
      "18777 6 6 6 6 5 6 6 6 \n",
      "18787 6 6 6 6 6 6 8 6 \n",
      "18840 4 9 4 4 4 4 4 4 \n",
      "18843 1 1 1 1 6 6 6 6 \n",
      "18843 1 1 1 1 6 6 6 6 \n",
      "18843 1 1 1 1 6 6 6 6 \n",
      "18843 1 1 1 1 6 6 6 6 \n",
      "18852 9 9 9 5 9 9 3 5 \n",
      "18852 9 9 9 5 9 9 3 5 \n",
      "18852 9 9 9 5 9 9 3 5 \n",
      "18928 7 7 7 7 7 7 7 9 \n",
      "18971 3 3 3 5 3 3 3 5 \n",
      "18971 3 3 3 5 3 3 3 5 \n",
      "19054 4 4 9 4 9 9 9 9 \n",
      "19054 4 4 9 4 9 9 9 9 \n",
      "19054 4 4 9 4 9 9 9 9 \n",
      "19054 4 4 9 4 9 9 9 9 \n",
      "19054 4 4 9 4 9 9 9 9 \n",
      "19076 9 9 9 9 4 7 7 4 \n",
      "19076 9 9 9 9 4 7 7 4 \n",
      "19076 9 9 9 9 4 7 7 4 \n",
      "19076 9 9 9 9 4 7 7 4 \n",
      "19083 1 1 1 1 1 7 7 7 \n",
      "19083 1 1 1 1 1 7 7 7 \n",
      "19083 1 1 1 1 1 7 7 7 \n",
      "19091 9 9 9 9 4 4 9 9 \n",
      "19091 9 9 9 9 4 4 9 9 \n",
      "19097 8 8 8 8 9 8 8 8 \n",
      "19143 5 5 5 5 3 3 3 5 \n",
      "19143 5 5 5 5 3 3 3 5 \n",
      "19143 5 5 5 5 3 3 3 5 \n",
      "19159 1 1 1 1 4 1 4 1 \n",
      "19159 1 1 1 1 4 1 4 1 \n",
      "19173 4 1 4 4 4 4 4 4 \n",
      "19186 9 9 9 9 9 4 9 9 \n",
      "19324 2 3 3 2 3 3 3 3 \n",
      "19324 2 3 3 2 3 3 3 3 \n",
      "19324 2 3 3 2 3 3 3 3 \n",
      "19324 2 3 3 2 3 3 3 3 \n",
      "19324 2 3 3 2 3 3 3 3 \n",
      "19324 2 3 3 2 3 3 3 3 \n",
      "19333 6 1 6 6 6 6 6 6 \n",
      "19351 9 1 1 9 7 1 1 9 \n",
      "19351 9 1 1 9 7 1 1 9 \n",
      "19351 9 1 1 9 7 1 1 9 \n",
      "19351 9 1 1 9 7 1 1 9 \n",
      "19351 9 1 1 9 7 1 1 9 \n",
      "19402 3 8 8 8 8 8 3 3 \n",
      "19402 3 8 8 8 8 8 3 3 \n",
      "19402 3 8 8 8 8 8 3 3 \n",
      "19402 3 8 8 8 8 8 3 3 \n",
      "19402 3 8 8 8 8 8 3 3 \n",
      "19411 7 1 7 7 7 7 7 7 \n",
      "19498 5 8 8 5 8 8 8 8 \n",
      "19498 5 8 8 5 8 8 8 8 \n",
      "19498 5 8 8 5 8 8 8 8 \n",
      "19498 5 8 8 5 8 8 8 8 \n",
      "19498 5 8 8 5 8 8 8 8 \n",
      "19498 5 8 8 5 8 8 8 8 \n",
      "19507 0 0 0 0 5 0 5 5 \n",
      "19507 0 0 0 0 5 0 5 5 \n",
      "19507 0 0 0 0 5 0 5 5 \n",
      "19542 8 8 8 8 9 8 9 9 \n",
      "19542 8 8 8 8 9 8 9 9 \n",
      "19542 8 8 8 8 9 8 9 9 \n",
      "19543 7 7 4 2 7 7 2 2 \n",
      "19543 7 7 4 2 7 7 2 2 \n",
      "19543 7 7 4 2 7 7 2 2 \n",
      "19543 7 7 4 2 7 7 2 2 \n",
      "19546 9 9 3 9 9 9 9 9 \n",
      "19559 6 6 6 6 5 6 5 5 \n",
      "19559 6 6 6 6 5 6 5 5 \n",
      "19559 6 6 6 6 5 6 5 5 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19585 0 6 6 6 9 9 6 9 \n",
      "19629 1 1 6 6 6 6 6 6 \n",
      "19629 1 1 6 6 6 6 6 6 \n",
      "19629 1 1 6 6 6 6 6 6 \n",
      "19629 1 1 6 6 6 6 6 6 \n",
      "19629 1 1 6 6 6 6 6 6 \n",
      "19629 1 1 6 6 6 6 6 6 \n",
      "19643 1 1 1 1 1 1 1 4 \n",
      "19649 4 4 1 4 4 4 4 4 \n",
      "19663 2 2 2 2 4 2 2 2 \n",
      "19674 0 0 0 0 0 0 6 0 \n",
      "19715 6 1 6 6 6 6 6 6 \n",
      "19754 7 7 7 2 7 7 7 7 \n",
      "19758 6 6 6 6 6 6 6 5 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19772 8 6 6 6 6 6 6 6 \n",
      "19791 7 9 7 7 7 7 7 7 \n",
      "19799 2 2 2 2 3 2 2 2 \n",
      "19804 9 9 9 9 9 9 2 9 \n",
      "19809 7 7 7 7 7 7 7 9 \n",
      "19842 1 1 1 1 1 1 1 8 \n",
      "19876 3 3 8 8 3 9 3 3 \n",
      "19876 3 3 8 8 3 9 3 3 \n",
      "19876 3 3 8 8 3 9 3 3 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19893 0 8 8 8 8 8 8 8 \n",
      "19896 0 0 0 0 8 0 8 0 \n",
      "19896 0 0 0 0 8 0 8 0 \n",
      "19906 1 1 1 1 4 1 7 4 \n",
      "19906 1 1 1 1 4 1 7 4 \n",
      "19906 1 1 1 1 4 1 7 4 \n",
      "19999 6 6 6 6 5 6 6 6 \n",
      "20021 0 0 0 0 9 0 2 0 \n",
      "20021 0 0 0 0 9 0 2 0 \n",
      "20039 2 2 8 2 2 2 2 2 \n",
      "20043 1 1 7 1 1 1 7 1 \n",
      "20043 1 1 7 1 1 1 7 1 \n",
      "20052 9 1 1 1 9 9 9 9 \n",
      "20052 9 1 1 1 9 9 9 9 \n",
      "20052 9 1 1 1 9 9 9 9 \n",
      "20071 6 6 6 6 5 6 6 6 \n",
      "20079 3 3 3 3 3 3 3 5 \n",
      "20101 5 5 3 5 3 3 5 5 \n",
      "20101 5 5 3 5 3 3 5 5 \n",
      "20101 5 5 3 5 3 3 5 5 \n",
      "20105 0 0 0 0 0 0 2 0 \n",
      "20112 9 4 9 9 4 4 4 9 \n",
      "20112 9 4 9 9 4 4 4 9 \n",
      "20112 9 4 9 9 4 4 4 9 \n",
      "20112 9 4 9 9 4 4 4 9 \n",
      "20131 6 6 6 6 6 8 6 6 \n",
      "20135 7 7 7 9 7 7 7 7 \n",
      "20141 2 8 2 2 2 2 2 2 \n",
      "20153 5 9 5 5 9 9 5 5 \n",
      "20153 5 9 5 5 9 9 5 5 \n",
      "20153 5 9 5 5 9 9 5 5 \n",
      "20164 6 6 6 6 6 6 6 8 \n",
      "20216 9 9 9 9 9 9 7 9 \n",
      "20226 5 8 5 5 5 5 5 5 \n",
      "20241 1 1 1 1 5 5 5 5 \n",
      "20241 1 1 1 1 5 5 5 5 \n",
      "20241 1 1 1 1 5 5 5 5 \n",
      "20241 1 1 1 1 5 5 5 5 \n",
      "20257 1 1 1 1 1 1 1 8 \n",
      "20280 1 1 1 1 1 1 1 3 \n",
      "20309 4 4 4 4 4 4 7 4 \n",
      "20335 0 0 0 0 0 0 2 0 \n",
      "20376 2 2 2 2 4 2 2 2 \n",
      "20387 7 7 7 9 7 7 7 7 \n",
      "20427 9 9 1 9 9 9 9 9 \n",
      "20437 6 6 6 6 6 6 6 5 \n",
      "20440 5 6 6 6 6 6 6 5 \n",
      "20440 5 6 6 6 6 6 6 5 \n",
      "20440 5 6 6 6 6 6 6 5 \n",
      "20440 5 6 6 6 6 6 6 5 \n",
      "20440 5 6 6 6 6 6 6 5 \n",
      "20440 5 6 6 6 6 6 6 5 \n",
      "20443 6 6 6 6 6 6 6 5 \n",
      "20455 4 4 4 4 4 4 9 4 \n",
      "20470 0 0 0 0 9 0 2 0 \n",
      "20470 0 0 0 0 9 0 2 0 \n",
      "20480 3 3 2 2 2 2 2 2 \n",
      "20480 3 3 2 2 2 2 2 2 \n",
      "20480 3 3 2 2 2 2 2 2 \n",
      "20480 3 3 2 2 2 2 2 2 \n",
      "20480 3 3 2 2 2 2 2 2 \n",
      "20480 3 3 2 2 2 2 2 2 \n",
      "20483 7 7 7 7 7 7 7 9 \n",
      "20509 1 1 1 1 3 3 3 5 \n",
      "20509 1 1 1 1 3 3 3 5 \n",
      "20509 1 1 1 1 3 3 3 5 \n",
      "20509 1 1 1 1 3 3 3 5 \n",
      "20532 8 1 8 8 8 8 8 8 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20537 9 4 4 4 4 4 4 4 \n",
      "20583 8 6 8 8 8 8 8 8 \n",
      "20643 8 8 0 8 8 8 8 8 \n",
      "20653 4 6 4 4 4 4 4 4 \n",
      "20665 7 1 7 7 1 7 7 1 \n",
      "20665 7 1 7 7 1 7 7 1 \n",
      "20665 7 1 7 7 1 7 7 1 \n",
      "20697 4 4 4 4 4 4 4 9 \n",
      "20717 7 4 4 4 4 7 7 7 \n",
      "20717 7 4 4 4 4 7 7 7 \n",
      "20717 7 4 4 4 4 7 7 7 \n",
      "20717 7 4 4 4 4 7 7 7 \n",
      "20718 3 2 3 3 3 3 3 3 \n",
      "20774 7 3 7 7 7 7 7 7 \n",
      "20792 6 6 6 6 5 6 6 6 \n",
      "20801 3 3 9 3 3 9 3 3 \n",
      "20801 3 3 9 3 3 9 3 3 \n",
      "20846 3 3 3 9 3 8 3 3 \n",
      "20846 3 3 3 9 3 8 3 3 \n",
      "20928 5 1 5 5 5 5 5 5 \n",
      "20946 5 5 5 5 3 5 5 5 \n",
      "20982 1 1 1 1 1 1 1 8 \n",
      "21009 4 9 4 4 4 4 4 9 \n",
      "21009 4 9 4 4 4 4 4 9 \n",
      "21044 6 6 6 6 5 6 6 5 \n",
      "21044 6 6 6 6 5 6 6 5 \n",
      "21138 9 9 9 9 9 9 4 9 \n",
      "21173 3 3 3 3 3 8 3 3 \n",
      "21221 7 7 7 7 7 7 7 9 \n",
      "21246 4 1 4 4 4 4 4 4 \n",
      "21307 7 7 7 7 7 7 7 9 \n",
      "21308 8 0 0 8 9 2 2 2 \n",
      "21308 8 0 0 8 9 2 2 2 \n",
      "21308 8 0 0 8 9 2 2 2 \n",
      "21308 8 0 0 8 9 2 2 2 \n",
      "21308 8 0 0 8 9 2 2 2 \n",
      "21308 8 0 0 8 9 2 2 2 \n",
      "21362 6 6 6 6 6 0 6 6 \n",
      "21376 1 1 1 1 1 1 7 1 \n",
      "21380 2 2 2 2 2 7 2 2 \n",
      "21388 4 1 4 4 7 7 7 7 \n",
      "21388 4 1 4 4 7 7 7 7 \n",
      "21388 4 1 4 4 7 7 7 7 \n",
      "21388 4 1 4 4 7 7 7 7 \n",
      "21388 4 1 4 4 7 7 7 7 \n",
      "21420 6 6 6 6 5 5 5 5 \n",
      "21420 6 6 6 6 5 5 5 5 \n",
      "21420 6 6 6 6 5 5 5 5 \n",
      "21420 6 6 6 6 5 5 5 5 \n",
      "21456 1 1 9 1 1 1 1 9 \n",
      "21456 1 1 9 1 1 1 1 9 \n",
      "21477 7 7 7 7 7 7 7 9 \n",
      "21484 0 0 0 0 0 0 8 8 \n",
      "21484 0 0 0 0 0 0 8 8 \n",
      "21539 5 8 5 5 5 5 5 5 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21566 4 6 6 6 6 6 6 6 \n",
      "21568 7 7 7 7 1 7 7 9 \n",
      "21568 7 7 7 7 1 7 7 9 \n",
      "21575 2 1 2 2 1 2 2 2 \n",
      "21575 2 1 2 2 1 2 2 2 \n",
      "21578 1 1 6 6 1 1 6 6 \n",
      "21578 1 1 6 6 1 1 6 6 \n",
      "21578 1 1 6 6 1 1 6 6 \n",
      "21578 1 1 6 6 1 1 6 6 \n",
      "21580 5 6 6 6 5 6 6 5 \n",
      "21580 5 6 6 6 5 6 6 5 \n",
      "21580 5 6 6 6 5 6 6 5 \n",
      "21580 5 6 6 6 5 6 6 5 \n",
      "21580 5 6 6 6 5 6 6 5 \n",
      "21626 6 6 6 6 5 6 6 6 \n",
      "21650 1 1 1 1 1 1 6 1 \n",
      "21658 8 8 8 8 8 8 3 8 \n",
      "21684 9 7 7 9 7 7 7 7 \n",
      "21684 9 7 7 9 7 7 7 7 \n",
      "21684 9 7 7 9 7 7 7 7 \n",
      "21684 9 7 7 9 7 7 7 7 \n",
      "21684 9 7 7 9 7 7 7 7 \n",
      "21684 9 7 7 9 7 7 7 7 \n",
      "21747 7 7 7 7 9 7 7 7 \n",
      "21758 2 2 2 2 1 2 2 2 \n",
      "21807 4 4 4 4 4 4 4 9 \n",
      "21851 6 6 6 6 5 5 6 5 \n",
      "21851 6 6 6 6 5 5 6 5 \n",
      "21851 6 6 6 6 5 5 6 5 \n",
      "21852 1 1 1 1 1 1 6 1 \n",
      "21881 1 1 1 1 1 2 1 1 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21893 4 5 5 5 5 5 5 5 \n",
      "21899 9 9 9 9 9 7 7 9 \n",
      "21899 9 9 9 9 9 7 7 9 \n",
      "21914 7 7 7 7 7 7 7 9 \n",
      "21950 0 0 0 0 0 0 2 0 \n",
      "21960 1 1 8 8 8 8 8 8 \n",
      "21960 1 1 8 8 8 8 8 8 \n",
      "21960 1 1 8 8 8 8 8 8 \n",
      "21960 1 1 8 8 8 8 8 8 \n",
      "21960 1 1 8 8 8 8 8 8 \n",
      "21960 1 1 8 8 8 8 8 8 \n",
      "21968 5 6 6 6 6 6 6 5 \n",
      "21968 5 6 6 6 6 6 6 5 \n",
      "21968 5 6 6 6 6 6 6 5 \n",
      "21968 5 6 6 6 6 6 6 5 \n",
      "21968 5 6 6 6 6 6 6 5 \n",
      "21968 5 6 6 6 6 6 6 5 \n",
      "21969 0 0 0 8 4 4 8 8 \n",
      "21969 0 0 0 8 4 4 8 8 \n",
      "21969 0 0 0 8 4 4 8 8 \n",
      "21969 0 0 0 8 4 4 8 8 \n",
      "21969 0 0 0 8 4 4 8 8 \n",
      "21975 2 2 2 2 3 2 2 2 \n",
      "22009 9 9 1 9 7 1 7 7 \n",
      "22009 9 9 1 9 7 1 7 7 \n",
      "22009 9 9 1 9 7 1 7 7 \n",
      "22009 9 9 1 9 7 1 7 7 \n",
      "22009 9 9 1 9 7 1 7 7 \n",
      "22015 3 3 5 3 3 3 3 3 \n",
      "22058 7 7 7 7 7 7 7 3 \n",
      "22066 9 9 9 9 9 8 9 9 \n",
      "22091 3 3 3 3 3 5 3 5 \n",
      "22091 3 3 3 3 3 5 3 5 \n",
      "22194 1 1 1 6 6 6 6 6 \n",
      "22194 1 1 1 6 6 6 6 6 \n",
      "22194 1 1 1 6 6 6 6 6 \n",
      "22194 1 1 1 6 6 6 6 6 \n",
      "22194 1 1 1 6 6 6 6 6 \n",
      "22219 3 3 3 3 3 8 3 3 \n",
      "22238 7 7 7 7 7 7 7 9 \n",
      "22250 2 2 2 2 3 2 2 2 \n",
      "22253 8 8 8 8 3 8 8 8 \n",
      "22261 9 9 9 9 9 9 5 5 \n",
      "22261 9 9 9 9 9 9 5 5 \n",
      "22289 5 5 5 5 5 8 5 5 \n",
      "22379 0 0 0 0 0 0 2 0 \n",
      "22401 1 1 1 1 1 1 1 9 \n",
      "22421 5 8 8 5 8 8 8 5 \n",
      "22421 5 8 8 5 8 8 8 5 \n",
      "22421 5 8 8 5 8 8 8 5 \n",
      "22421 5 8 8 5 8 8 8 5 \n",
      "22421 5 8 8 5 8 8 8 5 \n",
      "22464 7 7 7 2 7 7 7 7 \n",
      "22465 9 9 9 4 4 4 9 9 \n",
      "22465 9 9 9 4 4 4 9 9 \n",
      "22465 9 9 9 4 4 4 9 9 \n",
      "22484 7 7 7 9 7 7 7 7 \n",
      "22491 9 7 7 9 9 7 7 9 \n",
      "22491 9 7 7 9 9 7 7 9 \n",
      "22491 9 7 7 9 9 7 7 9 \n",
      "22491 9 7 7 9 9 7 7 9 \n",
      "22499 4 8 7 4 8 8 8 8 \n",
      "22499 4 8 7 4 8 8 8 8 \n",
      "22499 4 8 7 4 8 8 8 8 \n",
      "22499 4 8 7 4 8 8 8 8 \n",
      "22499 4 8 7 4 8 8 8 8 \n",
      "22499 4 8 7 4 8 8 8 8 \n",
      "22510 5 5 6 6 5 6 6 5 \n",
      "22510 5 5 6 6 5 6 6 5 \n",
      "22510 5 5 6 6 5 6 6 5 \n",
      "22510 5 5 6 6 5 6 6 5 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22513 0 6 6 6 6 6 6 6 \n",
      "22523 5 6 5 6 5 6 6 5 \n",
      "22523 5 6 5 6 5 6 6 5 \n",
      "22523 5 6 5 6 5 6 6 5 \n",
      "22523 5 6 5 6 5 6 6 5 \n",
      "22555 8 6 6 6 8 8 8 8 \n",
      "22555 8 6 6 6 8 8 8 8 \n",
      "22555 8 6 6 6 8 8 8 8 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22564 1 0 0 0 0 0 0 2 \n",
      "22565 1 1 1 1 1 1 6 6 \n",
      "22565 1 1 1 1 1 1 6 6 \n",
      "22622 7 7 7 7 4 7 7 4 \n",
      "22622 7 7 7 7 4 7 7 4 \n",
      "22627 9 9 9 9 9 9 3 9 \n",
      "22632 8 7 8 8 8 7 8 8 \n",
      "22632 8 7 8 8 8 7 8 8 \n",
      "22725 9 9 9 9 4 4 9 9 \n",
      "22725 9 9 9 9 4 4 9 9 \n",
      "22766 8 5 5 5 8 5 5 5 \n",
      "22766 8 5 5 5 8 5 5 5 \n",
      "22766 8 5 5 5 8 5 5 5 \n",
      "22766 8 5 5 5 8 5 5 5 \n",
      "22766 8 5 5 5 8 5 5 5 \n",
      "22766 8 5 5 5 8 5 5 5 \n",
      "22795 6 6 6 6 5 6 5 5 \n",
      "22795 6 6 6 6 5 6 5 5 \n",
      "22795 6 6 6 6 5 6 5 5 \n",
      "22806 6 6 6 6 5 6 6 5 \n",
      "22806 6 6 6 6 5 6 6 5 \n",
      "22823 4 4 4 4 4 4 9 9 \n",
      "22823 4 4 4 4 4 4 9 9 \n",
      "22828 9 9 9 9 9 8 8 8 \n",
      "22828 9 9 9 9 9 8 8 8 \n",
      "22828 9 9 9 9 9 8 8 8 \n",
      "22838 2 2 8 2 4 2 1 5 \n",
      "22838 2 2 8 2 4 2 1 5 \n",
      "22838 2 2 8 2 4 2 1 5 \n",
      "22838 2 2 8 2 4 2 1 5 \n",
      "22839 3 3 7 3 3 7 3 3 \n",
      "22839 3 3 7 3 3 7 3 3 \n",
      "22883 8 8 2 8 8 8 2 8 \n",
      "22883 8 8 2 8 8 8 2 8 \n",
      "22903 7 7 7 7 7 7 7 2 \n",
      "22923 4 4 4 4 4 4 4 9 \n",
      "22933 8 6 6 8 8 8 8 8 \n",
      "22933 8 6 6 8 8 8 8 8 \n",
      "22973 7 7 7 9 9 7 7 9 \n",
      "22973 7 7 7 9 9 7 7 9 \n",
      "22973 7 7 7 9 9 7 7 9 \n",
      "22980 0 0 0 6 6 6 6 6 \n",
      "22980 0 0 0 6 6 6 6 6 \n",
      "22980 0 0 0 6 6 6 6 6 \n",
      "22980 0 0 0 6 6 6 6 6 \n",
      "22980 0 0 0 6 6 6 6 6 \n",
      "23006 5 6 6 6 6 0 6 5 \n",
      "23006 5 6 6 6 6 0 6 5 \n",
      "23006 5 6 6 6 6 0 6 5 \n",
      "23006 5 6 6 6 6 0 6 5 \n",
      "23006 5 6 6 6 6 0 6 5 \n",
      "23006 5 6 6 6 6 0 6 5 \n",
      "23082 8 8 6 8 8 8 8 8 \n",
      "23113 6 6 6 6 5 6 6 6 \n",
      "23123 6 6 6 6 6 6 6 5 \n",
      "23156 8 7 8 8 7 7 7 7 \n",
      "23156 8 7 8 8 7 7 7 7 \n",
      "23156 8 7 8 8 7 7 7 7 \n",
      "23156 8 7 8 8 7 7 7 7 \n",
      "23156 8 7 8 8 7 7 7 7 \n",
      "23171 4 1 4 4 4 4 4 4 \n",
      "23188 4 6 6 6 4 6 6 6 \n",
      "23188 4 6 6 6 4 6 6 6 \n",
      "23188 4 6 6 6 4 6 6 6 \n",
      "23188 4 6 6 6 4 6 6 6 \n",
      "23188 4 6 6 6 4 6 6 6 \n",
      "23188 4 6 6 6 4 6 6 6 \n",
      "23251 6 6 6 6 6 6 6 5 \n",
      "23276 1 1 1 1 1 7 7 1 \n",
      "23276 1 1 1 1 1 7 7 1 \n",
      "23364 2 1 2 2 1 1 2 2 \n",
      "23364 2 1 2 2 1 1 2 2 \n",
      "23364 2 1 2 2 1 1 2 2 \n",
      "23425 4 4 4 4 4 7 7 4 \n",
      "23425 4 4 4 4 4 7 7 4 \n",
      "23435 8 7 7 8 7 9 2 9 \n",
      "23435 8 7 7 8 7 9 2 9 \n",
      "23435 8 7 7 8 7 9 2 9 \n",
      "23435 8 7 7 8 7 9 2 9 \n",
      "23435 8 7 7 8 7 9 2 9 \n",
      "23435 8 7 7 8 7 9 2 9 \n",
      "23536 2 2 2 2 3 2 2 2 \n",
      "23593 7 7 7 2 7 7 7 7 \n",
      "23597 6 6 6 6 6 6 6 5 \n",
      "23601 6 6 6 6 5 6 6 6 \n",
      "23618 6 6 6 6 5 6 5 5 \n",
      "23618 6 6 6 6 5 6 5 5 \n",
      "23618 6 6 6 6 5 6 5 5 \n",
      "23629 9 3 3 9 9 9 9 9 \n",
      "23629 9 3 3 9 9 9 9 9 \n",
      "23647 4 1 4 1 4 4 4 4 \n",
      "23647 4 1 4 1 4 4 4 4 \n",
      "23650 8 8 8 8 8 8 8 9 \n",
      "23660 7 9 7 7 7 7 7 9 \n",
      "23660 7 9 7 7 7 7 7 9 \n",
      "23703 4 4 4 4 4 4 7 4 \n",
      "23740 6 6 6 6 6 6 6 5 \n",
      "23744 9 9 9 9 4 9 9 9 \n",
      "23767 3 7 3 3 3 7 3 3 \n",
      "23767 3 7 3 3 3 7 3 3 \n",
      "23798 0 0 0 0 8 0 8 8 \n",
      "23798 0 0 0 0 8 0 8 8 \n",
      "23798 0 0 0 0 8 0 8 8 \n",
      "23820 1 1 1 1 1 1 7 1 \n",
      "23848 9 9 9 9 8 8 8 8 \n",
      "23848 9 9 9 9 8 8 8 8 \n",
      "23848 9 9 9 9 8 8 8 8 \n",
      "23848 9 9 9 9 8 8 8 8 \n",
      "23873 0 6 6 6 6 0 6 5 \n",
      "23873 0 6 6 6 6 0 6 5 \n",
      "23873 0 6 6 6 6 0 6 5 \n",
      "23873 0 6 6 6 6 0 6 5 \n",
      "23873 0 6 6 6 6 0 6 5 \n",
      "23873 0 6 6 6 6 0 6 5 \n",
      "23874 7 7 7 4 7 7 7 7 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23897 9 8 8 8 8 8 8 8 \n",
      "23913 4 4 4 4 4 4 9 4 \n",
      "23939 4 4 4 4 4 4 4 9 \n",
      "23957 5 6 5 5 5 5 5 5 \n",
      "24001 3 3 3 5 3 3 3 3 \n",
      "24015 9 4 4 9 4 9 9 9 \n",
      "24015 9 4 4 9 4 9 9 9 \n",
      "24015 9 4 4 9 4 9 9 9 \n",
      "24052 0 0 0 0 0 0 2 0 \n",
      "24062 9 2 2 2 2 2 2 9 \n",
      "24062 9 2 2 2 2 2 2 9 \n",
      "24062 9 2 2 2 2 2 2 9 \n",
      "24062 9 2 2 2 2 2 2 9 \n",
      "24062 9 2 2 2 2 2 2 9 \n",
      "24062 9 2 2 2 2 2 2 9 \n",
      "24086 1 1 1 1 1 1 1 8 \n",
      "24138 4 4 4 4 4 4 4 9 \n",
      "24155 1 1 6 6 1 2 2 6 \n",
      "24155 1 1 6 6 1 2 2 6 \n",
      "24155 1 1 6 6 1 2 2 6 \n",
      "24155 1 1 6 6 1 2 2 6 \n",
      "24155 1 1 6 6 1 2 2 6 \n",
      "24180 0 0 0 0 0 0 8 3 \n",
      "24180 0 0 0 0 0 0 8 3 \n",
      "24243 1 1 8 1 1 1 8 8 \n",
      "24243 1 1 8 1 1 1 8 8 \n",
      "24243 1 1 8 1 1 1 8 8 \n",
      "24245 9 9 7 9 7 7 7 9 \n",
      "24245 9 9 7 9 7 7 7 9 \n",
      "24245 9 9 7 9 7 7 7 9 \n",
      "24245 9 9 7 9 7 7 7 9 \n",
      "24329 4 9 4 4 4 4 4 9 \n",
      "24329 4 9 4 4 4 4 4 9 \n",
      "24349 3 5 5 3 5 5 5 5 \n",
      "24349 3 5 5 3 5 5 5 5 \n",
      "24349 3 5 5 3 5 5 5 5 \n",
      "24349 3 5 5 3 5 5 5 5 \n",
      "24349 3 5 5 3 5 5 5 5 \n",
      "24349 3 5 5 3 5 5 5 5 \n",
      "24404 4 4 4 6 6 6 6 6 \n",
      "24404 4 4 4 6 6 6 6 6 \n",
      "24404 4 4 4 6 6 6 6 6 \n",
      "24404 4 4 4 6 6 6 6 6 \n",
      "24404 4 4 4 6 6 6 6 6 \n",
      "24431 1 1 1 1 1 1 2 8 \n",
      "24431 1 1 1 1 1 1 2 8 \n",
      "24453 5 5 5 5 5 5 3 5 \n",
      "24460 7 7 7 7 4 7 7 7 \n",
      "24476 9 9 9 9 4 9 4 9 \n",
      "24476 9 9 9 9 4 9 4 9 \n",
      "24493 9 9 9 9 9 9 2 9 \n",
      "24505 5 5 5 5 5 0 5 5 \n",
      "24551 7 7 7 7 7 7 7 4 \n",
      "24564 4 4 4 4 4 4 7 4 \n",
      "24618 8 1 1 1 8 8 8 5 \n",
      "24618 8 1 1 1 8 8 8 5 \n",
      "24618 8 1 1 1 8 8 8 5 \n",
      "24618 8 1 1 1 8 8 8 5 \n",
      "24623 3 5 3 5 3 5 5 5 \n",
      "24623 3 5 3 5 3 5 5 5 \n",
      "24623 3 5 3 5 3 5 5 5 \n",
      "24623 3 5 3 5 3 5 5 5 \n",
      "24623 3 5 3 5 3 5 5 5 \n",
      "24671 9 9 9 9 9 9 7 9 \n",
      "24759 8 5 5 8 8 8 8 5 \n",
      "24759 8 5 5 8 8 8 8 5 \n",
      "24759 8 5 5 8 8 8 8 5 \n",
      "24767 5 5 5 5 3 3 3 5 \n",
      "24767 5 5 5 5 3 3 3 5 \n",
      "24767 5 5 5 5 3 3 3 5 \n",
      "24776 8 4 4 8 4 4 4 4 \n",
      "24776 8 4 4 8 4 4 4 4 \n",
      "24776 8 4 4 8 4 4 4 4 \n",
      "24776 8 4 4 8 4 4 4 4 \n",
      "24776 8 4 4 8 4 4 4 4 \n",
      "24776 8 4 4 8 4 4 4 4 \n",
      "24789 8 8 6 8 8 0 8 8 \n",
      "24789 8 8 6 8 8 0 8 8 \n",
      "24791 5 5 5 5 5 5 3 5 \n",
      "24809 1 1 7 1 7 7 7 7 \n",
      "24809 1 1 7 1 7 7 7 7 \n",
      "24809 1 1 7 1 7 7 7 7 \n",
      "24809 1 1 7 1 7 7 7 7 \n",
      "24809 1 1 7 1 7 7 7 7 \n",
      "24817 5 0 0 0 5 0 5 5 \n",
      "24817 5 0 0 0 5 0 5 5 \n",
      "24817 5 0 0 0 5 0 5 5 \n",
      "24817 5 0 0 0 5 0 5 5 \n",
      "24826 2 2 2 2 2 8 2 2 \n",
      "24862 7 7 7 7 7 7 7 9 \n",
      "24907 8 1 1 8 8 8 8 8 \n",
      "24907 8 1 1 8 8 8 8 8 \n",
      "24916 3 3 3 3 3 7 3 3 \n",
      "24971 3 1 1 3 3 3 3 3 \n",
      "24971 3 1 1 3 3 3 3 3 \n",
      "24974 9 9 9 5 9 9 9 9 \n",
      "24997 4 6 4 4 4 4 4 4 \n",
      "25034 1 1 1 1 1 7 1 1 \n",
      "25048 7 7 4 7 4 7 7 7 \n",
      "25048 7 7 4 7 4 7 7 7 \n",
      "25103 8 8 8 8 0 8 8 2 \n",
      "25103 8 8 8 8 0 8 8 2 \n",
      "25107 5 6 5 6 5 6 6 5 \n",
      "25107 5 6 5 6 5 6 6 5 \n",
      "25107 5 6 5 6 5 6 6 5 \n",
      "25107 5 6 5 6 5 6 6 5 \n",
      "25131 9 4 4 9 4 4 4 4 \n",
      "25131 9 4 4 9 4 4 4 4 \n",
      "25131 9 4 4 9 4 4 4 4 \n",
      "25131 9 4 4 9 4 4 4 4 \n",
      "25131 9 4 4 9 4 4 4 4 \n",
      "25131 9 4 4 9 4 4 4 4 \n",
      "25135 9 9 9 9 9 9 5 9 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25153 9 7 7 7 7 7 7 7 \n",
      "25155 6 6 6 6 5 6 6 6 \n",
      "25164 7 7 7 5 7 7 7 5 \n",
      "25164 7 7 7 5 7 7 7 5 \n",
      "25176 5 5 5 5 5 6 5 5 \n",
      "25206 2 2 2 2 3 2 2 2 \n",
      "25216 8 9 9 8 8 8 8 8 \n",
      "25216 8 9 9 8 8 8 8 8 \n",
      "25240 3 3 9 3 3 3 3 3 \n",
      "25256 4 9 4 4 4 4 4 4 \n",
      "25258 1 1 1 1 1 1 8 8 \n",
      "25258 1 1 1 1 1 1 8 8 \n",
      "25272 7 7 0 0 7 7 7 5 \n",
      "25272 7 7 0 0 7 7 7 5 \n",
      "25272 7 7 0 0 7 7 7 5 \n",
      "25397 8 8 8 4 8 8 8 8 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25423 9 3 5 5 5 5 5 5 \n",
      "25429 4 5 4 4 4 5 5 5 \n",
      "25429 4 5 4 4 4 5 5 5 \n",
      "25429 4 5 4 4 4 5 5 5 \n",
      "25429 4 5 4 4 4 5 5 5 \n",
      "25515 4 4 4 4 4 4 4 6 \n",
      "25527 1 1 1 6 1 6 6 6 \n",
      "25527 1 1 1 6 1 6 6 6 \n",
      "25527 1 1 1 6 1 6 6 6 \n",
      "25527 1 1 1 6 1 6 6 6 \n",
      "25544 6 6 6 6 6 6 6 5 \n",
      "25559 2 2 2 2 2 2 2 5 \n",
      "25603 7 7 7 7 7 7 7 9 \n",
      "25653 7 7 7 9 7 7 7 7 \n",
      "25656 7 9 7 7 7 7 7 7 \n",
      "25715 9 9 4 9 9 9 9 9 \n",
      "25855 2 6 2 2 2 2 2 2 \n",
      "25860 7 7 7 7 4 7 7 7 \n",
      "25861 8 8 8 8 8 8 8 5 \n",
      "25879 1 1 1 1 1 7 7 3 \n",
      "25879 1 1 1 1 1 7 7 3 \n",
      "25879 1 1 1 1 1 7 7 3 \n",
      "25889 9 9 9 9 7 7 7 9 \n",
      "25889 9 9 9 9 7 7 7 9 \n",
      "25889 9 9 9 9 7 7 7 9 \n",
      "25915 4 8 4 4 4 4 4 4 \n",
      "25920 5 6 5 5 5 5 5 5 \n",
      "25952 0 0 6 0 5 0 6 5 \n",
      "25952 0 0 6 0 5 0 6 5 \n",
      "25952 0 0 6 0 5 0 6 5 \n",
      "25952 0 0 6 0 5 0 6 5 \n",
      "25970 2 1 1 2 1 1 1 1 \n",
      "25970 2 1 1 2 1 1 1 1 \n",
      "25970 2 1 1 2 1 1 1 1 \n",
      "25970 2 1 1 2 1 1 1 1 \n",
      "25970 2 1 1 2 1 1 1 1 \n",
      "25970 2 1 1 2 1 1 1 1 \n",
      "26013 9 4 9 4 4 4 4 9 \n",
      "26013 9 4 9 4 4 4 4 9 \n",
      "26013 9 4 9 4 4 4 4 9 \n",
      "26013 9 4 9 4 4 4 4 9 \n",
      "26013 9 4 9 4 4 4 4 9 \n",
      "26079 3 3 3 2 3 3 3 3 \n",
      "26085 4 1 4 4 4 4 4 4 \n",
      "26091 9 9 9 3 3 9 3 3 \n",
      "26091 9 9 9 3 3 9 3 3 \n",
      "26091 9 9 9 3 3 9 3 3 \n",
      "26091 9 9 9 3 3 9 3 3 \n",
      "26123 8 2 8 8 2 2 2 2 \n",
      "26123 8 2 8 8 2 2 2 2 \n",
      "26123 8 2 8 8 2 2 2 2 \n",
      "26123 8 2 8 8 2 2 2 2 \n",
      "26123 8 2 8 8 2 2 2 2 \n",
      "26134 9 9 9 4 4 4 4 9 \n",
      "26134 9 9 9 4 4 4 4 9 \n",
      "26134 9 9 9 4 4 4 4 9 \n",
      "26134 9 9 9 4 4 4 4 9 \n",
      "26137 1 1 1 8 6 6 6 6 \n",
      "26137 1 1 1 8 6 6 6 6 \n",
      "26137 1 1 1 8 6 6 6 6 \n",
      "26137 1 1 1 8 6 6 6 6 \n",
      "26137 1 1 1 8 6 6 6 6 \n",
      "26164 2 1 2 2 2 2 2 2 \n",
      "26166 6 6 6 6 6 6 6 5 \n",
      "26170 0 0 0 0 0 0 0 2 \n",
      "26210 7 2 2 2 2 7 2 2 \n",
      "26210 7 2 2 2 2 7 2 2 \n",
      "26210 7 2 2 2 2 7 2 2 \n",
      "26210 7 2 2 2 2 7 2 2 \n",
      "26210 7 2 2 2 2 7 2 2 \n",
      "26210 7 2 2 2 2 7 2 2 \n",
      "26213 0 0 0 9 0 0 0 0 \n",
      "26260 7 1 7 7 7 7 7 7 \n",
      "26292 4 4 4 4 4 7 7 4 \n",
      "26292 4 4 4 4 4 7 7 4 \n",
      "26317 4 4 4 8 4 4 4 8 \n",
      "26317 4 4 4 8 4 4 4 8 \n",
      "26392 7 7 7 3 3 3 3 3 \n",
      "26392 7 7 7 3 3 3 3 3 \n",
      "26392 7 7 7 3 3 3 3 3 \n",
      "26392 7 7 7 3 3 3 3 3 \n",
      "26392 7 7 7 3 3 3 3 3 \n",
      "26411 2 2 2 2 2 2 2 8 \n",
      "26496 6 6 6 6 8 6 6 5 \n",
      "26496 6 6 6 6 8 6 6 5 \n",
      "26548 7 1 7 7 1 1 7 7 \n",
      "26548 7 1 7 7 1 1 7 7 \n",
      "26548 7 1 7 7 1 1 7 7 \n",
      "26554 7 1 7 1 7 7 7 7 \n",
      "26554 7 1 7 1 7 7 7 7 \n",
      "26571 6 6 6 6 8 8 8 8 \n",
      "26571 6 6 6 6 8 8 8 8 \n",
      "26571 6 6 6 6 8 8 8 8 \n",
      "26571 6 6 6 6 8 8 8 8 \n",
      "26641 6 6 6 6 5 5 5 5 \n",
      "26641 6 6 6 6 5 5 5 5 \n",
      "26641 6 6 6 6 5 5 5 5 \n",
      "26641 6 6 6 6 5 5 5 5 \n",
      "26650 0 2 0 2 9 0 2 2 \n",
      "26650 0 2 0 2 9 0 2 2 \n",
      "26650 0 2 0 2 9 0 2 2 \n",
      "26650 0 2 0 2 9 0 2 2 \n",
      "26650 0 2 0 2 9 0 2 2 \n",
      "26687 6 6 6 6 5 6 6 5 \n",
      "26687 6 6 6 6 5 6 6 5 \n",
      "26698 8 5 8 8 8 5 5 5 \n",
      "26698 8 5 8 8 8 5 5 5 \n",
      "26698 8 5 8 8 8 5 5 5 \n",
      "26698 8 5 8 8 8 5 5 5 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26764 0 2 2 2 2 2 2 2 \n",
      "26774 7 7 7 2 7 7 7 7 \n",
      "26800 4 4 4 4 4 4 9 4 \n",
      "26814 4 4 4 8 4 4 4 4 \n",
      "26883 1 1 8 1 1 1 8 9 \n",
      "26883 1 1 8 1 1 1 8 9 \n",
      "26883 1 1 8 1 1 1 8 9 \n",
      "26892 4 4 4 4 4 4 9 9 \n",
      "26892 4 4 4 4 4 4 9 9 \n",
      "26907 9 7 7 9 7 7 7 7 \n",
      "26907 9 7 7 9 7 7 7 7 \n",
      "26907 9 7 7 9 7 7 7 7 \n",
      "26907 9 7 7 9 7 7 7 7 \n",
      "26907 9 7 7 9 7 7 7 7 \n",
      "26907 9 7 7 9 7 7 7 7 \n",
      "26908 9 9 9 5 5 5 5 5 \n",
      "26908 9 9 9 5 5 5 5 5 \n",
      "26908 9 9 9 5 5 5 5 5 \n",
      "26908 9 9 9 5 5 5 5 5 \n",
      "26908 9 9 9 5 5 5 5 5 \n",
      "26912 8 8 8 8 4 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26949 9 8 8 8 8 8 8 8 \n",
      "26950 2 7 7 2 7 7 7 2 \n",
      "26950 2 7 7 2 7 7 7 2 \n",
      "26950 2 7 7 2 7 7 7 2 \n",
      "26950 2 7 7 2 7 7 7 2 \n",
      "26950 2 7 7 2 7 7 7 2 \n",
      "26954 1 1 1 1 1 1 7 1 \n",
      "26974 4 4 4 4 4 4 4 9 \n",
      "27078 2 7 2 2 2 2 2 2 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27081 9 8 8 8 8 8 8 8 \n",
      "27238 0 0 0 6 0 0 0 0 \n",
      "27314 6 6 6 6 5 6 6 5 \n",
      "27314 6 6 6 6 5 6 6 5 \n",
      "27336 0 1 0 0 1 1 2 8 \n",
      "27336 0 1 0 0 1 1 2 8 \n",
      "27336 0 1 0 0 1 1 2 8 \n",
      "27336 0 1 0 0 1 1 2 8 \n",
      "27336 0 1 0 0 1 1 2 8 \n",
      "27343 9 9 9 8 9 9 9 9 \n",
      "27352 9 4 4 4 4 4 4 9 \n",
      "27352 9 4 4 4 4 4 4 9 \n",
      "27352 9 4 4 4 4 4 4 9 \n",
      "27352 9 4 4 4 4 4 4 9 \n",
      "27352 9 4 4 4 4 4 4 9 \n",
      "27352 9 4 4 4 4 4 4 9 \n",
      "27372 2 2 2 2 3 7 3 3 \n",
      "27372 2 2 2 2 3 7 3 3 \n",
      "27372 2 2 2 2 3 7 3 3 \n",
      "27372 2 2 2 2 3 7 3 3 \n",
      "27402 2 1 1 2 2 2 2 2 \n",
      "27402 2 1 1 2 2 2 2 2 \n",
      "27406 3 3 9 3 3 3 3 3 \n",
      "27511 9 3 5 9 3 3 3 5 \n",
      "27511 9 3 5 9 3 3 3 5 \n",
      "27511 9 3 5 9 3 3 3 5 \n",
      "27511 9 3 5 9 3 3 3 5 \n",
      "27511 9 3 5 9 3 3 3 5 \n",
      "27511 9 3 5 9 3 3 3 5 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27512 4 6 6 6 6 6 6 6 \n",
      "27569 9 9 9 9 9 9 9 5 \n",
      "27593 4 4 4 4 4 1 4 9 \n",
      "27593 4 4 4 4 4 1 4 9 \n",
      "27627 0 0 0 0 0 0 2 0 \n",
      "27658 6 6 6 6 5 6 6 5 \n",
      "27658 6 6 6 6 5 6 6 5 \n",
      "27665 9 9 9 9 4 4 4 9 \n",
      "27665 9 9 9 9 4 4 4 9 \n",
      "27665 9 9 9 9 4 4 4 9 \n",
      "27692 4 4 4 4 4 4 8 4 \n",
      "27716 8 1 1 8 1 1 8 5 \n",
      "27716 8 1 1 8 1 1 8 5 \n",
      "27716 8 1 1 8 1 1 8 5 \n",
      "27716 8 1 1 8 1 1 8 5 \n",
      "27716 8 1 1 8 1 1 8 5 \n",
      "27724 9 9 9 8 9 8 9 9 \n",
      "27724 9 9 9 8 9 8 9 9 \n",
      "27799 9 7 9 9 9 7 7 9 \n",
      "27799 9 7 9 9 9 7 7 9 \n",
      "27799 9 7 9 9 9 7 7 9 \n",
      "27864 6 6 6 6 5 6 6 5 \n",
      "27864 6 6 6 6 5 6 6 5 \n",
      "27869 7 7 7 7 7 7 7 9 \n",
      "27916 1 1 6 6 1 1 6 6 \n",
      "27916 1 1 6 6 1 1 6 6 \n",
      "27916 1 1 6 6 1 1 6 6 \n",
      "27916 1 1 6 6 1 1 6 6 \n",
      "27918 2 2 4 2 4 2 2 2 \n",
      "27918 2 2 4 2 4 2 2 2 \n",
      "27937 9 4 4 9 4 4 4 9 \n",
      "27937 9 4 4 9 4 4 4 9 \n",
      "27937 9 4 4 9 4 4 4 9 \n",
      "27937 9 4 4 9 4 4 4 9 \n",
      "27937 9 4 4 9 4 4 4 9 \n",
      "27939 1 2 2 1 2 2 2 2 \n",
      "27939 1 2 2 1 2 2 2 2 \n",
      "27939 1 2 2 1 2 2 2 2 \n",
      "27939 1 2 2 1 2 2 2 2 \n",
      "27939 1 2 2 1 2 2 2 2 \n",
      "27939 1 2 2 1 2 2 2 2 \n",
      "27952 5 5 6 5 5 5 5 5 \n",
      "27967 0 0 0 0 0 0 2 0 \n",
      "27992 1 1 1 1 1 1 7 1 \n"
     ]
    }
   ],
   "source": [
    "for i in err:\n",
    "    print(i,end=' ')\n",
    "    for j in range(8):\n",
    "        print(label_data[j,i],end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:30:07.579039Z",
     "iopub.status.busy": "2021-06-12T19:30:07.577476Z",
     "iopub.status.idle": "2021-06-12T19:30:07.692607Z",
     "shell.execute_reply": "2021-06-12T19:30:07.692147Z",
     "shell.execute_reply.started": "2021-06-12T18:58:26.154078Z"
    },
    "papermill": {
     "duration": 3.176995,
     "end_time": "2021-06-12T19:30:07.692699",
     "exception": false,
     "start_time": "2021-06-12T19:30:04.515704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f75f0456550>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(testX[165,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-12T19:30:13.448249Z",
     "iopub.status.busy": "2021-06-12T19:30:13.447727Z",
     "iopub.status.idle": "2021-06-12T19:30:25.623683Z",
     "shell.execute_reply": "2021-06-12T19:30:25.622686Z",
     "shell.execute_reply.started": "2021-06-12T18:58:26.415018Z"
    },
    "papermill": {
     "duration": 15.063464,
     "end_time": "2021-06-12T19:30:25.623813",
     "exception": false,
     "start_time": "2021-06-12T19:30:10.560349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stacked_pred = pd.DataFrame(label_data).mode(axis=0).iloc[0]\n",
    "stacked_pred = stacked_pred.apply(lambda x:int(x))\n",
    "stacked_pred = pd.DataFrame({'Label': stacked_pred})\n",
    "xyz = pd.DataFrame(np.arange(1,stacked_pred.shape[0]+1),columns=['ImageId'])\n",
    "\n",
    "stacked_pred = pd.concat([xyz,stacked_pred],axis=1)\n",
    "stacked_pred.to_csv('STACK_OF_EIGHT.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.894342,
     "end_time": "2021-06-12T19:30:31.564818",
     "exception": false,
     "start_time": "2021-06-12T19:30:28.670476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "At Last i used stacked my different result that i had and used simple mode to decide my answer .\n",
    "My submissions were completed at this point but i have good feeling about this predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.249972,
     "end_time": "2021-06-12T19:30:37.815072",
     "exception": false,
     "start_time": "2021-06-12T19:30:34.565100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This marks the end of the notebook , hope it helps in your journey and hope we meet again soon .\n",
    "\n",
    "THANK YOU FOR READING ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.889319,
     "end_time": "2021-06-12T19:30:43.780525",
     "exception": false,
     "start_time": "2021-06-12T19:30:40.891206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.111588,
     "end_time": "2021-06-12T19:30:49.769032",
     "exception": false,
     "start_time": "2021-06-12T19:30:46.657444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1861.185857,
   "end_time": "2021-06-12T19:30:52.807829",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-12T18:59:51.621972",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
